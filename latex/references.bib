@article{ma2024era,
  title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, 
  author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
  journal={arXiv preprint arXiv:2402.17764},
  year={2024},
  url={https://arxiv.org/abs/2402.17764}, 
}

@article{wang2023bitnet,
  title={BitNet: Scaling 1-bit Transformers for Large Language Models}, 
  author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023},
  url={https://arxiv.org/abs/2310.11453},
}

@article{zhao2025direct,
  title={Direct Quantized Training of Language Models with Stochastic Rounding}, 
  author={Kaiyan Zhao and Tsuguchika Tabaru and Kenichi Kobayashi and Takumi Honda and Masafumi Yamazaki and Yoshimasa Tsuruoka},
  journal={Proceedings of Machine Learning Research},
  volume={304},
  year={2025},
  note={ACML 2025},
  url={https://arxiv.org/abs/2412.04787},
}

@article{semenov2025smooth,
  title={Smooth Approximations of the Rounding Function},
  author={Stanislav Semenov},
  journal={arXiv preprint arXiv:2504.19026},
  year={2025},
  url={https://arxiv.org/abs/2504.19026},
}

@article{daliri2024theory,
  title={Unlocking the Theory Behind Scaling 1-Bit Neural Networks},
  author={Majid Daliri and Zhao Song and Chiwun Yang},
  journal={arXiv preprint arXiv:2411.01663},
  year={2024},
  url={https://arxiv.org/abs/2411.01663},
}

@article{li2024continuous,
  title={Continuous Approximations for Improving Quantization Aware Training of LLMs},
  author={He Li and Jianhang Hong and Yuanzhuo Wu and Snehal Adbol and Zonglin Li},
  journal={arXiv preprint arXiv:2410.10849},
  year={2024},
  url={https://arxiv.org/abs/2410.10849}, 
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Yoshua Bengio and Nicholas LÃ©onard and Aaron Courville},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013},
  url={https://arxiv.org/abs/1308.3432}, 
}