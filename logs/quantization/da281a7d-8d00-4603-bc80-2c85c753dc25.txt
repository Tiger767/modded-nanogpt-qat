import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# Travis' Quantization Code

ENABLE_MLP_QUANTIZATION = True
ENABLE_ATTN_QUANTIZATION = True


class round_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input):
        ctx.input = input
        return torch.round(input)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()
        return grad_input


class noisy_round_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input):
        ctx.input = input
        return torch.round(input + torch.rand_like(input) - .5)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()
        return grad_input


class round_grad_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input):
        ctx.input = input
        return torch.round(input)

    @staticmethod
    def backward(ctx, grad_output):
        l = 1
        k = 5
        #x = l * (1 + torch.sign(grad_output - l) * torch.abs(grad_output - l)**(1/k))
        x = ctx.input
        x = x - torch.round(x - l / 2)
        grad = 1 / k * torch.abs(x - l / 2)**(1/k - 1)
        grad = grad.clamp(min=-3, max=3)
        return grad * grad_output


class clamp_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input, min_val=-1.0, max_val=1.0):
        ctx.input = input
        return torch.clamp(input, min_val, max_val)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()
        return grad_input, None, None


# def ternary_quantize(
#     x: torch.Tensor,
# ):
#     min_max_value = x.abs().mean().clamp(min=1e-5)
#     # clamp_pt?
#     quantized = clamp_pt.apply(round_pt.apply(x / min_max_value))
#     return min_max_value * quantized


# def ternary_quantize(
#     x: torch.Tensor,
# ):
#     min_max_value = x.abs().mean().clamp(min=1e-5)
#     quantized = clamp_pt.apply(noisy_round_pt.apply(x / min_max_value))
#     return min_max_value * quantized


def ternary_quantize(
    x: torch.Tensor,
):
    min_max_value = x.abs().mean().clamp(min=1e-5)
    quantized = clamp_pt.apply(round_grad_pt.apply(x / min_max_value))
    return min_max_value * quantized


# def quantized_sigmoid(x):
#     x = torch.nn.functional.sigmoid(x)
#     return noisy_round_pt.apply(x)


def quantized_sigmoid(x):
    x = torch.nn.functional.sigmoid(x)
    return round_grad_pt.apply(x)


def quantized_relu2(
    x: torch.Tensor,
    num_bits=8,
    eps=1e-5,
):
    # 1. Apply the ReLU-squared activation.
    # The result is guaranteed to be non-negative (>= 0).
    x = torch.nn.functional.relu(x)
    x = x.square()

    # 2. Find the maximum value along the inner dimension (last dim).
    # We clamp at eps to prevent division by zero if a row is all zeros.
    xmax = x.max(-1, keepdim=True).values.clamp(min=eps)

    # 3. Determine the number of quantization steps.
    # As requested: "num steps should be 2**num_bits - 1"
    num_steps = (2**num_bits - 1)

    # 4. Quantize the values.
    # The range is [0, xmax].
    # We scale x to [0, num_steps], round it, then scale it back.
    
    # Scale to [0, num_steps]
    scaled_x = x / xmax * num_steps
    
    # Round to the nearest integer step (using your custom grad function)
    rounded_x = round_grad_pt.apply(scaled_x)
    
    # Scale back to the original magnitude range [0, xmax]
    quantized_x = rounded_x / num_steps * xmax

    return quantized_x


def quantized_gelu(
    x: torch.Tensor,
    num_bits=8,
    eps=1e-5,
):
    x = torch.nn.functional.gelu(x)

    num_steps = (2**num_bits - 1) // 2

    pos = x >= 0
    xnmin = x.min(-1, keepdim=True).values.clamp(max=-eps)

    xpmax = x.max(-1, keepdim=True).values.clamp(min=eps)

    xp = round_pt.apply(x / xpmax * num_steps) / num_steps * xpmax
    xn = round_pt.apply(x / xnmin * num_steps) / num_steps * xnmin
    x = xp * pos + xn * (~pos)

    return x


class Quantize(nn.Module):
    def __init__(
        self,
        quantize_func,
        out_scalars=False,
        shape=None,
        device=None,
        dtype=None,
        *args,
        **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)
        self.quantize_func = quantize_func

        self._post_quantized_mode = False
        self._max_value = 1.0 #nn.Parameter(torch.tensor(1.0))

        self.out_scalars = None
        if out_scalars:
            self.out_scalars = nn.Parameter(
                torch.sign(
                    torch.randn((shape[0], 1, *shape[2:]), device=device, dtype=dtype)
                )
            )

    def forward(self, x):
        if self.quantize_func is None:
            return x
        if self._post_quantized_mode:
            return x * self._max_value
        if self.out_scalars is not None:
            return self.out_scalars * self.quantize_func(x)
        return self.quantize_func(x)

    def quantize(self, x, round=False, to_scalar=True, dtype=None):
        self._post_quantized_mode = True
        if self.quantize_func is None:
            return

        qx = self.quantize_func(x.data)
        self._max_value.data = torch.max(torch.abs(qx))
        qx = qx / self._max_value.data
        qx = qx.round() if round else qx
        qx = qx.to(dtype) if dtype is not None else qx
        if to_scalar:
            nu = torch.unique(qx.flatten())
            if len(nu) == 1:
                qx = nu[:1]
        x.data = qx


class QuantizedLinear(nn.Linear):
    def __init__(
        self, *args, weight_quantize_func=None, bias_quantize_func=None, **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)
        self.quantize_weight = Quantize(weight_quantize_func)
        self.quantize_bias = Quantize(bias_quantize_func)

    def forward(self, input):
        qw = self.quantize_weight(self.weight)
        qb = self.quantize_bias(self.bias)
        return F.linear(input, qw, qb)

    def quantize(self, dtype=None):
        self.quantize_weight.quantize(self.weight, dtype=dtype)
        self.quantize_bias.quantize(self.bias, dtype=dtype)


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # quantization
        self.quantize_qkvo_w = Quantize(ternary_quantize) if ENABLE_ATTN_QUANTIZATION else lambda x: x
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.quantize_qkvo_w(self.qkvo_w).view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # quantization
        self.quantize_c_fc = Quantize(ternary_quantize) if ENABLE_MLP_QUANTIZATION else lambda x: x
        self.quantize_c_proj = Quantize(ternary_quantize) if ENABLE_MLP_QUANTIZATION else lambda x: x
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.quantize_c_fc(self.c_fc).T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.quantize_c_fc(self.c_proj).type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 100 # 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = True
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
Running PyTorch 2.9.0+cu128 compiled for CUDA 12.8
Running Triton version 3.5.0
Sat Nov  8 15:03:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:06:00.0 Off |                    0 |
| N/A   34C    P0             78W /  350W |    1103MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            5619      C   /usr/bin/python3                       1094MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2285 train_time:687ms step_avg:686.78ms
step:2/2285 train_time:1240ms step_avg:620.11ms
step:3/2285 train_time:1889ms step_avg:629.69ms
step:4/2285 train_time:2585ms step_avg:646.28ms
step:5/2285 train_time:3288ms step_avg:657.61ms
step:6/2285 train_time:3996ms step_avg:665.93ms
step:7/2285 train_time:4693ms step_avg:670.48ms
step:8/2285 train_time:5400ms step_avg:674.95ms
step:9/2285 train_time:6106ms step_avg:678.50ms
step:10/2285 train_time:6818ms step_avg:681.84ms
step:11/2285 train_time:7527ms step_avg:684.30ms
step:12/2285 train_time:8240ms step_avg:686.65ms
step:13/2285 train_time:8949ms step_avg:688.37ms
step:14/2285 train_time:9662ms step_avg:690.14ms
step:15/2285 train_time:10368ms step_avg:691.20ms
step:16/2285 train_time:11081ms step_avg:692.56ms
step:17/2285 train_time:11791ms step_avg:693.59ms
step:18/2285 train_time:12504ms step_avg:694.67ms
step:19/2285 train_time:13213ms step_avg:695.41ms
step:20/2285 train_time:13929ms step_avg:696.43ms
step:21/2285 train_time:14638ms step_avg:697.05ms
step:22/2285 train_time:15353ms step_avg:697.87ms
step:23/2285 train_time:16062ms step_avg:698.34ms
step:24/2285 train_time:16778ms step_avg:699.09ms
step:25/2285 train_time:17489ms step_avg:699.57ms
step:26/2285 train_time:18204ms step_avg:700.14ms
step:27/2285 train_time:18914ms step_avg:700.53ms
step:28/2285 train_time:19629ms step_avg:701.04ms
step:29/2285 train_time:20340ms step_avg:701.37ms
step:30/2285 train_time:21058ms step_avg:701.94ms
step:31/2285 train_time:21769ms step_avg:702.23ms
step:32/2285 train_time:22485ms step_avg:702.66ms
step:33/2285 train_time:23197ms step_avg:702.93ms
step:34/2285 train_time:23914ms step_avg:703.34ms
step:35/2285 train_time:24622ms step_avg:703.48ms
step:36/2285 train_time:25339ms step_avg:703.87ms
step:37/2285 train_time:26051ms step_avg:704.09ms
step:38/2285 train_time:26763ms step_avg:704.29ms
step:39/2285 train_time:27471ms step_avg:704.39ms
step:40/2285 train_time:28184ms step_avg:704.59ms
step:41/2285 train_time:28892ms step_avg:704.68ms
step:42/2285 train_time:29604ms step_avg:704.86ms
step:43/2285 train_time:30312ms step_avg:704.93ms
step:44/2285 train_time:31025ms step_avg:705.11ms
step:45/2285 train_time:31731ms step_avg:705.14ms
step:46/2285 train_time:32445ms step_avg:705.33ms
step:47/2285 train_time:33154ms step_avg:705.40ms
step:48/2285 train_time:33867ms step_avg:705.56ms
step:49/2285 train_time:34575ms step_avg:705.62ms
step:50/2285 train_time:35290ms step_avg:705.79ms
step:51/2285 train_time:35999ms step_avg:705.86ms
step:52/2285 train_time:36714ms step_avg:706.04ms
step:53/2285 train_time:37424ms step_avg:706.11ms
step:54/2285 train_time:38141ms step_avg:706.32ms
step:55/2285 train_time:38851ms step_avg:706.37ms
step:56/2285 train_time:39567ms step_avg:706.55ms
step:57/2285 train_time:40278ms step_avg:706.64ms
step:58/2285 train_time:40995ms step_avg:706.81ms
step:59/2285 train_time:41704ms step_avg:706.85ms
step:60/2285 train_time:42422ms step_avg:707.03ms
step:61/2285 train_time:43134ms step_avg:707.11ms
step:62/2285 train_time:43850ms step_avg:707.27ms
step:63/2285 train_time:44562ms step_avg:707.33ms
step:64/2285 train_time:45282ms step_avg:707.53ms
step:65/2285 train_time:45994ms step_avg:707.60ms
step:66/2285 train_time:46712ms step_avg:707.76ms
step:67/2285 train_time:47422ms step_avg:707.79ms
step:68/2285 train_time:48143ms step_avg:707.98ms
step:69/2285 train_time:48856ms step_avg:708.06ms
step:70/2285 train_time:49574ms step_avg:708.20ms
step:71/2285 train_time:50286ms step_avg:708.25ms
step:72/2285 train_time:51005ms step_avg:708.41ms
step:73/2285 train_time:51720ms step_avg:708.50ms
step:74/2285 train_time:52439ms step_avg:708.64ms
step:75/2285 train_time:53155ms step_avg:708.73ms
step:76/2285 train_time:53875ms step_avg:708.88ms
step:77/2285 train_time:54588ms step_avg:708.93ms
step:78/2285 train_time:55307ms step_avg:709.07ms
step:79/2285 train_time:56023ms step_avg:709.15ms
step:80/2285 train_time:56742ms step_avg:709.28ms
step:81/2285 train_time:57458ms step_avg:709.36ms
step:82/2285 train_time:58178ms step_avg:709.49ms
step:83/2285 train_time:58893ms step_avg:709.56ms
step:84/2285 train_time:59612ms step_avg:709.67ms
step:85/2285 train_time:60328ms step_avg:709.74ms
step:86/2285 train_time:61049ms step_avg:709.87ms
step:87/2285 train_time:61763ms step_avg:709.92ms
step:88/2285 train_time:62485ms step_avg:710.06ms
step:89/2285 train_time:63201ms step_avg:710.12ms
step:90/2285 train_time:63924ms step_avg:710.27ms
step:91/2285 train_time:64640ms step_avg:710.33ms
step:92/2285 train_time:65362ms step_avg:710.46ms
step:93/2285 train_time:66077ms step_avg:710.51ms
step:94/2285 train_time:66800ms step_avg:710.64ms
step:95/2285 train_time:67516ms step_avg:710.70ms
step:96/2285 train_time:68239ms step_avg:710.82ms
step:97/2285 train_time:68957ms step_avg:710.90ms
step:98/2285 train_time:69681ms step_avg:711.03ms
step:99/2285 train_time:70397ms step_avg:711.08ms
step:100/2285 train_time:71120ms step_avg:711.20ms
step:100/2285 val_loss:4.9926 train_time:71216ms step_avg:712.16ms
step:101/2285 train_time:71835ms step_avg:711.24ms
step:102/2285 train_time:72560ms step_avg:711.38ms
step:103/2285 train_time:73278ms step_avg:711.44ms
step:104/2285 train_time:74002ms step_avg:711.56ms
step:105/2285 train_time:74719ms step_avg:711.61ms
step:106/2285 train_time:75444ms step_avg:711.74ms
step:107/2285 train_time:76163ms step_avg:711.80ms
step:108/2285 train_time:76884ms step_avg:711.89ms
step:109/2285 train_time:77604ms step_avg:711.97ms
step:110/2285 train_time:78326ms step_avg:712.05ms
step:111/2285 train_time:79046ms step_avg:712.13ms
step:112/2285 train_time:79770ms step_avg:712.23ms
step:113/2285 train_time:80487ms step_avg:712.27ms
step:114/2285 train_time:81211ms step_avg:712.38ms
step:115/2285 train_time:81930ms step_avg:712.43ms
step:116/2285 train_time:82655ms step_avg:712.54ms
step:117/2285 train_time:83373ms step_avg:712.59ms
step:118/2285 train_time:84098ms step_avg:712.69ms
step:119/2285 train_time:84815ms step_avg:712.73ms
step:120/2285 train_time:85539ms step_avg:712.82ms
step:121/2285 train_time:86258ms step_avg:712.88ms
step:122/2285 train_time:86983ms step_avg:712.98ms
step:123/2285 train_time:87701ms step_avg:713.02ms
step:124/2285 train_time:88423ms step_avg:713.09ms
step:125/2285 train_time:89143ms step_avg:713.14ms
step:126/2285 train_time:89866ms step_avg:713.22ms
step:127/2285 train_time:90586ms step_avg:713.28ms
step:128/2285 train_time:91309ms step_avg:713.35ms
step:129/2285 train_time:92028ms step_avg:713.39ms
step:130/2285 train_time:92751ms step_avg:713.47ms
step:131/2285 train_time:93468ms step_avg:713.50ms
step:132/2285 train_time:94194ms step_avg:713.59ms
step:133/2285 train_time:94913ms step_avg:713.64ms
step:134/2285 train_time:95637ms step_avg:713.71ms
step:135/2285 train_time:96358ms step_avg:713.76ms
step:136/2285 train_time:97084ms step_avg:713.85ms
step:137/2285 train_time:97805ms step_avg:713.91ms
step:138/2285 train_time:98529ms step_avg:713.98ms
step:139/2285 train_time:99247ms step_avg:714.01ms
step:140/2285 train_time:99972ms step_avg:714.09ms
step:141/2285 train_time:100690ms step_avg:714.11ms
step:142/2285 train_time:101416ms step_avg:714.19ms
step:143/2285 train_time:102134ms step_avg:714.23ms
step:144/2285 train_time:102859ms step_avg:714.30ms
step:145/2285 train_time:103577ms step_avg:714.33ms
step:146/2285 train_time:104303ms step_avg:714.40ms
step:147/2285 train_time:105020ms step_avg:714.42ms
step:148/2285 train_time:105745ms step_avg:714.49ms
step:149/2285 train_time:106466ms step_avg:714.53ms
step:150/2285 train_time:107188ms step_avg:714.59ms
step:151/2285 train_time:107906ms step_avg:714.61ms
step:152/2285 train_time:108631ms step_avg:714.68ms
step:153/2285 train_time:109348ms step_avg:714.70ms
step:154/2285 train_time:110073ms step_avg:714.76ms
step:155/2285 train_time:110791ms step_avg:714.78ms
step:156/2285 train_time:111517ms step_avg:714.85ms
step:157/2285 train_time:112236ms step_avg:714.88ms
step:158/2285 train_time:112959ms step_avg:714.93ms
step:159/2285 train_time:113678ms step_avg:714.95ms
step:160/2285 train_time:114401ms step_avg:715.01ms
step:161/2285 train_time:115118ms step_avg:715.02ms
step:162/2285 train_time:115841ms step_avg:715.07ms
step:163/2285 train_time:116560ms step_avg:715.09ms
step:164/2285 train_time:117283ms step_avg:715.14ms
step:165/2285 train_time:117999ms step_avg:715.15ms
step:166/2285 train_time:118724ms step_avg:715.21ms
step:167/2285 train_time:119442ms step_avg:715.22ms
step:168/2285 train_time:120165ms step_avg:715.27ms
step:169/2285 train_time:120883ms step_avg:715.28ms
step:170/2285 train_time:121606ms step_avg:715.33ms
step:171/2285 train_time:122323ms step_avg:715.34ms
step:172/2285 train_time:123046ms step_avg:715.39ms
step:173/2285 train_time:123765ms step_avg:715.41ms
step:174/2285 train_time:124486ms step_avg:715.44ms
step:175/2285 train_time:125203ms step_avg:715.45ms
step:176/2285 train_time:125925ms step_avg:715.48ms
step:177/2285 train_time:126642ms step_avg:715.49ms
step:178/2285 train_time:127365ms step_avg:715.53ms
step:179/2285 train_time:128081ms step_avg:715.54ms
step:180/2285 train_time:128804ms step_avg:715.58ms
step:181/2285 train_time:129522ms step_avg:715.59ms
step:182/2285 train_time:130245ms step_avg:715.63ms
step:183/2285 train_time:130962ms step_avg:715.64ms
step:184/2285 train_time:131684ms step_avg:715.67ms
step:185/2285 train_time:132401ms step_avg:715.68ms
step:186/2285 train_time:133123ms step_avg:715.72ms
step:187/2285 train_time:133841ms step_avg:715.73ms
step:188/2285 train_time:134564ms step_avg:715.77ms
step:189/2285 train_time:135281ms step_avg:715.77ms
step:190/2285 train_time:136003ms step_avg:715.81ms
step:191/2285 train_time:136721ms step_avg:715.82ms
step:192/2285 train_time:137444ms step_avg:715.86ms
step:193/2285 train_time:138162ms step_avg:715.87ms
step:194/2285 train_time:138885ms step_avg:715.90ms
step:195/2285 train_time:139602ms step_avg:715.91ms
step:196/2285 train_time:140324ms step_avg:715.94ms
step:197/2285 train_time:141041ms step_avg:715.95ms
step:198/2285 train_time:141764ms step_avg:715.98ms
step:199/2285 train_time:142480ms step_avg:715.98ms
step:200/2285 train_time:143204ms step_avg:716.02ms
step:200/2285 val_loss:4.3694 train_time:143301ms step_avg:716.50ms
step:201/2285 train_time:143921ms step_avg:716.02ms
step:202/2285 train_time:144645ms step_avg:716.06ms
step:203/2285 train_time:145363ms step_avg:716.07ms
step:204/2285 train_time:146085ms step_avg:716.10ms
step:205/2285 train_time:146804ms step_avg:716.12ms
step:206/2285 train_time:147526ms step_avg:716.15ms
step:207/2285 train_time:148243ms step_avg:716.15ms
step:208/2285 train_time:148965ms step_avg:716.18ms
step:209/2285 train_time:149684ms step_avg:716.19ms
step:210/2285 train_time:150406ms step_avg:716.22ms
step:211/2285 train_time:151124ms step_avg:716.23ms
step:212/2285 train_time:151846ms step_avg:716.26ms
step:213/2285 train_time:152565ms step_avg:716.27ms
step:214/2285 train_time:153287ms step_avg:716.29ms
step:215/2285 train_time:154005ms step_avg:716.30ms
step:216/2285 train_time:154728ms step_avg:716.33ms
step:217/2285 train_time:155446ms step_avg:716.34ms
step:218/2285 train_time:156168ms step_avg:716.37ms
step:219/2285 train_time:156886ms step_avg:716.38ms
step:220/2285 train_time:157607ms step_avg:716.40ms
step:221/2285 train_time:158325ms step_avg:716.40ms
step:222/2285 train_time:159047ms step_avg:716.43ms
step:223/2285 train_time:159766ms step_avg:716.44ms
step:224/2285 train_time:160488ms step_avg:716.46ms
step:225/2285 train_time:161204ms step_avg:716.46ms
step:226/2285 train_time:161927ms step_avg:716.49ms
step:227/2285 train_time:162646ms step_avg:716.50ms
step:228/2285 train_time:163367ms step_avg:716.52ms
step:229/2285 train_time:164084ms step_avg:716.53ms
step:230/2285 train_time:164806ms step_avg:716.55ms
step:231/2285 train_time:165524ms step_avg:716.55ms
step:232/2285 train_time:166245ms step_avg:716.57ms
step:233/2285 train_time:166965ms step_avg:716.59ms
step:234/2285 train_time:167686ms step_avg:716.60ms
step:235/2285 train_time:168402ms step_avg:716.61ms
step:236/2285 train_time:169125ms step_avg:716.63ms
step:237/2285 train_time:169845ms step_avg:716.64ms
step:238/2285 train_time:170565ms step_avg:716.66ms
step:239/2285 train_time:171282ms step_avg:716.66ms
step:240/2285 train_time:172005ms step_avg:716.69ms
step:241/2285 train_time:172722ms step_avg:716.69ms
step:242/2285 train_time:173445ms step_avg:716.71ms
step:243/2285 train_time:174164ms step_avg:716.73ms
step:244/2285 train_time:174887ms step_avg:716.75ms
step:245/2285 train_time:175603ms step_avg:716.75ms
step:246/2285 train_time:176326ms step_avg:716.77ms
step:247/2285 train_time:177045ms step_avg:716.78ms
step:248/2285 train_time:177767ms step_avg:716.80ms
step:249/2285 train_time:178484ms step_avg:716.80ms
step:250/2285 train_time:179204ms step_avg:716.82ms
step:251/2285 train_time:179923ms step_avg:716.83ms
step:252/2285 train_time:180646ms step_avg:716.85ms
step:253/2285 train_time:181365ms step_avg:716.86ms
step:254/2285 train_time:182085ms step_avg:716.87ms
step:255/2285 train_time:182805ms step_avg:716.88ms
step:256/2285 train_time:183529ms step_avg:716.91ms
step:257/2285 train_time:184248ms step_avg:716.92ms
step:258/2285 train_time:184970ms step_avg:716.94ms
step:259/2285 train_time:185686ms step_avg:716.93ms
step:260/2285 train_time:186409ms step_avg:716.96ms
step:261/2285 train_time:187127ms step_avg:716.96ms
step:262/2285 train_time:187850ms step_avg:716.98ms
step:263/2285 train_time:188567ms step_avg:716.99ms
step:264/2285 train_time:189290ms step_avg:717.01ms
step:265/2285 train_time:190007ms step_avg:717.01ms
step:266/2285 train_time:190731ms step_avg:717.03ms
step:267/2285 train_time:191448ms step_avg:717.03ms
step:268/2285 train_time:192172ms step_avg:717.06ms
step:269/2285 train_time:192889ms step_avg:717.06ms
step:270/2285 train_time:193613ms step_avg:717.08ms
step:271/2285 train_time:194330ms step_avg:717.08ms
step:272/2285 train_time:195053ms step_avg:717.11ms
step:273/2285 train_time:195772ms step_avg:717.11ms
step:274/2285 train_time:196495ms step_avg:717.13ms
step:275/2285 train_time:197212ms step_avg:717.13ms
step:276/2285 train_time:197936ms step_avg:717.16ms
step:277/2285 train_time:198656ms step_avg:717.17ms
step:278/2285 train_time:199379ms step_avg:717.19ms
step:279/2285 train_time:200096ms step_avg:717.19ms
step:280/2285 train_time:200821ms step_avg:717.22ms
step:281/2285 train_time:201538ms step_avg:717.22ms
step:282/2285 train_time:202260ms step_avg:717.23ms
step:283/2285 train_time:202980ms step_avg:717.25ms
step:284/2285 train_time:203704ms step_avg:717.27ms
step:285/2285 train_time:204422ms step_avg:717.27ms
step:286/2285 train_time:205147ms step_avg:717.30ms
step:287/2285 train_time:205865ms step_avg:717.30ms
step:288/2285 train_time:206586ms step_avg:717.31ms
step:289/2285 train_time:207303ms step_avg:717.31ms
step:290/2285 train_time:208025ms step_avg:717.33ms
step:291/2285 train_time:208743ms step_avg:717.33ms
step:292/2285 train_time:209465ms step_avg:717.35ms
step:293/2285 train_time:210183ms step_avg:717.35ms
step:294/2285 train_time:210906ms step_avg:717.37ms
step:295/2285 train_time:211624ms step_avg:717.37ms
step:296/2285 train_time:212347ms step_avg:717.39ms
step:297/2285 train_time:213063ms step_avg:717.38ms
step:298/2285 train_time:213786ms step_avg:717.40ms
step:299/2285 train_time:214504ms step_avg:717.40ms
step:300/2285 train_time:215226ms step_avg:717.42ms
step:300/2285 val_loss:4.1651 train_time:215324ms step_avg:717.75ms
step:301/2285 train_time:215943ms step_avg:717.42ms
step:302/2285 train_time:216668ms step_avg:717.44ms
step:303/2285 train_time:217383ms step_avg:717.43ms
step:304/2285 train_time:218105ms step_avg:717.45ms
step:305/2285 train_time:218824ms step_avg:717.46ms
step:306/2285 train_time:219547ms step_avg:717.48ms
step:307/2285 train_time:220263ms step_avg:717.47ms
step:308/2285 train_time:220984ms step_avg:717.48ms
step:309/2285 train_time:221705ms step_avg:717.49ms
step:310/2285 train_time:222427ms step_avg:717.51ms
step:311/2285 train_time:223144ms step_avg:717.50ms
step:312/2285 train_time:223867ms step_avg:717.52ms
step:313/2285 train_time:224584ms step_avg:717.52ms
step:314/2285 train_time:225308ms step_avg:717.54ms
step:315/2285 train_time:226025ms step_avg:717.54ms
step:316/2285 train_time:226748ms step_avg:717.56ms
step:317/2285 train_time:227464ms step_avg:717.55ms
step:318/2285 train_time:228185ms step_avg:717.56ms
step:319/2285 train_time:228903ms step_avg:717.56ms
step:320/2285 train_time:229626ms step_avg:717.58ms
step:321/2285 train_time:230343ms step_avg:717.58ms
step:322/2285 train_time:231066ms step_avg:717.59ms
step:323/2285 train_time:231781ms step_avg:717.59ms
step:324/2285 train_time:232505ms step_avg:717.61ms
step:325/2285 train_time:233221ms step_avg:717.60ms
step:326/2285 train_time:233944ms step_avg:717.62ms
step:327/2285 train_time:234659ms step_avg:717.61ms
step:328/2285 train_time:235383ms step_avg:717.63ms
step:329/2285 train_time:236102ms step_avg:717.63ms
step:330/2285 train_time:236823ms step_avg:717.64ms
step:331/2285 train_time:237539ms step_avg:717.64ms
step:332/2285 train_time:238262ms step_avg:717.66ms
step:333/2285 train_time:238980ms step_avg:717.66ms
step:334/2285 train_time:239703ms step_avg:717.67ms
step:335/2285 train_time:240421ms step_avg:717.67ms
step:336/2285 train_time:241143ms step_avg:717.69ms
step:337/2285 train_time:241860ms step_avg:717.68ms
step:338/2285 train_time:242583ms step_avg:717.70ms
step:339/2285 train_time:243301ms step_avg:717.70ms
step:340/2285 train_time:244024ms step_avg:717.72ms
step:341/2285 train_time:244741ms step_avg:717.72ms
step:342/2285 train_time:245463ms step_avg:717.73ms
step:343/2285 train_time:246179ms step_avg:717.72ms
step:344/2285 train_time:246902ms step_avg:717.74ms
step:345/2285 train_time:247620ms step_avg:717.74ms
step:346/2285 train_time:248344ms step_avg:717.76ms
step:347/2285 train_time:249059ms step_avg:717.75ms
step:348/2285 train_time:249784ms step_avg:717.77ms
step:349/2285 train_time:250503ms step_avg:717.77ms
step:350/2285 train_time:251225ms step_avg:717.79ms
step:351/2285 train_time:251943ms step_avg:717.79ms
step:352/2285 train_time:252666ms step_avg:717.80ms
step:353/2285 train_time:253381ms step_avg:717.79ms
step:354/2285 train_time:254105ms step_avg:717.81ms
step:355/2285 train_time:254823ms step_avg:717.81ms
step:356/2285 train_time:255546ms step_avg:717.83ms
step:357/2285 train_time:256263ms step_avg:717.82ms
step:358/2285 train_time:256985ms step_avg:717.83ms
step:359/2285 train_time:257701ms step_avg:717.83ms
step:360/2285 train_time:258423ms step_avg:717.84ms
step:361/2285 train_time:259142ms step_avg:717.84ms
step:362/2285 train_time:259864ms step_avg:717.86ms
step:363/2285 train_time:260580ms step_avg:717.85ms
step:364/2285 train_time:261304ms step_avg:717.87ms
step:365/2285 train_time:262022ms step_avg:717.87ms
step:366/2285 train_time:262745ms step_avg:717.88ms
step:367/2285 train_time:263461ms step_avg:717.88ms
step:368/2285 train_time:264183ms step_avg:717.89ms
step:369/2285 train_time:264902ms step_avg:717.89ms
step:370/2285 train_time:265626ms step_avg:717.91ms
step:371/2285 train_time:266344ms step_avg:717.91ms
step:372/2285 train_time:267066ms step_avg:717.92ms
step:373/2285 train_time:267783ms step_avg:717.92ms
step:374/2285 train_time:268506ms step_avg:717.93ms
step:375/2285 train_time:269223ms step_avg:717.93ms
step:376/2285 train_time:269947ms step_avg:717.94ms
step:377/2285 train_time:270662ms step_avg:717.94ms
step:378/2285 train_time:271385ms step_avg:717.95ms
step:379/2285 train_time:272101ms step_avg:717.94ms
step:380/2285 train_time:272824ms step_avg:717.96ms
step:381/2285 train_time:273540ms step_avg:717.95ms
step:382/2285 train_time:274262ms step_avg:717.96ms
step:383/2285 train_time:274979ms step_avg:717.96ms
step:384/2285 train_time:275703ms step_avg:717.98ms
step:385/2285 train_time:276420ms step_avg:717.97ms
step:386/2285 train_time:277143ms step_avg:717.99ms
step:387/2285 train_time:277859ms step_avg:717.98ms
step:388/2285 train_time:278581ms step_avg:717.99ms
step:389/2285 train_time:279297ms step_avg:717.99ms
step:390/2285 train_time:280021ms step_avg:718.00ms
step:391/2285 train_time:280738ms step_avg:718.00ms
step:392/2285 train_time:281461ms step_avg:718.01ms
step:393/2285 train_time:282178ms step_avg:718.01ms
step:394/2285 train_time:282903ms step_avg:718.03ms
step:395/2285 train_time:283619ms step_avg:718.02ms
step:396/2285 train_time:284344ms step_avg:718.04ms
step:397/2285 train_time:285059ms step_avg:718.03ms
step:398/2285 train_time:285783ms step_avg:718.05ms
step:399/2285 train_time:286498ms step_avg:718.04ms
step:400/2285 train_time:287222ms step_avg:718.05ms
step:400/2285 val_loss:4.0344 train_time:287318ms step_avg:718.30ms
step:401/2285 train_time:287936ms step_avg:718.05ms
step:402/2285 train_time:288661ms step_avg:718.06ms
step:403/2285 train_time:289375ms step_avg:718.05ms
step:404/2285 train_time:290098ms step_avg:718.06ms
step:405/2285 train_time:290814ms step_avg:718.06ms
step:406/2285 train_time:291535ms step_avg:718.07ms
step:407/2285 train_time:292252ms step_avg:718.06ms
step:408/2285 train_time:292975ms step_avg:718.07ms
step:409/2285 train_time:293689ms step_avg:718.07ms
step:410/2285 train_time:294410ms step_avg:718.07ms
step:411/2285 train_time:295128ms step_avg:718.07ms
step:412/2285 train_time:295848ms step_avg:718.08ms
step:413/2285 train_time:296563ms step_avg:718.07ms
step:414/2285 train_time:297287ms step_avg:718.08ms
step:415/2285 train_time:298002ms step_avg:718.08ms
step:416/2285 train_time:298724ms step_avg:718.09ms
step:417/2285 train_time:299440ms step_avg:718.08ms
step:418/2285 train_time:300161ms step_avg:718.09ms
step:419/2285 train_time:300876ms step_avg:718.08ms
step:420/2285 train_time:301597ms step_avg:718.09ms
step:421/2285 train_time:302315ms step_avg:718.09ms
step:422/2285 train_time:303038ms step_avg:718.10ms
step:423/2285 train_time:303754ms step_avg:718.10ms
step:424/2285 train_time:304476ms step_avg:718.10ms
step:425/2285 train_time:305193ms step_avg:718.10ms
step:426/2285 train_time:305915ms step_avg:718.11ms
step:427/2285 train_time:306630ms step_avg:718.10ms
step:428/2285 train_time:307349ms step_avg:718.10ms
step:429/2285 train_time:308066ms step_avg:718.10ms
step:430/2285 train_time:308787ms step_avg:718.11ms
step:431/2285 train_time:309502ms step_avg:718.10ms
step:432/2285 train_time:310226ms step_avg:718.11ms
step:433/2285 train_time:310942ms step_avg:718.11ms
step:434/2285 train_time:311663ms step_avg:718.12ms
step:435/2285 train_time:312379ms step_avg:718.11ms
step:436/2285 train_time:313102ms step_avg:718.12ms
step:437/2285 train_time:313818ms step_avg:718.12ms
step:438/2285 train_time:314540ms step_avg:718.13ms
step:439/2285 train_time:315255ms step_avg:718.12ms
step:440/2285 train_time:315974ms step_avg:718.12ms
step:441/2285 train_time:316688ms step_avg:718.11ms
step:442/2285 train_time:317409ms step_avg:718.12ms
step:443/2285 train_time:318124ms step_avg:718.11ms
step:444/2285 train_time:318846ms step_avg:718.12ms
step:445/2285 train_time:319562ms step_avg:718.12ms
step:446/2285 train_time:320286ms step_avg:718.13ms
step:447/2285 train_time:321000ms step_avg:718.12ms
step:448/2285 train_time:321720ms step_avg:718.12ms
step:449/2285 train_time:322437ms step_avg:718.12ms
step:450/2285 train_time:323158ms step_avg:718.13ms
step:451/2285 train_time:323872ms step_avg:718.12ms
step:452/2285 train_time:324593ms step_avg:718.13ms
step:453/2285 train_time:325309ms step_avg:718.12ms
step:454/2285 train_time:326028ms step_avg:718.12ms
step:455/2285 train_time:326744ms step_avg:718.12ms
step:456/2285 train_time:327466ms step_avg:718.13ms
step:457/2285 train_time:328180ms step_avg:718.12ms
step:458/2285 train_time:328901ms step_avg:718.12ms
step:459/2285 train_time:329617ms step_avg:718.12ms
step:460/2285 train_time:330338ms step_avg:718.13ms
step:461/2285 train_time:331054ms step_avg:718.12ms
step:462/2285 train_time:331775ms step_avg:718.13ms
step:463/2285 train_time:332488ms step_avg:718.12ms
step:464/2285 train_time:333208ms step_avg:718.12ms
step:465/2285 train_time:333924ms step_avg:718.12ms
step:466/2285 train_time:334646ms step_avg:718.12ms
step:467/2285 train_time:335361ms step_avg:718.12ms
step:468/2285 train_time:336082ms step_avg:718.12ms
step:469/2285 train_time:336797ms step_avg:718.12ms
step:470/2285 train_time:337520ms step_avg:718.13ms
step:471/2285 train_time:338235ms step_avg:718.12ms
step:472/2285 train_time:338957ms step_avg:718.13ms
step:473/2285 train_time:339672ms step_avg:718.12ms
step:474/2285 train_time:340392ms step_avg:718.13ms
step:475/2285 train_time:341107ms step_avg:718.12ms
step:476/2285 train_time:341828ms step_avg:718.13ms
step:477/2285 train_time:342544ms step_avg:718.12ms
step:478/2285 train_time:343266ms step_avg:718.13ms
step:479/2285 train_time:343980ms step_avg:718.12ms
step:480/2285 train_time:344702ms step_avg:718.13ms
step:481/2285 train_time:345417ms step_avg:718.12ms
step:482/2285 train_time:346139ms step_avg:718.13ms
step:483/2285 train_time:346854ms step_avg:718.12ms
step:484/2285 train_time:347577ms step_avg:718.13ms
step:485/2285 train_time:348291ms step_avg:718.13ms
step:486/2285 train_time:349012ms step_avg:718.13ms
step:487/2285 train_time:349729ms step_avg:718.13ms
step:488/2285 train_time:350448ms step_avg:718.13ms
step:489/2285 train_time:351164ms step_avg:718.13ms
step:490/2285 train_time:351886ms step_avg:718.14ms
step:491/2285 train_time:352601ms step_avg:718.13ms
step:492/2285 train_time:353323ms step_avg:718.14ms
step:493/2285 train_time:354038ms step_avg:718.13ms
step:494/2285 train_time:354761ms step_avg:718.14ms
step:495/2285 train_time:355476ms step_avg:718.13ms
step:496/2285 train_time:356199ms step_avg:718.14ms
step:497/2285 train_time:356915ms step_avg:718.14ms
step:498/2285 train_time:357637ms step_avg:718.15ms
step:499/2285 train_time:358353ms step_avg:718.14ms
step:500/2285 train_time:359075ms step_avg:718.15ms
step:500/2285 val_loss:3.9473 train_time:359171ms step_avg:718.34ms
step:501/2285 train_time:359790ms step_avg:718.14ms
step:502/2285 train_time:360512ms step_avg:718.15ms
step:503/2285 train_time:361228ms step_avg:718.15ms
step:504/2285 train_time:361950ms step_avg:718.15ms
step:505/2285 train_time:362667ms step_avg:718.15ms
step:506/2285 train_time:363389ms step_avg:718.16ms
step:507/2285 train_time:364104ms step_avg:718.15ms
step:508/2285 train_time:364827ms step_avg:718.16ms
step:509/2285 train_time:365541ms step_avg:718.16ms
step:510/2285 train_time:366262ms step_avg:718.16ms
step:511/2285 train_time:366977ms step_avg:718.16ms
step:512/2285 train_time:367700ms step_avg:718.16ms
step:513/2285 train_time:368415ms step_avg:718.16ms
step:514/2285 train_time:369136ms step_avg:718.16ms
step:515/2285 train_time:369850ms step_avg:718.16ms
step:516/2285 train_time:370572ms step_avg:718.16ms
step:517/2285 train_time:371288ms step_avg:718.16ms
step:518/2285 train_time:372010ms step_avg:718.17ms
step:519/2285 train_time:372725ms step_avg:718.16ms
step:520/2285 train_time:373448ms step_avg:718.17ms
step:521/2285 train_time:374165ms step_avg:718.17ms
step:522/2285 train_time:374889ms step_avg:718.18ms
step:523/2285 train_time:375604ms step_avg:718.17ms
step:524/2285 train_time:376326ms step_avg:718.18ms
step:525/2285 train_time:377041ms step_avg:718.17ms
step:526/2285 train_time:377764ms step_avg:718.18ms
step:527/2285 train_time:378478ms step_avg:718.17ms
step:528/2285 train_time:379202ms step_avg:718.19ms
step:529/2285 train_time:379917ms step_avg:718.18ms
step:530/2285 train_time:380639ms step_avg:718.19ms
step:531/2285 train_time:381354ms step_avg:718.18ms
step:532/2285 train_time:382077ms step_avg:718.19ms
step:533/2285 train_time:382793ms step_avg:718.19ms
step:534/2285 train_time:383512ms step_avg:718.19ms
step:535/2285 train_time:384228ms step_avg:718.18ms
step:536/2285 train_time:384949ms step_avg:718.19ms
step:537/2285 train_time:385665ms step_avg:718.18ms
step:538/2285 train_time:386387ms step_avg:718.19ms
step:539/2285 train_time:387102ms step_avg:718.19ms
step:540/2285 train_time:387823ms step_avg:718.19ms
step:541/2285 train_time:388539ms step_avg:718.19ms
step:542/2285 train_time:389261ms step_avg:718.19ms
step:543/2285 train_time:389977ms step_avg:718.19ms
step:544/2285 train_time:390698ms step_avg:718.20ms
step:545/2285 train_time:391412ms step_avg:718.19ms
step:546/2285 train_time:392132ms step_avg:718.19ms
step:547/2285 train_time:392847ms step_avg:718.19ms
step:548/2285 train_time:393570ms step_avg:718.19ms
step:549/2285 train_time:394285ms step_avg:718.19ms
step:550/2285 train_time:395006ms step_avg:718.19ms
step:551/2285 train_time:395722ms step_avg:718.19ms
step:552/2285 train_time:396444ms step_avg:718.20ms
step:553/2285 train_time:397158ms step_avg:718.19ms
step:554/2285 train_time:397879ms step_avg:718.19ms
step:555/2285 train_time:398595ms step_avg:718.19ms
step:556/2285 train_time:399315ms step_avg:718.19ms
step:557/2285 train_time:400032ms step_avg:718.19ms
step:558/2285 train_time:400752ms step_avg:718.19ms
step:559/2285 train_time:401469ms step_avg:718.19ms
step:560/2285 train_time:402191ms step_avg:718.20ms
step:561/2285 train_time:402907ms step_avg:718.19ms
step:562/2285 train_time:403630ms step_avg:718.20ms
step:563/2285 train_time:404345ms step_avg:718.20ms
step:564/2285 train_time:405067ms step_avg:718.20ms
step:565/2285 train_time:405781ms step_avg:718.20ms
step:566/2285 train_time:406503ms step_avg:718.20ms
step:567/2285 train_time:407219ms step_avg:718.20ms
step:568/2285 train_time:407941ms step_avg:718.21ms
step:569/2285 train_time:408657ms step_avg:718.20ms
step:570/2285 train_time:409379ms step_avg:718.21ms
step:571/2285 train_time:410094ms step_avg:718.20ms
step:572/2285 train_time:410814ms step_avg:718.21ms
step:573/2285 train_time:411531ms step_avg:718.20ms
step:574/2285 train_time:412251ms step_avg:718.21ms
step:575/2285 train_time:412967ms step_avg:718.20ms
step:576/2285 train_time:413690ms step_avg:718.21ms
step:577/2285 train_time:414405ms step_avg:718.21ms
step:578/2285 train_time:415128ms step_avg:718.21ms
step:579/2285 train_time:415845ms step_avg:718.21ms
step:580/2285 train_time:416566ms step_avg:718.22ms
step:581/2285 train_time:417280ms step_avg:718.21ms
step:582/2285 train_time:418002ms step_avg:718.22ms
step:583/2285 train_time:418717ms step_avg:718.21ms
step:584/2285 train_time:419437ms step_avg:718.21ms
step:585/2285 train_time:420151ms step_avg:718.21ms
step:586/2285 train_time:420871ms step_avg:718.21ms
step:587/2285 train_time:421588ms step_avg:718.21ms
step:588/2285 train_time:422308ms step_avg:718.21ms
step:589/2285 train_time:423025ms step_avg:718.21ms
step:590/2285 train_time:423745ms step_avg:718.21ms
step:591/2285 train_time:424465ms step_avg:718.22ms
step:592/2285 train_time:425183ms step_avg:718.21ms
step:593/2285 train_time:425898ms step_avg:718.21ms
step:594/2285 train_time:426619ms step_avg:718.21ms
step:595/2285 train_time:427335ms step_avg:718.21ms
step:596/2285 train_time:428056ms step_avg:718.21ms
step:597/2285 train_time:428773ms step_avg:718.21ms
step:598/2285 train_time:429492ms step_avg:718.21ms
step:599/2285 train_time:430209ms step_avg:718.21ms
step:600/2285 train_time:430931ms step_avg:718.22ms
step:600/2285 val_loss:3.8850 train_time:431028ms step_avg:718.38ms
step:601/2285 train_time:431648ms step_avg:718.22ms
step:602/2285 train_time:432370ms step_avg:718.22ms
step:603/2285 train_time:433084ms step_avg:718.22ms
step:604/2285 train_time:433805ms step_avg:718.22ms
step:605/2285 train_time:434520ms step_avg:718.21ms
step:606/2285 train_time:435243ms step_avg:718.22ms
step:607/2285 train_time:435957ms step_avg:718.22ms
step:608/2285 train_time:436678ms step_avg:718.22ms
step:609/2285 train_time:437393ms step_avg:718.22ms
step:610/2285 train_time:438117ms step_avg:718.22ms
step:611/2285 train_time:438834ms step_avg:718.22ms
step:612/2285 train_time:439556ms step_avg:718.23ms
step:613/2285 train_time:440272ms step_avg:718.23ms
step:614/2285 train_time:440995ms step_avg:718.23ms
step:615/2285 train_time:441711ms step_avg:718.23ms
step:616/2285 train_time:442434ms step_avg:718.24ms
step:617/2285 train_time:443149ms step_avg:718.23ms
step:618/2285 train_time:443870ms step_avg:718.24ms
step:619/2285 train_time:444585ms step_avg:718.23ms
step:620/2285 train_time:445307ms step_avg:718.24ms
step:621/2285 train_time:446022ms step_avg:718.23ms
step:622/2285 train_time:446744ms step_avg:718.24ms
step:623/2285 train_time:447459ms step_avg:718.23ms
step:624/2285 train_time:448182ms step_avg:718.24ms
step:625/2285 train_time:448898ms step_avg:718.24ms
step:626/2285 train_time:449619ms step_avg:718.24ms
step:627/2285 train_time:450335ms step_avg:718.24ms
step:628/2285 train_time:451056ms step_avg:718.24ms
step:629/2285 train_time:451771ms step_avg:718.24ms
step:630/2285 train_time:452492ms step_avg:718.24ms
step:631/2285 train_time:453206ms step_avg:718.24ms
step:632/2285 train_time:453928ms step_avg:718.24ms
step:633/2285 train_time:454645ms step_avg:718.24ms
step:634/2285 train_time:455365ms step_avg:718.24ms
step:635/2285 train_time:456083ms step_avg:718.24ms
step:636/2285 train_time:456805ms step_avg:718.25ms
step:637/2285 train_time:457520ms step_avg:718.24ms
step:638/2285 train_time:458243ms step_avg:718.25ms
step:639/2285 train_time:458961ms step_avg:718.25ms
step:640/2285 train_time:459683ms step_avg:718.26ms
step:641/2285 train_time:460399ms step_avg:718.25ms
step:642/2285 train_time:461121ms step_avg:718.26ms
step:643/2285 train_time:461837ms step_avg:718.25ms
step:644/2285 train_time:462558ms step_avg:718.26ms
step:645/2285 train_time:463274ms step_avg:718.25ms
step:646/2285 train_time:463997ms step_avg:718.26ms
step:647/2285 train_time:464711ms step_avg:718.26ms
step:648/2285 train_time:465433ms step_avg:718.26ms
step:649/2285 train_time:466148ms step_avg:718.26ms
step:650/2285 train_time:466866ms step_avg:718.26ms
step:651/2285 train_time:467582ms step_avg:718.25ms
step:652/2285 train_time:468303ms step_avg:718.26ms
step:653/2285 train_time:469021ms step_avg:718.26ms
step:654/2285 train_time:469744ms step_avg:718.26ms
step:655/2285 train_time:470458ms step_avg:718.26ms
step:656/2285 train_time:471179ms step_avg:718.26ms
step:657/2285 train_time:471896ms step_avg:718.26ms
step:658/2285 train_time:472617ms step_avg:718.26ms
step:659/2285 train_time:473333ms step_avg:718.26ms
step:660/2285 train_time:474055ms step_avg:718.27ms
step:661/2285 train_time:474771ms step_avg:718.26ms
step:662/2285 train_time:475495ms step_avg:718.27ms
step:663/2285 train_time:476212ms step_avg:718.27ms
step:664/2285 train_time:476933ms step_avg:718.27ms
step:665/2285 train_time:477650ms step_avg:718.27ms
step:666/2285 train_time:478371ms step_avg:718.28ms
step:667/2285 train_time:479085ms step_avg:718.27ms
step:668/2285 train_time:479805ms step_avg:718.27ms
step:669/2285 train_time:480521ms step_avg:718.27ms
step:670/2285 train_time:481243ms step_avg:718.27ms
step:671/2285 train_time:481957ms step_avg:718.27ms
step:672/2285 train_time:482679ms step_avg:718.27ms
step:673/2285 train_time:483394ms step_avg:718.27ms
step:674/2285 train_time:484115ms step_avg:718.27ms
step:675/2285 train_time:484831ms step_avg:718.27ms
step:676/2285 train_time:485555ms step_avg:718.28ms
step:677/2285 train_time:486272ms step_avg:718.28ms
step:678/2285 train_time:486993ms step_avg:718.28ms
step:679/2285 train_time:487712ms step_avg:718.28ms
step:680/2285 train_time:488434ms step_avg:718.29ms
step:681/2285 train_time:489151ms step_avg:718.28ms
step:682/2285 train_time:489874ms step_avg:718.29ms
step:683/2285 train_time:490591ms step_avg:718.29ms
step:684/2285 train_time:491313ms step_avg:718.29ms
step:685/2285 train_time:492028ms step_avg:718.29ms
step:686/2285 train_time:492751ms step_avg:718.30ms
step:687/2285 train_time:493466ms step_avg:718.29ms
step:688/2285 train_time:494186ms step_avg:718.29ms
step:689/2285 train_time:494900ms step_avg:718.29ms
step:690/2285 train_time:495622ms step_avg:718.29ms
step:691/2285 train_time:496338ms step_avg:718.29ms
step:692/2285 train_time:497059ms step_avg:718.29ms
step:693/2285 train_time:497774ms step_avg:718.29ms
step:694/2285 train_time:498496ms step_avg:718.29ms
step:695/2285 train_time:499212ms step_avg:718.29ms
step:696/2285 train_time:499935ms step_avg:718.30ms
step:697/2285 train_time:500651ms step_avg:718.29ms
step:698/2285 train_time:501373ms step_avg:718.30ms
step:699/2285 train_time:502087ms step_avg:718.29ms
step:700/2285 train_time:502808ms step_avg:718.30ms
step:700/2285 val_loss:3.8340 train_time:502904ms step_avg:718.43ms
step:701/2285 train_time:503521ms step_avg:718.29ms
step:702/2285 train_time:504245ms step_avg:718.30ms
step:703/2285 train_time:504961ms step_avg:718.29ms
step:704/2285 train_time:505683ms step_avg:718.30ms
step:705/2285 train_time:506399ms step_avg:718.30ms
step:706/2285 train_time:507121ms step_avg:718.30ms
step:707/2285 train_time:508678ms step_avg:719.49ms
step:708/2285 train_time:509369ms step_avg:719.45ms
step:709/2285 train_time:510091ms step_avg:719.45ms
step:710/2285 train_time:510818ms step_avg:719.46ms
step:711/2285 train_time:511527ms step_avg:719.45ms
step:712/2285 train_time:512248ms step_avg:719.45ms
step:713/2285 train_time:512966ms step_avg:719.45ms
step:714/2285 train_time:513687ms step_avg:719.45ms
step:715/2285 train_time:514400ms step_avg:719.44ms
step:716/2285 train_time:515123ms step_avg:719.44ms
step:717/2285 train_time:515839ms step_avg:719.44ms
step:718/2285 train_time:516558ms step_avg:719.44ms
step:719/2285 train_time:517274ms step_avg:719.44ms
step:720/2285 train_time:517997ms step_avg:719.44ms
step:721/2285 train_time:518713ms step_avg:719.44ms
step:722/2285 train_time:519436ms step_avg:719.44ms
step:723/2285 train_time:520152ms step_avg:719.44ms
step:724/2285 train_time:520874ms step_avg:719.44ms
step:725/2285 train_time:521591ms step_avg:719.44ms
step:726/2285 train_time:522312ms step_avg:719.44ms
step:727/2285 train_time:523028ms step_avg:719.43ms
step:728/2285 train_time:523749ms step_avg:719.44ms
step:729/2285 train_time:524466ms step_avg:719.43ms
step:730/2285 train_time:525188ms step_avg:719.44ms
step:731/2285 train_time:525904ms step_avg:719.43ms
step:732/2285 train_time:526627ms step_avg:719.44ms
step:733/2285 train_time:527343ms step_avg:719.43ms
step:734/2285 train_time:528064ms step_avg:719.43ms
step:735/2285 train_time:528780ms step_avg:719.43ms
step:736/2285 train_time:529498ms step_avg:719.43ms
step:737/2285 train_time:530215ms step_avg:719.42ms
step:738/2285 train_time:530936ms step_avg:719.43ms
step:739/2285 train_time:531652ms step_avg:719.42ms
step:740/2285 train_time:532375ms step_avg:719.43ms
step:741/2285 train_time:533090ms step_avg:719.42ms
step:742/2285 train_time:533812ms step_avg:719.42ms
step:743/2285 train_time:534528ms step_avg:719.42ms
step:744/2285 train_time:535249ms step_avg:719.42ms
step:745/2285 train_time:535966ms step_avg:719.42ms
step:746/2285 train_time:536687ms step_avg:719.42ms
step:747/2285 train_time:537402ms step_avg:719.41ms
step:748/2285 train_time:538123ms step_avg:719.42ms
step:749/2285 train_time:538839ms step_avg:719.41ms
step:750/2285 train_time:539569ms step_avg:719.43ms
step:751/2285 train_time:540294ms step_avg:719.43ms
step:752/2285 train_time:541027ms step_avg:719.45ms
step:753/2285 train_time:541750ms step_avg:719.46ms
step:754/2285 train_time:542482ms step_avg:719.47ms
step:755/2285 train_time:543205ms step_avg:719.48ms
step:756/2285 train_time:543937ms step_avg:719.49ms
step:757/2285 train_time:544664ms step_avg:719.50ms
step:758/2285 train_time:545397ms step_avg:719.52ms
step:759/2285 train_time:546122ms step_avg:719.53ms
step:760/2285 train_time:546853ms step_avg:719.54ms
step:761/2285 train_time:547580ms step_avg:719.55ms
step:762/2285 train_time:548308ms step_avg:719.56ms
step:763/2285 train_time:549032ms step_avg:719.57ms
step:764/2285 train_time:549761ms step_avg:719.58ms
step:765/2285 train_time:550485ms step_avg:719.59ms
step:766/2285 train_time:551217ms step_avg:719.60ms
step:767/2285 train_time:551943ms step_avg:719.61ms
step:768/2285 train_time:552674ms step_avg:719.63ms
step:769/2285 train_time:553399ms step_avg:719.63ms
step:770/2285 train_time:554128ms step_avg:719.65ms
step:771/2285 train_time:554854ms step_avg:719.66ms
step:772/2285 train_time:555587ms step_avg:719.67ms
step:773/2285 train_time:556312ms step_avg:719.68ms
step:774/2285 train_time:557043ms step_avg:719.69ms
step:775/2285 train_time:557765ms step_avg:719.70ms
step:776/2285 train_time:558495ms step_avg:719.71ms
step:777/2285 train_time:559221ms step_avg:719.72ms
step:778/2285 train_time:559950ms step_avg:719.73ms
step:779/2285 train_time:560675ms step_avg:719.74ms
step:780/2285 train_time:561407ms step_avg:719.75ms
step:781/2285 train_time:562132ms step_avg:719.76ms
step:782/2285 train_time:562864ms step_avg:719.77ms
step:783/2285 train_time:563586ms step_avg:719.78ms
step:784/2285 train_time:564319ms step_avg:719.79ms
step:785/2285 train_time:565044ms step_avg:719.80ms
step:786/2285 train_time:565775ms step_avg:719.82ms
step:787/2285 train_time:566496ms step_avg:719.82ms
step:788/2285 train_time:567226ms step_avg:719.83ms
step:789/2285 train_time:567949ms step_avg:719.83ms
step:790/2285 train_time:568679ms step_avg:719.85ms
step:791/2285 train_time:569403ms step_avg:719.85ms
step:792/2285 train_time:570134ms step_avg:719.87ms
step:793/2285 train_time:570857ms step_avg:719.87ms
step:794/2285 train_time:571588ms step_avg:719.88ms
step:795/2285 train_time:572313ms step_avg:719.89ms
step:796/2285 train_time:573045ms step_avg:719.91ms
step:797/2285 train_time:573768ms step_avg:719.91ms
step:798/2285 train_time:574499ms step_avg:719.92ms
step:799/2285 train_time:575225ms step_avg:719.93ms
step:800/2285 train_time:575958ms step_avg:719.95ms
step:800/2285 val_loss:3.7622 train_time:576056ms step_avg:720.07ms
step:801/2285 train_time:576678ms step_avg:719.95ms
step:802/2285 train_time:577404ms step_avg:719.96ms
step:803/2285 train_time:578125ms step_avg:719.96ms
step:804/2285 train_time:578852ms step_avg:719.97ms
step:805/2285 train_time:579573ms step_avg:719.97ms
step:806/2285 train_time:580298ms step_avg:719.97ms
step:807/2285 train_time:581018ms step_avg:719.97ms
step:808/2285 train_time:581746ms step_avg:719.98ms
step:809/2285 train_time:582466ms step_avg:719.98ms
step:810/2285 train_time:583192ms step_avg:719.99ms
step:811/2285 train_time:583914ms step_avg:719.99ms
step:812/2285 train_time:584639ms step_avg:720.00ms
step:813/2285 train_time:585363ms step_avg:720.00ms
step:814/2285 train_time:586091ms step_avg:720.01ms
step:815/2285 train_time:586812ms step_avg:720.01ms
step:816/2285 train_time:587540ms step_avg:720.02ms
step:817/2285 train_time:588260ms step_avg:720.03ms
step:818/2285 train_time:588989ms step_avg:720.04ms
step:819/2285 train_time:589710ms step_avg:720.04ms
step:820/2285 train_time:590441ms step_avg:720.05ms
step:821/2285 train_time:591165ms step_avg:720.06ms
step:822/2285 train_time:591893ms step_avg:720.06ms
step:823/2285 train_time:592617ms step_avg:720.07ms
step:824/2285 train_time:593344ms step_avg:720.08ms
step:825/2285 train_time:594068ms step_avg:720.08ms
step:826/2285 train_time:594800ms step_avg:720.10ms
step:827/2285 train_time:595523ms step_avg:720.10ms
step:828/2285 train_time:596252ms step_avg:720.11ms
step:829/2285 train_time:596976ms step_avg:720.12ms
step:830/2285 train_time:597705ms step_avg:720.13ms
step:831/2285 train_time:598425ms step_avg:720.13ms
step:832/2285 train_time:599157ms step_avg:720.14ms
step:833/2285 train_time:599879ms step_avg:720.14ms
step:834/2285 train_time:600606ms step_avg:720.15ms
step:835/2285 train_time:601328ms step_avg:720.15ms
step:836/2285 train_time:602061ms step_avg:720.17ms
step:837/2285 train_time:602785ms step_avg:720.17ms
step:838/2285 train_time:603514ms step_avg:720.18ms
step:839/2285 train_time:604239ms step_avg:720.19ms
step:840/2285 train_time:604974ms step_avg:720.21ms
step:841/2285 train_time:605697ms step_avg:720.21ms
step:842/2285 train_time:606426ms step_avg:720.22ms
step:843/2285 train_time:607154ms step_avg:720.23ms
step:844/2285 train_time:607884ms step_avg:720.24ms
step:845/2285 train_time:608607ms step_avg:720.25ms
step:846/2285 train_time:609340ms step_avg:720.26ms
step:847/2285 train_time:610066ms step_avg:720.27ms
step:848/2285 train_time:610796ms step_avg:720.28ms
step:849/2285 train_time:611518ms step_avg:720.28ms
step:850/2285 train_time:612247ms step_avg:720.29ms
step:851/2285 train_time:612970ms step_avg:720.29ms
step:852/2285 train_time:613702ms step_avg:720.31ms
step:853/2285 train_time:614424ms step_avg:720.31ms
step:854/2285 train_time:615156ms step_avg:720.32ms
step:855/2285 train_time:615879ms step_avg:720.33ms
step:856/2285 train_time:616609ms step_avg:720.34ms
step:857/2285 train_time:617333ms step_avg:720.34ms
step:858/2285 train_time:618062ms step_avg:720.35ms
step:859/2285 train_time:618788ms step_avg:720.36ms
step:860/2285 train_time:619521ms step_avg:720.37ms
step:861/2285 train_time:620248ms step_avg:720.38ms
step:862/2285 train_time:620979ms step_avg:720.39ms
step:863/2285 train_time:621705ms step_avg:720.40ms
step:864/2285 train_time:622434ms step_avg:720.41ms
step:865/2285 train_time:623160ms step_avg:720.42ms
step:866/2285 train_time:623894ms step_avg:720.43ms
step:867/2285 train_time:624618ms step_avg:720.44ms
step:868/2285 train_time:625351ms step_avg:720.45ms
step:869/2285 train_time:626078ms step_avg:720.46ms
step:870/2285 train_time:626808ms step_avg:720.47ms
step:871/2285 train_time:627531ms step_avg:720.47ms
step:872/2285 train_time:628263ms step_avg:720.49ms
step:873/2285 train_time:628989ms step_avg:720.49ms
step:874/2285 train_time:629722ms step_avg:720.51ms
step:875/2285 train_time:630446ms step_avg:720.51ms
step:876/2285 train_time:631177ms step_avg:720.52ms
step:877/2285 train_time:631900ms step_avg:720.52ms
step:878/2285 train_time:632632ms step_avg:720.54ms
step:879/2285 train_time:633358ms step_avg:720.54ms
step:880/2285 train_time:634091ms step_avg:720.56ms
step:881/2285 train_time:634818ms step_avg:720.57ms
step:882/2285 train_time:635548ms step_avg:720.58ms
step:883/2285 train_time:636277ms step_avg:720.59ms
step:884/2285 train_time:637006ms step_avg:720.59ms
step:885/2285 train_time:637728ms step_avg:720.60ms
step:886/2285 train_time:638461ms step_avg:720.61ms
step:887/2285 train_time:639187ms step_avg:720.62ms
step:888/2285 train_time:639920ms step_avg:720.63ms
step:889/2285 train_time:640646ms step_avg:720.64ms
step:890/2285 train_time:641381ms step_avg:720.65ms
step:891/2285 train_time:642107ms step_avg:720.66ms
step:892/2285 train_time:642840ms step_avg:720.67ms
step:893/2285 train_time:643567ms step_avg:720.68ms
step:894/2285 train_time:644298ms step_avg:720.69ms
step:895/2285 train_time:645023ms step_avg:720.70ms
step:896/2285 train_time:645755ms step_avg:720.71ms
step:897/2285 train_time:646480ms step_avg:720.71ms
step:898/2285 train_time:647209ms step_avg:720.72ms
step:899/2285 train_time:647936ms step_avg:720.73ms
step:900/2285 train_time:648665ms step_avg:720.74ms
step:900/2285 val_loss:3.7242 train_time:648763ms step_avg:720.85ms
step:901/2285 train_time:649391ms step_avg:720.74ms
step:902/2285 train_time:650126ms step_avg:720.76ms
step:903/2285 train_time:650853ms step_avg:720.77ms
step:904/2285 train_time:651584ms step_avg:720.78ms
step:905/2285 train_time:652310ms step_avg:720.79ms
step:906/2285 train_time:653046ms step_avg:720.80ms
step:907/2285 train_time:653769ms step_avg:720.80ms
step:908/2285 train_time:654503ms step_avg:720.82ms
step:909/2285 train_time:655233ms step_avg:720.83ms
step:910/2285 train_time:655965ms step_avg:720.84ms
step:911/2285 train_time:656690ms step_avg:720.84ms
step:912/2285 train_time:657423ms step_avg:720.86ms
step:913/2285 train_time:658149ms step_avg:720.86ms
step:914/2285 train_time:658881ms step_avg:720.88ms
step:915/2285 train_time:659605ms step_avg:720.88ms
step:916/2285 train_time:660338ms step_avg:720.89ms
step:917/2285 train_time:661064ms step_avg:720.90ms
step:918/2285 train_time:661796ms step_avg:720.91ms
step:919/2285 train_time:662523ms step_avg:720.92ms
step:920/2285 train_time:663256ms step_avg:720.93ms
step:921/2285 train_time:663983ms step_avg:720.94ms
step:922/2285 train_time:664718ms step_avg:720.95ms
step:923/2285 train_time:665443ms step_avg:720.96ms
step:924/2285 train_time:666176ms step_avg:720.97ms
step:925/2285 train_time:666904ms step_avg:720.98ms
step:926/2285 train_time:667638ms step_avg:720.99ms
step:927/2285 train_time:668363ms step_avg:721.00ms
step:928/2285 train_time:669097ms step_avg:721.01ms
step:929/2285 train_time:669823ms step_avg:721.01ms
step:930/2285 train_time:670557ms step_avg:721.03ms
step:931/2285 train_time:671282ms step_avg:721.03ms
step:932/2285 train_time:672013ms step_avg:721.04ms
step:933/2285 train_time:672741ms step_avg:721.05ms
step:934/2285 train_time:673470ms step_avg:721.06ms
step:935/2285 train_time:674194ms step_avg:721.06ms
step:936/2285 train_time:674927ms step_avg:721.08ms
step:937/2285 train_time:675654ms step_avg:721.08ms
step:938/2285 train_time:676384ms step_avg:721.09ms
step:939/2285 train_time:677111ms step_avg:721.10ms
step:940/2285 train_time:677844ms step_avg:721.11ms
step:941/2285 train_time:678568ms step_avg:721.11ms
step:942/2285 train_time:679299ms step_avg:721.12ms
step:943/2285 train_time:680025ms step_avg:721.13ms
step:944/2285 train_time:680758ms step_avg:721.14ms
step:945/2285 train_time:681483ms step_avg:721.15ms
step:946/2285 train_time:682218ms step_avg:721.16ms
step:947/2285 train_time:682944ms step_avg:721.17ms
step:948/2285 train_time:683673ms step_avg:721.17ms
step:949/2285 train_time:684402ms step_avg:721.18ms
step:950/2285 train_time:685135ms step_avg:721.20ms
step:951/2285 train_time:685864ms step_avg:721.20ms
step:952/2285 train_time:686596ms step_avg:721.21ms
step:953/2285 train_time:687322ms step_avg:721.22ms
step:954/2285 train_time:688056ms step_avg:721.23ms
step:955/2285 train_time:688782ms step_avg:721.24ms
step:956/2285 train_time:689517ms step_avg:721.25ms
step:957/2285 train_time:690244ms step_avg:721.26ms
step:958/2285 train_time:690978ms step_avg:721.27ms
step:959/2285 train_time:691705ms step_avg:721.28ms
step:960/2285 train_time:692440ms step_avg:721.29ms
step:961/2285 train_time:693164ms step_avg:721.29ms
step:962/2285 train_time:693898ms step_avg:721.31ms
step:963/2285 train_time:694624ms step_avg:721.31ms
step:964/2285 train_time:695359ms step_avg:721.33ms
step:965/2285 train_time:696084ms step_avg:721.33ms
step:966/2285 train_time:696818ms step_avg:721.34ms
step:967/2285 train_time:697544ms step_avg:721.35ms
step:968/2285 train_time:698277ms step_avg:721.36ms
step:969/2285 train_time:699003ms step_avg:721.36ms
step:970/2285 train_time:699738ms step_avg:721.38ms
step:971/2285 train_time:700464ms step_avg:721.38ms
step:972/2285 train_time:701197ms step_avg:721.40ms
step:973/2285 train_time:701923ms step_avg:721.40ms
step:974/2285 train_time:702657ms step_avg:721.41ms
step:975/2285 train_time:703383ms step_avg:721.42ms
step:976/2285 train_time:704117ms step_avg:721.43ms
step:977/2285 train_time:704844ms step_avg:721.44ms
step:978/2285 train_time:705577ms step_avg:721.45ms
step:979/2285 train_time:706304ms step_avg:721.45ms
step:980/2285 train_time:707038ms step_avg:721.47ms
step:981/2285 train_time:707764ms step_avg:721.47ms
step:982/2285 train_time:708497ms step_avg:721.48ms
step:983/2285 train_time:709224ms step_avg:721.49ms
step:984/2285 train_time:709957ms step_avg:721.50ms
step:985/2285 train_time:710684ms step_avg:721.51ms
step:986/2285 train_time:711417ms step_avg:721.52ms
step:987/2285 train_time:712145ms step_avg:721.52ms
step:988/2285 train_time:712877ms step_avg:721.54ms
step:989/2285 train_time:713603ms step_avg:721.54ms
step:990/2285 train_time:714337ms step_avg:721.55ms
step:991/2285 train_time:715064ms step_avg:721.56ms
step:992/2285 train_time:715796ms step_avg:721.57ms
step:993/2285 train_time:716522ms step_avg:721.57ms
step:994/2285 train_time:717255ms step_avg:721.58ms
step:995/2285 train_time:717985ms step_avg:721.59ms
step:996/2285 train_time:718717ms step_avg:721.60ms
step:997/2285 train_time:719443ms step_avg:721.61ms
step:998/2285 train_time:720177ms step_avg:721.62ms
step:999/2285 train_time:720903ms step_avg:721.62ms
step:1000/2285 train_time:721636ms step_avg:721.64ms
step:1000/2285 val_loss:3.6990 train_time:721734ms step_avg:721.73ms
step:1001/2285 train_time:722361ms step_avg:721.64ms
step:1002/2285 train_time:723097ms step_avg:721.65ms
step:1003/2285 train_time:723823ms step_avg:721.66ms
step:1004/2285 train_time:724556ms step_avg:721.67ms
step:1005/2285 train_time:725281ms step_avg:721.67ms
step:1006/2285 train_time:726018ms step_avg:721.69ms
step:1007/2285 train_time:726743ms step_avg:721.69ms
step:1008/2285 train_time:727476ms step_avg:721.70ms
step:1009/2285 train_time:728203ms step_avg:721.71ms
step:1010/2285 train_time:728937ms step_avg:721.72ms
step:1011/2285 train_time:729662ms step_avg:721.72ms
step:1012/2285 train_time:730396ms step_avg:721.73ms
step:1013/2285 train_time:731121ms step_avg:721.74ms
step:1014/2285 train_time:731856ms step_avg:721.75ms
step:1015/2285 train_time:732580ms step_avg:721.75ms
step:1016/2285 train_time:733316ms step_avg:721.77ms
step:1017/2285 train_time:734043ms step_avg:721.77ms
step:1018/2285 train_time:734776ms step_avg:721.78ms
step:1019/2285 train_time:735502ms step_avg:721.79ms
step:1020/2285 train_time:736237ms step_avg:721.80ms
step:1021/2285 train_time:736960ms step_avg:721.80ms
step:1022/2285 train_time:737694ms step_avg:721.81ms
step:1023/2285 train_time:738421ms step_avg:721.82ms
step:1024/2285 train_time:739155ms step_avg:721.83ms
step:1025/2285 train_time:739880ms step_avg:721.83ms
step:1026/2285 train_time:740616ms step_avg:721.85ms
step:1027/2285 train_time:741342ms step_avg:721.85ms
step:1028/2285 train_time:742076ms step_avg:721.86ms
step:1029/2285 train_time:742802ms step_avg:721.87ms
step:1030/2285 train_time:743537ms step_avg:721.88ms
step:1031/2285 train_time:744262ms step_avg:721.88ms
step:1032/2285 train_time:744996ms step_avg:721.90ms
step:1033/2285 train_time:745724ms step_avg:721.90ms
step:1034/2285 train_time:746456ms step_avg:721.91ms
step:1035/2285 train_time:747181ms step_avg:721.91ms
step:1036/2285 train_time:747918ms step_avg:721.93ms
step:1037/2285 train_time:748643ms step_avg:721.93ms
step:1038/2285 train_time:749378ms step_avg:721.94ms
step:1039/2285 train_time:750102ms step_avg:721.95ms
step:1040/2285 train_time:750838ms step_avg:721.96ms
step:1041/2285 train_time:751564ms step_avg:721.96ms
step:1042/2285 train_time:752298ms step_avg:721.98ms
step:1043/2285 train_time:753023ms step_avg:721.98ms
step:1044/2285 train_time:753757ms step_avg:721.99ms
step:1045/2285 train_time:754482ms step_avg:721.99ms
step:1046/2285 train_time:755217ms step_avg:722.01ms
step:1047/2285 train_time:755944ms step_avg:722.01ms
step:1048/2285 train_time:756676ms step_avg:722.02ms
step:1049/2285 train_time:757402ms step_avg:722.02ms
step:1050/2285 train_time:758136ms step_avg:722.03ms
step:1051/2285 train_time:758858ms step_avg:722.03ms
step:1052/2285 train_time:759592ms step_avg:722.05ms
step:1053/2285 train_time:760316ms step_avg:722.05ms
step:1054/2285 train_time:761049ms step_avg:722.06ms
step:1055/2285 train_time:761772ms step_avg:722.06ms
step:1056/2285 train_time:762506ms step_avg:722.07ms
step:1057/2285 train_time:763234ms step_avg:722.08ms
step:1058/2285 train_time:763966ms step_avg:722.08ms
step:1059/2285 train_time:764692ms step_avg:722.09ms
step:1060/2285 train_time:765425ms step_avg:722.10ms
step:1061/2285 train_time:766153ms step_avg:722.10ms
step:1062/2285 train_time:766886ms step_avg:722.11ms
step:1063/2285 train_time:767613ms step_avg:722.12ms
step:1064/2285 train_time:768347ms step_avg:722.13ms
step:1065/2285 train_time:769074ms step_avg:722.14ms
step:1066/2285 train_time:769807ms step_avg:722.15ms
step:1067/2285 train_time:770534ms step_avg:722.15ms
step:1068/2285 train_time:771266ms step_avg:722.16ms
step:1069/2285 train_time:771993ms step_avg:722.16ms
step:1070/2285 train_time:772727ms step_avg:722.17ms
step:1071/2285 train_time:773453ms step_avg:722.18ms
step:1072/2285 train_time:774185ms step_avg:722.19ms
step:1073/2285 train_time:774913ms step_avg:722.19ms
step:1074/2285 train_time:775644ms step_avg:722.20ms
step:1075/2285 train_time:776372ms step_avg:722.21ms
step:1076/2285 train_time:777105ms step_avg:722.22ms
step:1077/2285 train_time:777831ms step_avg:722.22ms
step:1078/2285 train_time:778561ms step_avg:722.23ms
step:1079/2285 train_time:779288ms step_avg:722.23ms
step:1080/2285 train_time:780018ms step_avg:722.24ms
step:1081/2285 train_time:780744ms step_avg:722.24ms
step:1082/2285 train_time:781476ms step_avg:722.25ms
step:1083/2285 train_time:782202ms step_avg:722.25ms
step:1084/2285 train_time:782937ms step_avg:722.27ms
step:1085/2285 train_time:783662ms step_avg:722.27ms
step:1086/2285 train_time:784396ms step_avg:722.28ms
step:1087/2285 train_time:785123ms step_avg:722.28ms
step:1088/2285 train_time:785857ms step_avg:722.30ms
step:1089/2285 train_time:786583ms step_avg:722.30ms
step:1090/2285 train_time:787316ms step_avg:722.31ms
step:1091/2285 train_time:788044ms step_avg:722.31ms
step:1092/2285 train_time:788779ms step_avg:722.32ms
step:1093/2285 train_time:789503ms step_avg:722.33ms
step:1094/2285 train_time:790238ms step_avg:722.34ms
step:1095/2285 train_time:790962ms step_avg:722.34ms
step:1096/2285 train_time:791697ms step_avg:722.35ms
step:1097/2285 train_time:792423ms step_avg:722.35ms
step:1098/2285 train_time:793157ms step_avg:722.36ms
step:1099/2285 train_time:793883ms step_avg:722.37ms
step:1100/2285 train_time:794618ms step_avg:722.38ms
step:1100/2285 val_loss:3.6687 train_time:794714ms step_avg:722.47ms
step:1101/2285 train_time:795341ms step_avg:722.38ms
step:1102/2285 train_time:796078ms step_avg:722.39ms
step:1103/2285 train_time:796802ms step_avg:722.40ms
step:1104/2285 train_time:797534ms step_avg:722.40ms
step:1105/2285 train_time:798262ms step_avg:722.41ms
step:1106/2285 train_time:798997ms step_avg:722.42ms
step:1107/2285 train_time:799720ms step_avg:722.42ms
step:1108/2285 train_time:800456ms step_avg:722.43ms
step:1109/2285 train_time:801182ms step_avg:722.44ms
step:1110/2285 train_time:801917ms step_avg:722.45ms
step:1111/2285 train_time:802643ms step_avg:722.45ms
step:1112/2285 train_time:803375ms step_avg:722.46ms
step:1113/2285 train_time:804103ms step_avg:722.46ms
step:1114/2285 train_time:804834ms step_avg:722.47ms
step:1115/2285 train_time:805558ms step_avg:722.47ms
step:1116/2285 train_time:806296ms step_avg:722.49ms
step:1117/2285 train_time:807021ms step_avg:722.49ms
step:1118/2285 train_time:807755ms step_avg:722.50ms
step:1119/2285 train_time:808482ms step_avg:722.50ms
step:1120/2285 train_time:809217ms step_avg:722.51ms
step:1121/2285 train_time:809942ms step_avg:722.52ms
step:1122/2285 train_time:810674ms step_avg:722.53ms
step:1123/2285 train_time:811402ms step_avg:722.53ms
step:1124/2285 train_time:812134ms step_avg:722.54ms
step:1125/2285 train_time:812859ms step_avg:722.54ms
step:1126/2285 train_time:813594ms step_avg:722.55ms
step:1127/2285 train_time:814320ms step_avg:722.56ms
step:1128/2285 train_time:815053ms step_avg:722.56ms
step:1129/2285 train_time:815783ms step_avg:722.57ms
step:1130/2285 train_time:816515ms step_avg:722.58ms
step:1131/2285 train_time:817241ms step_avg:722.58ms
step:1132/2285 train_time:817972ms step_avg:722.59ms
step:1133/2285 train_time:818694ms step_avg:722.59ms
step:1134/2285 train_time:819425ms step_avg:722.60ms
step:1135/2285 train_time:820151ms step_avg:722.60ms
step:1136/2285 train_time:820881ms step_avg:722.61ms
step:1137/2285 train_time:821604ms step_avg:722.61ms
step:1138/2285 train_time:822335ms step_avg:722.61ms
step:1139/2285 train_time:823061ms step_avg:722.62ms
step:1140/2285 train_time:823795ms step_avg:722.63ms
step:1141/2285 train_time:824519ms step_avg:722.63ms
step:1142/2285 train_time:825250ms step_avg:722.64ms
step:1143/2285 train_time:825978ms step_avg:722.64ms
step:1144/2285 train_time:826709ms step_avg:722.65ms
step:1145/2285 train_time:827433ms step_avg:722.65ms
step:1146/2285 train_time:828167ms step_avg:722.66ms
step:1147/2285 train_time:828893ms step_avg:722.66ms
step:1148/2285 train_time:829625ms step_avg:722.67ms
step:1149/2285 train_time:830351ms step_avg:722.67ms
step:1150/2285 train_time:831081ms step_avg:722.68ms
step:1151/2285 train_time:831810ms step_avg:722.68ms
step:1152/2285 train_time:832539ms step_avg:722.69ms
step:1153/2285 train_time:833262ms step_avg:722.69ms
step:1154/2285 train_time:833997ms step_avg:722.70ms
step:1155/2285 train_time:834720ms step_avg:722.70ms
step:1156/2285 train_time:835453ms step_avg:722.71ms
step:1157/2285 train_time:836179ms step_avg:722.71ms
step:1158/2285 train_time:836909ms step_avg:722.72ms
step:1159/2285 train_time:837633ms step_avg:722.72ms
step:1160/2285 train_time:838363ms step_avg:722.73ms
step:1161/2285 train_time:839087ms step_avg:722.73ms
step:1162/2285 train_time:839817ms step_avg:722.73ms
step:1163/2285 train_time:840542ms step_avg:722.74ms
step:1164/2285 train_time:841277ms step_avg:722.75ms
step:1165/2285 train_time:842000ms step_avg:722.75ms
step:1166/2285 train_time:842731ms step_avg:722.75ms
step:1167/2285 train_time:843456ms step_avg:722.76ms
step:1168/2285 train_time:844188ms step_avg:722.76ms
step:1169/2285 train_time:844911ms step_avg:722.76ms
step:1170/2285 train_time:845644ms step_avg:722.77ms
step:1171/2285 train_time:846368ms step_avg:722.77ms
step:1172/2285 train_time:847097ms step_avg:722.78ms
step:1173/2285 train_time:847821ms step_avg:722.78ms
step:1174/2285 train_time:848555ms step_avg:722.79ms
step:1175/2285 train_time:849280ms step_avg:722.79ms
step:1176/2285 train_time:850010ms step_avg:722.80ms
step:1177/2285 train_time:850734ms step_avg:722.80ms
step:1178/2285 train_time:851466ms step_avg:722.81ms
step:1179/2285 train_time:852190ms step_avg:722.81ms
step:1180/2285 train_time:852919ms step_avg:722.81ms
step:1181/2285 train_time:853643ms step_avg:722.81ms
step:1182/2285 train_time:854372ms step_avg:722.82ms
step:1183/2285 train_time:855093ms step_avg:722.82ms
step:1184/2285 train_time:855823ms step_avg:722.82ms
step:1185/2285 train_time:856548ms step_avg:722.83ms
step:1186/2285 train_time:857279ms step_avg:722.83ms
step:1187/2285 train_time:858003ms step_avg:722.83ms
step:1188/2285 train_time:858733ms step_avg:722.84ms
step:1189/2285 train_time:859456ms step_avg:722.84ms
step:1190/2285 train_time:860189ms step_avg:722.85ms
step:1191/2285 train_time:860912ms step_avg:722.85ms
step:1192/2285 train_time:861641ms step_avg:722.85ms
step:1193/2285 train_time:862366ms step_avg:722.85ms
step:1194/2285 train_time:863094ms step_avg:722.86ms
step:1195/2285 train_time:863816ms step_avg:722.86ms
step:1196/2285 train_time:864546ms step_avg:722.86ms
step:1197/2285 train_time:865272ms step_avg:722.87ms
step:1198/2285 train_time:866003ms step_avg:722.87ms
step:1199/2285 train_time:866727ms step_avg:722.88ms
step:1200/2285 train_time:867458ms step_avg:722.88ms
step:1200/2285 val_loss:3.6422 train_time:867554ms step_avg:722.96ms
step:1201/2285 train_time:868181ms step_avg:722.88ms
step:1202/2285 train_time:868912ms step_avg:722.89ms
step:1203/2285 train_time:869638ms step_avg:722.89ms
step:1204/2285 train_time:870367ms step_avg:722.90ms
step:1205/2285 train_time:871091ms step_avg:722.90ms
step:1206/2285 train_time:871820ms step_avg:722.90ms
step:1207/2285 train_time:872543ms step_avg:722.90ms
step:1208/2285 train_time:873275ms step_avg:722.91ms
step:1209/2285 train_time:873999ms step_avg:722.91ms
step:1210/2285 train_time:874728ms step_avg:722.92ms
step:1211/2285 train_time:875453ms step_avg:722.92ms
step:1212/2285 train_time:876183ms step_avg:722.92ms
step:1213/2285 train_time:876906ms step_avg:722.92ms
step:1214/2285 train_time:877636ms step_avg:722.93ms
step:1215/2285 train_time:878362ms step_avg:722.93ms
step:1216/2285 train_time:879091ms step_avg:722.94ms
step:1217/2285 train_time:879814ms step_avg:722.94ms
step:1218/2285 train_time:880547ms step_avg:722.94ms
step:1219/2285 train_time:881272ms step_avg:722.95ms
step:1220/2285 train_time:882000ms step_avg:722.95ms
step:1221/2285 train_time:882724ms step_avg:722.95ms
step:1222/2285 train_time:883456ms step_avg:722.96ms
step:1223/2285 train_time:884178ms step_avg:722.96ms
step:1224/2285 train_time:884907ms step_avg:722.96ms
step:1225/2285 train_time:885633ms step_avg:722.97ms
step:1226/2285 train_time:886362ms step_avg:722.97ms
step:1227/2285 train_time:887086ms step_avg:722.97ms
step:1228/2285 train_time:887817ms step_avg:722.98ms
step:1229/2285 train_time:888542ms step_avg:722.98ms
step:1230/2285 train_time:889271ms step_avg:722.98ms
step:1231/2285 train_time:889993ms step_avg:722.98ms
step:1232/2285 train_time:890726ms step_avg:722.99ms
step:1233/2285 train_time:891451ms step_avg:722.99ms
step:1234/2285 train_time:892180ms step_avg:723.00ms
step:1235/2285 train_time:892902ms step_avg:723.00ms
step:1236/2285 train_time:893631ms step_avg:723.00ms
step:1237/2285 train_time:894354ms step_avg:723.00ms
step:1238/2285 train_time:895085ms step_avg:723.01ms
step:1239/2285 train_time:895810ms step_avg:723.01ms
step:1240/2285 train_time:896537ms step_avg:723.01ms
step:1241/2285 train_time:897263ms step_avg:723.02ms
step:1242/2285 train_time:897991ms step_avg:723.02ms
step:1243/2285 train_time:898713ms step_avg:723.02ms
step:1244/2285 train_time:899446ms step_avg:723.03ms
step:1245/2285 train_time:900172ms step_avg:723.03ms
step:1246/2285 train_time:900900ms step_avg:723.03ms
step:1247/2285 train_time:901623ms step_avg:723.03ms
step:1248/2285 train_time:902355ms step_avg:723.04ms
step:1249/2285 train_time:903081ms step_avg:723.04ms
step:1250/2285 train_time:903810ms step_avg:723.05ms
step:1251/2285 train_time:904532ms step_avg:723.05ms
step:1252/2285 train_time:905264ms step_avg:723.05ms
step:1253/2285 train_time:905988ms step_avg:723.06ms
step:1254/2285 train_time:906717ms step_avg:723.06ms
step:1255/2285 train_time:907440ms step_avg:723.06ms
step:1256/2285 train_time:908167ms step_avg:723.06ms
step:1257/2285 train_time:908892ms step_avg:723.06ms
step:1258/2285 train_time:909620ms step_avg:723.07ms
step:1259/2285 train_time:910343ms step_avg:723.07ms
step:1260/2285 train_time:911074ms step_avg:723.07ms
step:1261/2285 train_time:911797ms step_avg:723.07ms
step:1262/2285 train_time:912526ms step_avg:723.08ms
step:1263/2285 train_time:913253ms step_avg:723.08ms
step:1264/2285 train_time:913982ms step_avg:723.09ms
step:1265/2285 train_time:914705ms step_avg:723.09ms
step:1266/2285 train_time:915436ms step_avg:723.09ms
step:1267/2285 train_time:916159ms step_avg:723.09ms
step:1268/2285 train_time:916888ms step_avg:723.10ms
step:1269/2285 train_time:917611ms step_avg:723.10ms
step:1270/2285 train_time:918340ms step_avg:723.10ms
step:1271/2285 train_time:919063ms step_avg:723.10ms
step:1272/2285 train_time:919791ms step_avg:723.11ms
step:1273/2285 train_time:920513ms step_avg:723.11ms
step:1274/2285 train_time:921244ms step_avg:723.11ms
step:1275/2285 train_time:921968ms step_avg:723.11ms
step:1276/2285 train_time:922696ms step_avg:723.12ms
step:1277/2285 train_time:923419ms step_avg:723.12ms
step:1278/2285 train_time:924149ms step_avg:723.12ms
step:1279/2285 train_time:924873ms step_avg:723.12ms
step:1280/2285 train_time:925602ms step_avg:723.13ms
step:1281/2285 train_time:926329ms step_avg:723.13ms
step:1282/2285 train_time:927058ms step_avg:723.13ms
step:1283/2285 train_time:927781ms step_avg:723.13ms
step:1284/2285 train_time:928512ms step_avg:723.14ms
step:1285/2285 train_time:929234ms step_avg:723.14ms
step:1286/2285 train_time:929965ms step_avg:723.15ms
step:1287/2285 train_time:930687ms step_avg:723.14ms
step:1288/2285 train_time:931417ms step_avg:723.15ms
step:1289/2285 train_time:932142ms step_avg:723.15ms
step:1290/2285 train_time:932872ms step_avg:723.16ms
step:1291/2285 train_time:933594ms step_avg:723.16ms
step:1292/2285 train_time:934324ms step_avg:723.16ms
step:1293/2285 train_time:935048ms step_avg:723.16ms
step:1294/2285 train_time:935777ms step_avg:723.17ms
step:1295/2285 train_time:936499ms step_avg:723.17ms
step:1296/2285 train_time:937230ms step_avg:723.17ms
step:1297/2285 train_time:937952ms step_avg:723.17ms
step:1298/2285 train_time:938680ms step_avg:723.17ms
step:1299/2285 train_time:939403ms step_avg:723.17ms
step:1300/2285 train_time:940132ms step_avg:723.18ms
step:1300/2285 val_loss:3.6108 train_time:940229ms step_avg:723.25ms
step:1301/2285 train_time:940854ms step_avg:723.18ms
step:1302/2285 train_time:941584ms step_avg:723.18ms
step:1303/2285 train_time:942308ms step_avg:723.18ms
step:1304/2285 train_time:943039ms step_avg:723.19ms
step:1305/2285 train_time:943763ms step_avg:723.19ms
step:1306/2285 train_time:944493ms step_avg:723.19ms
step:1307/2285 train_time:945213ms step_avg:723.19ms
step:1308/2285 train_time:945942ms step_avg:723.20ms
step:1309/2285 train_time:946669ms step_avg:723.20ms
step:1310/2285 train_time:947397ms step_avg:723.20ms
step:1311/2285 train_time:948119ms step_avg:723.20ms
step:1312/2285 train_time:948851ms step_avg:723.21ms
step:1313/2285 train_time:949574ms step_avg:723.21ms
step:1314/2285 train_time:950303ms step_avg:723.21ms
step:1315/2285 train_time:951027ms step_avg:723.21ms
step:1316/2285 train_time:951755ms step_avg:723.22ms
step:1317/2285 train_time:952478ms step_avg:723.22ms
step:1318/2285 train_time:953207ms step_avg:723.22ms
step:1319/2285 train_time:953929ms step_avg:723.22ms
step:1320/2285 train_time:954658ms step_avg:723.23ms
step:1321/2285 train_time:955379ms step_avg:723.22ms
step:1322/2285 train_time:956110ms step_avg:723.23ms
step:1323/2285 train_time:956836ms step_avg:723.23ms
step:1324/2285 train_time:957564ms step_avg:723.24ms
step:1325/2285 train_time:958289ms step_avg:723.24ms
step:1326/2285 train_time:959022ms step_avg:723.24ms
step:1327/2285 train_time:959747ms step_avg:723.25ms
step:1328/2285 train_time:960475ms step_avg:723.25ms
step:1329/2285 train_time:961200ms step_avg:723.25ms
step:1330/2285 train_time:961930ms step_avg:723.26ms
step:1331/2285 train_time:962655ms step_avg:723.26ms
step:1332/2285 train_time:963384ms step_avg:723.26ms
step:1333/2285 train_time:964109ms step_avg:723.26ms
step:1334/2285 train_time:964839ms step_avg:723.27ms
step:1335/2285 train_time:965563ms step_avg:723.27ms
step:1336/2285 train_time:966292ms step_avg:723.27ms
step:1337/2285 train_time:967017ms step_avg:723.27ms
step:1338/2285 train_time:967746ms step_avg:723.28ms
step:1339/2285 train_time:968469ms step_avg:723.28ms
step:1340/2285 train_time:969201ms step_avg:723.28ms
step:1341/2285 train_time:969924ms step_avg:723.28ms
step:1342/2285 train_time:970652ms step_avg:723.29ms
step:1343/2285 train_time:971377ms step_avg:723.29ms
step:1344/2285 train_time:972107ms step_avg:723.29ms
step:1345/2285 train_time:972829ms step_avg:723.29ms
step:1346/2285 train_time:973560ms step_avg:723.30ms
step:1347/2285 train_time:974282ms step_avg:723.30ms
step:1348/2285 train_time:975011ms step_avg:723.30ms
step:1349/2285 train_time:975736ms step_avg:723.30ms
step:1350/2285 train_time:976466ms step_avg:723.31ms
step:1351/2285 train_time:977188ms step_avg:723.31ms
step:1352/2285 train_time:977917ms step_avg:723.31ms
step:1353/2285 train_time:978640ms step_avg:723.31ms
step:1354/2285 train_time:979371ms step_avg:723.32ms
step:1355/2285 train_time:980094ms step_avg:723.32ms
step:1356/2285 train_time:980823ms step_avg:723.32ms
step:1357/2285 train_time:981548ms step_avg:723.32ms
step:1358/2285 train_time:982275ms step_avg:723.33ms
step:1359/2285 train_time:982998ms step_avg:723.32ms
step:1360/2285 train_time:983729ms step_avg:723.33ms
step:1361/2285 train_time:984449ms step_avg:723.33ms
step:1362/2285 train_time:985179ms step_avg:723.33ms
step:1363/2285 train_time:985898ms step_avg:723.33ms
step:1364/2285 train_time:986628ms step_avg:723.33ms
step:1365/2285 train_time:987349ms step_avg:723.33ms
step:1366/2285 train_time:988075ms step_avg:723.33ms
step:1367/2285 train_time:988799ms step_avg:723.33ms
step:1368/2285 train_time:989528ms step_avg:723.34ms
step:1369/2285 train_time:990250ms step_avg:723.34ms
step:1370/2285 train_time:990980ms step_avg:723.34ms
step:1371/2285 train_time:991701ms step_avg:723.34ms
step:1372/2285 train_time:992432ms step_avg:723.35ms
step:1373/2285 train_time:993155ms step_avg:723.35ms
step:1374/2285 train_time:993883ms step_avg:723.35ms
step:1375/2285 train_time:994607ms step_avg:723.35ms
step:1376/2285 train_time:995334ms step_avg:723.35ms
step:1377/2285 train_time:996059ms step_avg:723.35ms
step:1378/2285 train_time:996787ms step_avg:723.36ms
step:1379/2285 train_time:997509ms step_avg:723.36ms
step:1380/2285 train_time:998236ms step_avg:723.36ms
step:1381/2285 train_time:998959ms step_avg:723.36ms
step:1382/2285 train_time:999689ms step_avg:723.36ms
step:1383/2285 train_time:1000410ms step_avg:723.36ms
step:1384/2285 train_time:1001139ms step_avg:723.37ms
step:1385/2285 train_time:1001863ms step_avg:723.37ms
step:1386/2285 train_time:1002592ms step_avg:723.37ms
step:1387/2285 train_time:1003316ms step_avg:723.37ms
step:1388/2285 train_time:1004046ms step_avg:723.38ms
step:1389/2285 train_time:1004769ms step_avg:723.38ms
step:1390/2285 train_time:1005498ms step_avg:723.38ms
step:1391/2285 train_time:1006218ms step_avg:723.38ms
step:1392/2285 train_time:1006950ms step_avg:723.38ms
step:1393/2285 train_time:1007673ms step_avg:723.38ms
step:1394/2285 train_time:1008401ms step_avg:723.39ms
step:1395/2285 train_time:1009124ms step_avg:723.39ms
step:1396/2285 train_time:1009854ms step_avg:723.39ms
step:1397/2285 train_time:1010577ms step_avg:723.39ms
step:1398/2285 train_time:1011307ms step_avg:723.40ms
step:1399/2285 train_time:1012028ms step_avg:723.39ms
step:1400/2285 train_time:1012756ms step_avg:723.40ms
step:1400/2285 val_loss:3.5888 train_time:1012853ms step_avg:723.47ms
step:1401/2285 train_time:1013478ms step_avg:723.40ms
step:1402/2285 train_time:1014208ms step_avg:723.40ms
step:1403/2285 train_time:1014932ms step_avg:723.40ms
step:1404/2285 train_time:1015659ms step_avg:723.40ms
step:1405/2285 train_time:1016384ms step_avg:723.40ms
step:1406/2285 train_time:1017111ms step_avg:723.41ms
step:1407/2285 train_time:1017834ms step_avg:723.41ms
step:1408/2285 train_time:1018564ms step_avg:723.41ms
step:1409/2285 train_time:1019285ms step_avg:723.41ms
step:1410/2285 train_time:1020013ms step_avg:723.41ms
step:1411/2285 train_time:1020735ms step_avg:723.41ms
step:1412/2285 train_time:1021466ms step_avg:723.42ms
step:1413/2285 train_time:1022190ms step_avg:723.42ms
step:1414/2285 train_time:1022916ms step_avg:723.42ms
step:1415/2285 train_time:1023640ms step_avg:723.42ms
step:1416/2285 train_time:1024370ms step_avg:723.43ms
step:1417/2285 train_time:1025094ms step_avg:723.43ms
step:1418/2285 train_time:1025822ms step_avg:723.43ms
step:1419/2285 train_time:1026544ms step_avg:723.43ms
step:1420/2285 train_time:1027276ms step_avg:723.43ms
step:1421/2285 train_time:1027996ms step_avg:723.43ms
step:1422/2285 train_time:1028724ms step_avg:723.43ms
step:1423/2285 train_time:1029448ms step_avg:723.43ms
step:1424/2285 train_time:1030176ms step_avg:723.44ms
step:1425/2285 train_time:1030897ms step_avg:723.44ms
step:1426/2285 train_time:1031627ms step_avg:723.44ms
step:1427/2285 train_time:1032348ms step_avg:723.44ms
step:1428/2285 train_time:1033078ms step_avg:723.44ms
step:1429/2285 train_time:1033801ms step_avg:723.44ms
step:1430/2285 train_time:1034530ms step_avg:723.45ms
step:1431/2285 train_time:1035253ms step_avg:723.45ms
step:1432/2285 train_time:1035982ms step_avg:723.45ms
step:1433/2285 train_time:1036704ms step_avg:723.45ms
step:1434/2285 train_time:1037432ms step_avg:723.45ms
step:1435/2285 train_time:1038156ms step_avg:723.45ms
step:1436/2285 train_time:1038888ms step_avg:723.46ms
step:1437/2285 train_time:1039612ms step_avg:723.46ms
step:1438/2285 train_time:1040340ms step_avg:723.46ms
step:1439/2285 train_time:1041064ms step_avg:723.46ms
step:1440/2285 train_time:1041792ms step_avg:723.47ms
step:1441/2285 train_time:1042515ms step_avg:723.47ms
step:1442/2285 train_time:1043242ms step_avg:723.47ms
step:1443/2285 train_time:1043964ms step_avg:723.47ms
step:1444/2285 train_time:1044697ms step_avg:723.47ms
step:1445/2285 train_time:1045418ms step_avg:723.47ms
step:1446/2285 train_time:1046146ms step_avg:723.48ms
step:1447/2285 train_time:1046871ms step_avg:723.48ms
step:1448/2285 train_time:1047599ms step_avg:723.48ms
step:1449/2285 train_time:1048324ms step_avg:723.48ms
step:1450/2285 train_time:1049052ms step_avg:723.48ms
step:1451/2285 train_time:1049775ms step_avg:723.48ms
step:1452/2285 train_time:1050504ms step_avg:723.49ms
step:1453/2285 train_time:1051225ms step_avg:723.49ms
step:1454/2285 train_time:1051955ms step_avg:723.49ms
step:1455/2285 train_time:1052674ms step_avg:723.49ms
step:1456/2285 train_time:1053404ms step_avg:723.49ms
step:1457/2285 train_time:1054126ms step_avg:723.49ms
step:1458/2285 train_time:1054855ms step_avg:723.49ms
step:1459/2285 train_time:1055577ms step_avg:723.49ms
step:1460/2285 train_time:1056308ms step_avg:723.50ms
step:1461/2285 train_time:1057030ms step_avg:723.50ms
step:1462/2285 train_time:1057758ms step_avg:723.50ms
step:1463/2285 train_time:1058482ms step_avg:723.50ms
step:1464/2285 train_time:1059213ms step_avg:723.51ms
step:1465/2285 train_time:1059935ms step_avg:723.50ms
step:1466/2285 train_time:1060662ms step_avg:723.51ms
step:1467/2285 train_time:1061384ms step_avg:723.51ms
step:1468/2285 train_time:1062113ms step_avg:723.51ms
step:1469/2285 train_time:1062835ms step_avg:723.51ms
step:1470/2285 train_time:1063561ms step_avg:723.51ms
step:1471/2285 train_time:1064284ms step_avg:723.51ms
step:1472/2285 train_time:1065011ms step_avg:723.51ms
step:1473/2285 train_time:1065733ms step_avg:723.51ms
step:1474/2285 train_time:1066460ms step_avg:723.51ms
step:1475/2285 train_time:1067185ms step_avg:723.52ms
step:1476/2285 train_time:1067914ms step_avg:723.52ms
step:1477/2285 train_time:1068636ms step_avg:723.52ms
step:1478/2285 train_time:1069368ms step_avg:723.52ms
step:1479/2285 train_time:1070093ms step_avg:723.52ms
step:1480/2285 train_time:1070820ms step_avg:723.53ms
step:1481/2285 train_time:1071543ms step_avg:723.53ms
step:1482/2285 train_time:1072271ms step_avg:723.53ms
step:1483/2285 train_time:1072995ms step_avg:723.53ms
step:1484/2285 train_time:1073723ms step_avg:723.53ms
step:1485/2285 train_time:1074445ms step_avg:723.53ms
step:1486/2285 train_time:1075175ms step_avg:723.54ms
step:1487/2285 train_time:1075896ms step_avg:723.53ms
step:1488/2285 train_time:1076627ms step_avg:723.54ms
step:1489/2285 train_time:1077349ms step_avg:723.54ms
step:1490/2285 train_time:1078077ms step_avg:723.54ms
step:1491/2285 train_time:1078801ms step_avg:723.54ms
step:1492/2285 train_time:1079530ms step_avg:723.55ms
step:1493/2285 train_time:1080253ms step_avg:723.55ms
step:1494/2285 train_time:1080982ms step_avg:723.55ms
step:1495/2285 train_time:1081705ms step_avg:723.55ms
step:1496/2285 train_time:1082434ms step_avg:723.55ms
step:1497/2285 train_time:1083156ms step_avg:723.55ms
step:1498/2285 train_time:1083893ms step_avg:723.56ms
step:1499/2285 train_time:1084618ms step_avg:723.56ms
step:1500/2285 train_time:1085351ms step_avg:723.57ms
step:1500/2285 val_loss:3.5536 train_time:1085450ms step_avg:723.63ms
step:1501/2285 train_time:1086077ms step_avg:723.57ms
step:1502/2285 train_time:1086808ms step_avg:723.57ms
step:1503/2285 train_time:1087531ms step_avg:723.57ms
step:1504/2285 train_time:1088258ms step_avg:723.58ms
step:1505/2285 train_time:1088982ms step_avg:723.58ms
step:1506/2285 train_time:1089715ms step_avg:723.58ms
step:1507/2285 train_time:1090438ms step_avg:723.58ms
step:1508/2285 train_time:1091171ms step_avg:723.59ms
step:1509/2285 train_time:1091892ms step_avg:723.59ms
step:1510/2285 train_time:1092623ms step_avg:723.59ms
step:1511/2285 train_time:1093349ms step_avg:723.59ms
step:1512/2285 train_time:1094078ms step_avg:723.60ms
step:1513/2285 train_time:1094803ms step_avg:723.60ms
step:1514/2285 train_time:1095535ms step_avg:723.60ms
step:1515/2285 train_time:1096259ms step_avg:723.60ms
step:1516/2285 train_time:1096993ms step_avg:723.61ms
step:1517/2285 train_time:1097720ms step_avg:723.61ms
step:1518/2285 train_time:1098455ms step_avg:723.62ms
step:1519/2285 train_time:1099179ms step_avg:723.62ms
step:1520/2285 train_time:1099916ms step_avg:723.63ms
step:1521/2285 train_time:1100641ms step_avg:723.63ms
step:1522/2285 train_time:1101374ms step_avg:723.64ms
step:1523/2285 train_time:1102099ms step_avg:723.64ms
step:1524/2285 train_time:1102834ms step_avg:723.64ms
step:1525/2285 train_time:1103559ms step_avg:723.65ms
step:1526/2285 train_time:1104296ms step_avg:723.65ms
step:1527/2285 train_time:1105021ms step_avg:723.65ms
step:1528/2285 train_time:1105756ms step_avg:723.66ms
step:1529/2285 train_time:1106481ms step_avg:723.66ms
step:1530/2285 train_time:1107217ms step_avg:723.67ms
step:1531/2285 train_time:1107942ms step_avg:723.67ms
step:1532/2285 train_time:1108673ms step_avg:723.68ms
step:1533/2285 train_time:1109402ms step_avg:723.68ms
step:1534/2285 train_time:1110137ms step_avg:723.69ms
step:1535/2285 train_time:1110861ms step_avg:723.69ms
step:1536/2285 train_time:1111596ms step_avg:723.70ms
step:1537/2285 train_time:1112323ms step_avg:723.70ms
step:1538/2285 train_time:1113055ms step_avg:723.70ms
step:1539/2285 train_time:1113780ms step_avg:723.70ms
step:1540/2285 train_time:1114516ms step_avg:723.71ms
step:1541/2285 train_time:1115243ms step_avg:723.71ms
step:1542/2285 train_time:1115975ms step_avg:723.72ms
step:1543/2285 train_time:1116701ms step_avg:723.72ms
step:1544/2285 train_time:1117436ms step_avg:723.73ms
step:1545/2285 train_time:1118161ms step_avg:723.73ms
step:1546/2285 train_time:1118897ms step_avg:723.74ms
step:1547/2285 train_time:1119624ms step_avg:723.74ms
step:1548/2285 train_time:1120358ms step_avg:723.75ms
step:1549/2285 train_time:1121087ms step_avg:723.75ms
step:1550/2285 train_time:1121819ms step_avg:723.75ms
step:1551/2285 train_time:1122546ms step_avg:723.76ms
step:1552/2285 train_time:1123276ms step_avg:723.76ms
step:1553/2285 train_time:1124003ms step_avg:723.76ms
step:1554/2285 train_time:1124736ms step_avg:723.77ms
step:1555/2285 train_time:1125461ms step_avg:723.77ms
step:1556/2285 train_time:1126198ms step_avg:723.78ms
step:1557/2285 train_time:1126924ms step_avg:723.78ms
step:1558/2285 train_time:1127657ms step_avg:723.79ms
step:1559/2285 train_time:1128383ms step_avg:723.79ms
step:1560/2285 train_time:1129117ms step_avg:723.79ms
step:1561/2285 train_time:1129844ms step_avg:723.80ms
step:1562/2285 train_time:1130576ms step_avg:723.80ms
step:1563/2285 train_time:1131305ms step_avg:723.80ms
step:1564/2285 train_time:1132038ms step_avg:723.81ms
step:1565/2285 train_time:1132763ms step_avg:723.81ms
step:1566/2285 train_time:1133498ms step_avg:723.82ms
step:1567/2285 train_time:1134225ms step_avg:723.82ms
step:1568/2285 train_time:1134958ms step_avg:723.83ms
step:1569/2285 train_time:1135686ms step_avg:723.83ms
step:1570/2285 train_time:1136418ms step_avg:723.83ms
step:1571/2285 train_time:1137147ms step_avg:723.84ms
step:1572/2285 train_time:1137880ms step_avg:723.84ms
step:1573/2285 train_time:1138606ms step_avg:723.84ms
step:1574/2285 train_time:1139338ms step_avg:723.85ms
step:1575/2285 train_time:1140064ms step_avg:723.85ms
step:1576/2285 train_time:1140798ms step_avg:723.86ms
step:1577/2285 train_time:1141528ms step_avg:723.86ms
step:1578/2285 train_time:1142263ms step_avg:723.87ms
step:1579/2285 train_time:1142992ms step_avg:723.87ms
step:1580/2285 train_time:1143728ms step_avg:723.88ms
step:1581/2285 train_time:1144454ms step_avg:723.88ms
step:1582/2285 train_time:1145187ms step_avg:723.89ms
step:1583/2285 train_time:1145915ms step_avg:723.89ms
step:1584/2285 train_time:1146649ms step_avg:723.89ms
step:1585/2285 train_time:1147376ms step_avg:723.90ms
step:1586/2285 train_time:1148111ms step_avg:723.90ms
step:1587/2285 train_time:1148839ms step_avg:723.91ms
step:1588/2285 train_time:1149576ms step_avg:723.91ms
step:1589/2285 train_time:1150303ms step_avg:723.92ms
step:1590/2285 train_time:1151038ms step_avg:723.92ms
step:1591/2285 train_time:1151764ms step_avg:723.92ms
step:1592/2285 train_time:1152496ms step_avg:723.93ms
step:1593/2285 train_time:1153225ms step_avg:723.93ms
step:1594/2285 train_time:1153957ms step_avg:723.94ms
step:1595/2285 train_time:1154687ms step_avg:723.94ms
step:1596/2285 train_time:1155420ms step_avg:723.95ms
step:1597/2285 train_time:1156150ms step_avg:723.95ms
step:1598/2285 train_time:1156881ms step_avg:723.96ms
step:1599/2285 train_time:1157610ms step_avg:723.96ms
step:1600/2285 train_time:1158346ms step_avg:723.97ms
step:1600/2285 val_loss:3.5269 train_time:1158445ms step_avg:724.03ms
step:1601/2285 train_time:1159073ms step_avg:723.97ms
step:1602/2285 train_time:1159806ms step_avg:723.97ms
step:1603/2285 train_time:1160535ms step_avg:723.98ms
step:1604/2285 train_time:1161268ms step_avg:723.98ms
step:1605/2285 train_time:1161996ms step_avg:723.99ms
step:1606/2285 train_time:1162729ms step_avg:723.99ms
step:1607/2285 train_time:1163455ms step_avg:723.99ms
step:1608/2285 train_time:1164188ms step_avg:724.00ms
step:1609/2285 train_time:1164916ms step_avg:724.00ms
step:1610/2285 train_time:1165649ms step_avg:724.01ms
step:1611/2285 train_time:1166377ms step_avg:724.01ms
step:1612/2285 train_time:1167108ms step_avg:724.01ms
step:1613/2285 train_time:1167839ms step_avg:724.02ms
step:1614/2285 train_time:1168571ms step_avg:724.02ms
step:1615/2285 train_time:1169302ms step_avg:724.03ms
step:1616/2285 train_time:1170038ms step_avg:724.03ms
step:1617/2285 train_time:1170765ms step_avg:724.04ms
step:1618/2285 train_time:1171499ms step_avg:724.04ms
step:1619/2285 train_time:1172226ms step_avg:724.04ms
step:1620/2285 train_time:1172960ms step_avg:724.05ms
step:1621/2285 train_time:1173690ms step_avg:724.05ms
step:1622/2285 train_time:1174425ms step_avg:724.06ms
step:1623/2285 train_time:1175153ms step_avg:724.06ms
step:1624/2285 train_time:1175887ms step_avg:724.07ms
step:1625/2285 train_time:1176615ms step_avg:724.07ms
step:1626/2285 train_time:1177349ms step_avg:724.08ms
step:1627/2285 train_time:1178076ms step_avg:724.08ms
step:1628/2285 train_time:1178810ms step_avg:724.08ms
step:1629/2285 train_time:1179536ms step_avg:724.09ms
step:1630/2285 train_time:1180269ms step_avg:724.09ms
step:1631/2285 train_time:1180995ms step_avg:724.09ms
step:1632/2285 train_time:1181728ms step_avg:724.10ms
step:1633/2285 train_time:1182461ms step_avg:724.10ms
step:1634/2285 train_time:1183197ms step_avg:724.11ms
step:1635/2285 train_time:1183928ms step_avg:724.11ms
step:1636/2285 train_time:1184659ms step_avg:724.12ms
step:1637/2285 train_time:1185388ms step_avg:724.12ms
step:1638/2285 train_time:1186125ms step_avg:724.13ms
step:1639/2285 train_time:1186853ms step_avg:724.13ms
step:1640/2285 train_time:1187589ms step_avg:724.14ms
step:1641/2285 train_time:1188320ms step_avg:724.14ms
step:1642/2285 train_time:1189055ms step_avg:724.15ms
step:1643/2285 train_time:1189784ms step_avg:724.15ms
step:1644/2285 train_time:1190519ms step_avg:724.16ms
step:1645/2285 train_time:1191250ms step_avg:724.16ms
step:1646/2285 train_time:1191986ms step_avg:724.17ms
step:1647/2285 train_time:1192714ms step_avg:724.17ms
step:1648/2285 train_time:1193448ms step_avg:724.18ms
step:1649/2285 train_time:1194174ms step_avg:724.18ms
step:1650/2285 train_time:1194909ms step_avg:724.19ms
step:1651/2285 train_time:1195637ms step_avg:724.19ms
step:1652/2285 train_time:1196368ms step_avg:724.19ms
step:1653/2285 train_time:1197097ms step_avg:724.20ms
step:1654/2285 train_time:1197829ms step_avg:724.20ms
step:1655/2285 train_time:1198559ms step_avg:724.20ms
step:1656/2285 train_time:1199295ms step_avg:724.21ms
step:1657/2285 train_time:1200024ms step_avg:724.21ms
step:1658/2285 train_time:1200758ms step_avg:724.22ms
step:1659/2285 train_time:1201486ms step_avg:724.22ms
step:1660/2285 train_time:1202224ms step_avg:724.23ms
step:1661/2285 train_time:1202954ms step_avg:724.23ms
step:1662/2285 train_time:1203689ms step_avg:724.24ms
step:1663/2285 train_time:1204418ms step_avg:724.24ms
step:1664/2285 train_time:1205152ms step_avg:724.25ms
step:1665/2285 train_time:1205879ms step_avg:724.25ms
step:1666/2285 train_time:1206616ms step_avg:724.26ms
step:1667/2285 train_time:1207343ms step_avg:724.26ms
step:1668/2285 train_time:1208080ms step_avg:724.27ms
step:1669/2285 train_time:1208808ms step_avg:724.27ms
step:1670/2285 train_time:1209544ms step_avg:724.28ms
step:1671/2285 train_time:1210273ms step_avg:724.28ms
step:1672/2285 train_time:1211009ms step_avg:724.29ms
step:1673/2285 train_time:1211738ms step_avg:724.29ms
step:1674/2285 train_time:1212471ms step_avg:724.30ms
step:1675/2285 train_time:1213199ms step_avg:724.30ms
step:1676/2285 train_time:1213935ms step_avg:724.30ms
step:1677/2285 train_time:1214664ms step_avg:724.31ms
step:1678/2285 train_time:1215398ms step_avg:724.31ms
step:1679/2285 train_time:1216131ms step_avg:724.32ms
step:1680/2285 train_time:1216869ms step_avg:724.33ms
step:1681/2285 train_time:1217597ms step_avg:724.33ms
step:1682/2285 train_time:1218330ms step_avg:724.33ms
step:1683/2285 train_time:1219062ms step_avg:724.34ms
step:1684/2285 train_time:1219798ms step_avg:724.35ms
step:1685/2285 train_time:1220529ms step_avg:724.35ms
step:1686/2285 train_time:1221269ms step_avg:724.36ms
step:1687/2285 train_time:1221995ms step_avg:724.36ms
step:1688/2285 train_time:1222729ms step_avg:724.37ms
step:1689/2285 train_time:1223463ms step_avg:724.37ms
step:1690/2285 train_time:1224199ms step_avg:724.38ms
step:1691/2285 train_time:1224927ms step_avg:724.38ms
step:1692/2285 train_time:1225660ms step_avg:724.39ms
step:1693/2285 train_time:1226391ms step_avg:724.39ms
step:1694/2285 train_time:1227129ms step_avg:724.40ms
step:1695/2285 train_time:1227858ms step_avg:724.40ms
step:1696/2285 train_time:1228591ms step_avg:724.40ms
step:1697/2285 train_time:1229322ms step_avg:724.41ms
step:1698/2285 train_time:1230060ms step_avg:724.42ms
step:1699/2285 train_time:1230787ms step_avg:724.42ms
step:1700/2285 train_time:1231525ms step_avg:724.43ms
step:1700/2285 val_loss:3.5047 train_time:1231622ms step_avg:724.48ms
step:1701/2285 train_time:1232251ms step_avg:724.43ms
step:1702/2285 train_time:1232984ms step_avg:724.43ms
step:1703/2285 train_time:1233711ms step_avg:724.43ms
step:1704/2285 train_time:1234445ms step_avg:724.44ms
step:1705/2285 train_time:1235177ms step_avg:724.44ms
step:1706/2285 train_time:1235912ms step_avg:724.45ms
step:1707/2285 train_time:1236641ms step_avg:724.45ms
step:1708/2285 train_time:1237377ms step_avg:724.46ms
step:1709/2285 train_time:1238107ms step_avg:724.46ms
step:1710/2285 train_time:1238844ms step_avg:724.47ms
step:1711/2285 train_time:1239572ms step_avg:724.47ms
step:1712/2285 train_time:1240305ms step_avg:724.48ms
step:1713/2285 train_time:1241036ms step_avg:724.48ms
step:1714/2285 train_time:1241772ms step_avg:724.49ms
step:1715/2285 train_time:1242501ms step_avg:724.49ms
step:1716/2285 train_time:1243236ms step_avg:724.50ms
step:1717/2285 train_time:1243964ms step_avg:724.50ms
step:1718/2285 train_time:1244697ms step_avg:724.50ms
step:1719/2285 train_time:1245423ms step_avg:724.50ms
step:1720/2285 train_time:1246157ms step_avg:724.51ms
step:1721/2285 train_time:1246885ms step_avg:724.51ms
step:1722/2285 train_time:1247621ms step_avg:724.52ms
step:1723/2285 train_time:1248349ms step_avg:724.52ms
step:1724/2285 train_time:1249085ms step_avg:724.53ms
step:1725/2285 train_time:1249813ms step_avg:724.53ms
step:1726/2285 train_time:1250545ms step_avg:724.53ms
step:1727/2285 train_time:1251275ms step_avg:724.54ms
step:1728/2285 train_time:1252008ms step_avg:724.54ms
step:1729/2285 train_time:1252738ms step_avg:724.55ms
step:1730/2285 train_time:1253476ms step_avg:724.55ms
step:1731/2285 train_time:1254204ms step_avg:724.55ms
step:1732/2285 train_time:1254943ms step_avg:724.56ms
step:1733/2285 train_time:1255670ms step_avg:724.56ms
step:1734/2285 train_time:1256405ms step_avg:724.57ms
step:1735/2285 train_time:1257132ms step_avg:724.57ms
step:1736/2285 train_time:1257865ms step_avg:724.58ms
step:1737/2285 train_time:1258592ms step_avg:724.58ms
step:1738/2285 train_time:1259327ms step_avg:724.58ms
step:1739/2285 train_time:1260055ms step_avg:724.59ms
step:1740/2285 train_time:1260788ms step_avg:724.59ms
step:1741/2285 train_time:1261518ms step_avg:724.59ms
step:1742/2285 train_time:1262254ms step_avg:724.60ms
step:1743/2285 train_time:1262984ms step_avg:724.60ms
step:1744/2285 train_time:1263719ms step_avg:724.61ms
step:1745/2285 train_time:1264449ms step_avg:724.61ms
step:1746/2285 train_time:1265186ms step_avg:724.62ms
step:1747/2285 train_time:1265913ms step_avg:724.62ms
step:1748/2285 train_time:1266644ms step_avg:724.62ms
step:1749/2285 train_time:1267373ms step_avg:724.63ms
step:1750/2285 train_time:1268106ms step_avg:724.63ms
step:1751/2285 train_time:1268836ms step_avg:724.63ms
step:1752/2285 train_time:1269573ms step_avg:724.64ms
step:1753/2285 train_time:1270301ms step_avg:724.64ms
step:1754/2285 train_time:1271036ms step_avg:724.65ms
step:1755/2285 train_time:1271765ms step_avg:724.65ms
step:1756/2285 train_time:1272503ms step_avg:724.66ms
step:1757/2285 train_time:1273231ms step_avg:724.66ms
step:1758/2285 train_time:1273964ms step_avg:724.67ms
step:1759/2285 train_time:1274690ms step_avg:724.67ms
step:1760/2285 train_time:1275425ms step_avg:724.67ms
step:1761/2285 train_time:1276152ms step_avg:724.67ms
step:1762/2285 train_time:1276885ms step_avg:724.68ms
step:1763/2285 train_time:1277612ms step_avg:724.68ms
step:1764/2285 train_time:1278348ms step_avg:724.69ms
step:1765/2285 train_time:1279078ms step_avg:724.69ms
step:1766/2285 train_time:1279813ms step_avg:724.70ms
step:1767/2285 train_time:1280541ms step_avg:724.70ms
step:1768/2285 train_time:1281275ms step_avg:724.70ms
step:1769/2285 train_time:1282004ms step_avg:724.71ms
step:1770/2285 train_time:1282737ms step_avg:724.71ms
step:1771/2285 train_time:1283465ms step_avg:724.71ms
step:1772/2285 train_time:1284196ms step_avg:724.72ms
step:1773/2285 train_time:1284923ms step_avg:724.72ms
step:1774/2285 train_time:1285658ms step_avg:724.72ms
step:1775/2285 train_time:1286382ms step_avg:724.72ms
step:1776/2285 train_time:1287117ms step_avg:724.73ms
step:1777/2285 train_time:1287846ms step_avg:724.73ms
step:1778/2285 train_time:1288585ms step_avg:724.74ms
step:1779/2285 train_time:1289313ms step_avg:724.74ms
step:1780/2285 train_time:1290046ms step_avg:724.75ms
step:1781/2285 train_time:1290779ms step_avg:724.75ms
step:1782/2285 train_time:1291512ms step_avg:724.75ms
step:1783/2285 train_time:1292240ms step_avg:724.76ms
step:1784/2285 train_time:1292976ms step_avg:724.76ms
step:1785/2285 train_time:1293702ms step_avg:724.76ms
step:1786/2285 train_time:1294437ms step_avg:724.77ms
step:1787/2285 train_time:1295162ms step_avg:724.77ms
step:1788/2285 train_time:1295897ms step_avg:724.77ms
step:1789/2285 train_time:1296627ms step_avg:724.78ms
step:1790/2285 train_time:1297364ms step_avg:724.78ms
step:1791/2285 train_time:1298092ms step_avg:724.79ms
step:1792/2285 train_time:1298824ms step_avg:724.79ms
step:1793/2285 train_time:1299551ms step_avg:724.79ms
step:1794/2285 train_time:1300287ms step_avg:724.80ms
step:1795/2285 train_time:1301014ms step_avg:724.80ms
step:1796/2285 train_time:1301746ms step_avg:724.80ms
step:1797/2285 train_time:1302473ms step_avg:724.80ms
step:1798/2285 train_time:1303206ms step_avg:724.81ms
step:1799/2285 train_time:1303931ms step_avg:724.81ms
step:1800/2285 train_time:1304667ms step_avg:724.81ms
step:1800/2285 val_loss:3.4849 train_time:1304763ms step_avg:724.87ms
step:1801/2285 train_time:1305393ms step_avg:724.82ms
step:1802/2285 train_time:1306129ms step_avg:724.82ms
step:1803/2285 train_time:1306861ms step_avg:724.83ms
step:1804/2285 train_time:1307596ms step_avg:724.83ms
step:1805/2285 train_time:1308325ms step_avg:724.83ms
step:1806/2285 train_time:1309057ms step_avg:724.84ms
step:1807/2285 train_time:1309785ms step_avg:724.84ms
step:1808/2285 train_time:1310520ms step_avg:724.84ms
step:1809/2285 train_time:1311247ms step_avg:724.85ms
step:1810/2285 train_time:1311982ms step_avg:724.85ms
step:1811/2285 train_time:1312712ms step_avg:724.85ms
step:1812/2285 train_time:1313448ms step_avg:724.86ms
step:1813/2285 train_time:1314174ms step_avg:724.86ms
step:1814/2285 train_time:1314909ms step_avg:724.87ms
step:1815/2285 train_time:1315636ms step_avg:724.87ms
step:1816/2285 train_time:1316368ms step_avg:724.87ms
step:1817/2285 train_time:1317098ms step_avg:724.88ms
step:1818/2285 train_time:1317835ms step_avg:724.88ms
step:1819/2285 train_time:1318564ms step_avg:724.88ms
step:1820/2285 train_time:1319298ms step_avg:724.89ms
step:1821/2285 train_time:1320024ms step_avg:724.89ms
step:1822/2285 train_time:1320759ms step_avg:724.89ms
step:1823/2285 train_time:1321484ms step_avg:724.90ms
step:1824/2285 train_time:1322220ms step_avg:724.90ms
step:1825/2285 train_time:1322946ms step_avg:724.90ms
step:1826/2285 train_time:1323680ms step_avg:724.91ms
step:1827/2285 train_time:1324407ms step_avg:724.91ms
step:1828/2285 train_time:1325141ms step_avg:724.91ms
step:1829/2285 train_time:1325870ms step_avg:724.92ms
step:1830/2285 train_time:1326608ms step_avg:724.92ms
step:1831/2285 train_time:1327335ms step_avg:724.92ms
step:1832/2285 train_time:1328069ms step_avg:724.93ms
step:1833/2285 train_time:1328798ms step_avg:724.93ms
step:1834/2285 train_time:1329531ms step_avg:724.94ms
step:1835/2285 train_time:1330254ms step_avg:724.93ms
step:1836/2285 train_time:1330990ms step_avg:724.94ms
step:1837/2285 train_time:1331720ms step_avg:724.94ms
step:1838/2285 train_time:1332453ms step_avg:724.95ms
step:1839/2285 train_time:1333182ms step_avg:724.95ms
step:1840/2285 train_time:1333918ms step_avg:724.96ms
step:1841/2285 train_time:1334646ms step_avg:724.96ms
step:1842/2285 train_time:1335378ms step_avg:724.96ms
step:1843/2285 train_time:1336107ms step_avg:724.96ms
step:1844/2285 train_time:1336840ms step_avg:724.97ms
step:1845/2285 train_time:1337566ms step_avg:724.97ms
step:1846/2285 train_time:1338301ms step_avg:724.97ms
step:1847/2285 train_time:1339030ms step_avg:724.98ms
step:1848/2285 train_time:1339766ms step_avg:724.98ms
step:1849/2285 train_time:1340493ms step_avg:724.98ms
step:1850/2285 train_time:1341230ms step_avg:724.99ms
step:1851/2285 train_time:1341960ms step_avg:724.99ms
step:1852/2285 train_time:1342698ms step_avg:725.00ms
step:1853/2285 train_time:1343425ms step_avg:725.00ms
step:1854/2285 train_time:1344159ms step_avg:725.01ms
step:1855/2285 train_time:1344885ms step_avg:725.01ms
step:1856/2285 train_time:1345620ms step_avg:725.01ms
step:1857/2285 train_time:1346347ms step_avg:725.01ms
step:1858/2285 train_time:1347083ms step_avg:725.02ms
step:1859/2285 train_time:1347813ms step_avg:725.02ms
step:1860/2285 train_time:1348550ms step_avg:725.03ms
step:1861/2285 train_time:1349278ms step_avg:725.03ms
step:1862/2285 train_time:1350011ms step_avg:725.03ms
step:1863/2285 train_time:1350742ms step_avg:725.04ms
step:1864/2285 train_time:1351475ms step_avg:725.04ms
step:1865/2285 train_time:1352204ms step_avg:725.04ms
step:1866/2285 train_time:1352939ms step_avg:725.05ms
step:1867/2285 train_time:1353669ms step_avg:725.05ms
step:1868/2285 train_time:1354405ms step_avg:725.06ms
step:1869/2285 train_time:1355135ms step_avg:725.06ms
step:1870/2285 train_time:1355870ms step_avg:725.06ms
step:1871/2285 train_time:1356597ms step_avg:725.07ms
step:1872/2285 train_time:1357330ms step_avg:725.07ms
step:1873/2285 train_time:1358057ms step_avg:725.07ms
step:1874/2285 train_time:1358789ms step_avg:725.07ms
step:1875/2285 train_time:1359516ms step_avg:725.08ms
step:1876/2285 train_time:1360249ms step_avg:725.08ms
step:1877/2285 train_time:1360979ms step_avg:725.08ms
step:1878/2285 train_time:1361712ms step_avg:725.09ms
step:1879/2285 train_time:1362443ms step_avg:725.09ms
step:1880/2285 train_time:1363178ms step_avg:725.09ms
step:1881/2285 train_time:1363905ms step_avg:725.10ms
step:1882/2285 train_time:1364639ms step_avg:725.10ms
step:1883/2285 train_time:1365372ms step_avg:725.10ms
step:1884/2285 train_time:1366107ms step_avg:725.11ms
step:1885/2285 train_time:1366834ms step_avg:725.11ms
step:1886/2285 train_time:1367569ms step_avg:725.12ms
step:1887/2285 train_time:1368297ms step_avg:725.12ms
step:1888/2285 train_time:1369031ms step_avg:725.12ms
step:1889/2285 train_time:1369757ms step_avg:725.12ms
step:1890/2285 train_time:1370491ms step_avg:725.13ms
step:1891/2285 train_time:1371223ms step_avg:725.13ms
step:1892/2285 train_time:1371959ms step_avg:725.14ms
step:1893/2285 train_time:1372688ms step_avg:725.14ms
step:1894/2285 train_time:1373423ms step_avg:725.14ms
step:1895/2285 train_time:1374148ms step_avg:725.14ms
step:1896/2285 train_time:1374880ms step_avg:725.15ms
step:1897/2285 train_time:1375609ms step_avg:725.15ms
step:1898/2285 train_time:1376344ms step_avg:725.15ms
step:1899/2285 train_time:1377073ms step_avg:725.16ms
step:1900/2285 train_time:1377811ms step_avg:725.16ms
step:1900/2285 val_loss:3.4667 train_time:1377908ms step_avg:725.21ms
step:1901/2285 train_time:1378537ms step_avg:725.16ms
step:1902/2285 train_time:1379271ms step_avg:725.17ms
step:1903/2285 train_time:1380004ms step_avg:725.17ms
step:1904/2285 train_time:1380742ms step_avg:725.18ms
step:1905/2285 train_time:1381469ms step_avg:725.18ms
step:1906/2285 train_time:1382203ms step_avg:725.19ms
step:1907/2285 train_time:1382933ms step_avg:725.19ms
step:1908/2285 train_time:1383669ms step_avg:725.19ms
step:1909/2285 train_time:1384396ms step_avg:725.19ms
step:1910/2285 train_time:1385131ms step_avg:725.20ms
step:1911/2285 train_time:1385857ms step_avg:725.20ms
step:1912/2285 train_time:1386591ms step_avg:725.20ms
step:1913/2285 train_time:1387322ms step_avg:725.21ms
step:1914/2285 train_time:1388058ms step_avg:725.21ms
step:1915/2285 train_time:1388786ms step_avg:725.21ms
step:1916/2285 train_time:1389521ms step_avg:725.22ms
step:1917/2285 train_time:1390247ms step_avg:725.22ms
step:1918/2285 train_time:1390981ms step_avg:725.22ms
step:1919/2285 train_time:1391711ms step_avg:725.23ms
step:1920/2285 train_time:1392449ms step_avg:725.23ms
step:1921/2285 train_time:1393176ms step_avg:725.23ms
step:1922/2285 train_time:1393911ms step_avg:725.24ms
step:1923/2285 train_time:1394641ms step_avg:725.24ms
step:1924/2285 train_time:1395374ms step_avg:725.25ms
step:1925/2285 train_time:1396105ms step_avg:725.25ms
step:1926/2285 train_time:1396842ms step_avg:725.26ms
step:1927/2285 train_time:1397568ms step_avg:725.26ms
step:1928/2285 train_time:1398301ms step_avg:725.26ms
step:1929/2285 train_time:1399034ms step_avg:725.26ms
step:1930/2285 train_time:1399770ms step_avg:725.27ms
step:1931/2285 train_time:1400496ms step_avg:725.27ms
step:1932/2285 train_time:1401231ms step_avg:725.27ms
step:1933/2285 train_time:1401960ms step_avg:725.28ms
step:1934/2285 train_time:1402692ms step_avg:725.28ms
step:1935/2285 train_time:1403424ms step_avg:725.28ms
step:1936/2285 train_time:1404161ms step_avg:725.29ms
step:1937/2285 train_time:1404889ms step_avg:725.29ms
step:1938/2285 train_time:1405622ms step_avg:725.30ms
step:1939/2285 train_time:1406350ms step_avg:725.30ms
step:1940/2285 train_time:1407086ms step_avg:725.30ms
step:1941/2285 train_time:1407815ms step_avg:725.30ms
step:1942/2285 train_time:1408551ms step_avg:725.31ms
step:1943/2285 train_time:1409281ms step_avg:725.31ms
step:1944/2285 train_time:1410013ms step_avg:725.32ms
step:1945/2285 train_time:1410744ms step_avg:725.32ms
step:1946/2285 train_time:1411479ms step_avg:725.32ms
step:1947/2285 train_time:1412207ms step_avg:725.32ms
step:1948/2285 train_time:1412941ms step_avg:725.33ms
step:1949/2285 train_time:1413667ms step_avg:725.33ms
step:1950/2285 train_time:1414402ms step_avg:725.33ms
step:1951/2285 train_time:1415132ms step_avg:725.34ms
step:1952/2285 train_time:1415868ms step_avg:725.34ms
step:1953/2285 train_time:1416596ms step_avg:725.34ms
step:1954/2285 train_time:1417332ms step_avg:725.35ms
step:1955/2285 train_time:1418058ms step_avg:725.35ms
step:1956/2285 train_time:1418793ms step_avg:725.35ms
step:1957/2285 train_time:1419519ms step_avg:725.35ms
step:1958/2285 train_time:1420253ms step_avg:725.36ms
step:1959/2285 train_time:1420982ms step_avg:725.36ms
step:1960/2285 train_time:1421720ms step_avg:725.37ms
step:1961/2285 train_time:1422450ms step_avg:725.37ms
step:1962/2285 train_time:1423186ms step_avg:725.38ms
step:1963/2285 train_time:1423915ms step_avg:725.38ms
step:1964/2285 train_time:1424652ms step_avg:725.38ms
step:1965/2285 train_time:1425378ms step_avg:725.38ms
step:1966/2285 train_time:1426112ms step_avg:725.39ms
step:1967/2285 train_time:1426839ms step_avg:725.39ms
step:1968/2285 train_time:1427572ms step_avg:725.39ms
step:1969/2285 train_time:1428301ms step_avg:725.39ms
step:1970/2285 train_time:1429033ms step_avg:725.40ms
step:1971/2285 train_time:1429762ms step_avg:725.40ms
step:1972/2285 train_time:1430496ms step_avg:725.40ms
step:1973/2285 train_time:1431227ms step_avg:725.41ms
step:1974/2285 train_time:1431961ms step_avg:725.41ms
step:1975/2285 train_time:1432689ms step_avg:725.41ms
step:1976/2285 train_time:1433423ms step_avg:725.42ms
step:1977/2285 train_time:1434149ms step_avg:725.42ms
step:1978/2285 train_time:1434883ms step_avg:725.42ms
step:1979/2285 train_time:1435612ms step_avg:725.42ms
step:1980/2285 train_time:1436349ms step_avg:725.43ms
step:1981/2285 train_time:1437076ms step_avg:725.43ms
step:1982/2285 train_time:1437810ms step_avg:725.43ms
step:1983/2285 train_time:1438538ms step_avg:725.44ms
step:1984/2285 train_time:1439272ms step_avg:725.44ms
step:1985/2285 train_time:1440000ms step_avg:725.44ms
step:1986/2285 train_time:1440733ms step_avg:725.44ms
step:1987/2285 train_time:1441459ms step_avg:725.45ms
step:1988/2285 train_time:1442192ms step_avg:725.45ms
step:1989/2285 train_time:1442920ms step_avg:725.45ms
step:1990/2285 train_time:1443653ms step_avg:725.45ms
step:1991/2285 train_time:1444379ms step_avg:725.45ms
step:1992/2285 train_time:1445112ms step_avg:725.46ms
step:1993/2285 train_time:1445842ms step_avg:725.46ms
step:1994/2285 train_time:1446578ms step_avg:725.47ms
step:1995/2285 train_time:1447307ms step_avg:725.47ms
step:1996/2285 train_time:1448043ms step_avg:725.47ms
step:1997/2285 train_time:1448771ms step_avg:725.47ms
step:1998/2285 train_time:1449510ms step_avg:725.48ms
step:1999/2285 train_time:1450239ms step_avg:725.48ms
step:2000/2285 train_time:1450972ms step_avg:725.49ms
step:2000/2285 val_loss:3.4502 train_time:1451069ms step_avg:725.53ms
step:2001/2285 train_time:1451700ms step_avg:725.49ms
step:2002/2285 train_time:1452435ms step_avg:725.49ms
step:2003/2285 train_time:1453163ms step_avg:725.49ms
step:2004/2285 train_time:1453897ms step_avg:725.50ms
step:2005/2285 train_time:1454625ms step_avg:725.50ms
step:2006/2285 train_time:1455362ms step_avg:725.50ms
step:2007/2285 train_time:1456089ms step_avg:725.51ms
step:2008/2285 train_time:1456825ms step_avg:725.51ms
step:2009/2285 train_time:1457553ms step_avg:725.51ms
step:2010/2285 train_time:1458286ms step_avg:725.52ms
step:2011/2285 train_time:1459014ms step_avg:725.52ms
step:2012/2285 train_time:1459745ms step_avg:725.52ms
step:2013/2285 train_time:1460473ms step_avg:725.52ms
step:2014/2285 train_time:1461206ms step_avg:725.52ms
step:2015/2285 train_time:1461938ms step_avg:725.53ms
step:2016/2285 train_time:1462675ms step_avg:725.53ms
step:2017/2285 train_time:1463402ms step_avg:725.53ms
step:2018/2285 train_time:1464135ms step_avg:725.54ms
step:2019/2285 train_time:1464863ms step_avg:725.54ms
step:2020/2285 train_time:1465599ms step_avg:725.54ms
step:2021/2285 train_time:1466329ms step_avg:725.55ms
step:2022/2285 train_time:1467066ms step_avg:725.55ms
step:2023/2285 train_time:1467798ms step_avg:725.56ms
step:2024/2285 train_time:1468532ms step_avg:725.56ms
step:2025/2285 train_time:1469261ms step_avg:725.56ms
step:2026/2285 train_time:1469996ms step_avg:725.57ms
step:2027/2285 train_time:1470724ms step_avg:725.57ms
step:2028/2285 train_time:1471457ms step_avg:725.57ms
step:2029/2285 train_time:1472182ms step_avg:725.57ms
step:2030/2285 train_time:1472916ms step_avg:725.57ms
step:2031/2285 train_time:1473647ms step_avg:725.58ms
step:2032/2285 train_time:1474383ms step_avg:725.58ms
step:2033/2285 train_time:1475109ms step_avg:725.58ms
step:2034/2285 train_time:1475846ms step_avg:725.59ms
step:2035/2285 train_time:1476573ms step_avg:725.59ms
step:2036/2285 train_time:1477307ms step_avg:725.59ms
step:2037/2285 train_time:1478037ms step_avg:725.59ms
step:2038/2285 train_time:1478773ms step_avg:725.60ms
step:2039/2285 train_time:1479501ms step_avg:725.60ms
step:2040/2285 train_time:1480236ms step_avg:725.61ms
step:2041/2285 train_time:1480966ms step_avg:725.61ms
step:2042/2285 train_time:1481698ms step_avg:725.61ms
step:2043/2285 train_time:1482428ms step_avg:725.61ms
step:2044/2285 train_time:1483166ms step_avg:725.62ms
step:2045/2285 train_time:1483896ms step_avg:725.62ms
step:2046/2285 train_time:1484635ms step_avg:725.63ms
step:2047/2285 train_time:1485363ms step_avg:725.63ms
step:2048/2285 train_time:1486097ms step_avg:725.63ms
step:2049/2285 train_time:1486827ms step_avg:725.64ms
step:2050/2285 train_time:1487566ms step_avg:725.64ms
step:2051/2285 train_time:1488294ms step_avg:725.64ms
step:2052/2285 train_time:1489028ms step_avg:725.65ms
step:2053/2285 train_time:1489758ms step_avg:725.65ms
step:2054/2285 train_time:1490494ms step_avg:725.65ms
step:2055/2285 train_time:1491221ms step_avg:725.65ms
step:2056/2285 train_time:1491957ms step_avg:725.66ms
step:2057/2285 train_time:1492684ms step_avg:725.66ms
step:2058/2285 train_time:1493419ms step_avg:725.67ms
step:2059/2285 train_time:1494146ms step_avg:725.67ms
step:2060/2285 train_time:1494882ms step_avg:725.67ms
step:2061/2285 train_time:1495610ms step_avg:725.67ms
step:2062/2285 train_time:1496347ms step_avg:725.68ms
step:2063/2285 train_time:1497078ms step_avg:725.68ms
step:2064/2285 train_time:1497814ms step_avg:725.68ms
step:2065/2285 train_time:1498544ms step_avg:725.69ms
step:2066/2285 train_time:1499278ms step_avg:725.69ms
step:2067/2285 train_time:1500008ms step_avg:725.69ms
step:2068/2285 train_time:1500746ms step_avg:725.70ms
step:2069/2285 train_time:1501473ms step_avg:725.70ms
step:2070/2285 train_time:1502207ms step_avg:725.70ms
step:2071/2285 train_time:1502939ms step_avg:725.71ms
step:2072/2285 train_time:1503677ms step_avg:725.71ms
step:2073/2285 train_time:1504404ms step_avg:725.71ms
step:2074/2285 train_time:1505138ms step_avg:725.72ms
step:2075/2285 train_time:1505865ms step_avg:725.72ms
step:2076/2285 train_time:1506599ms step_avg:725.72ms
step:2077/2285 train_time:1507324ms step_avg:725.72ms
step:2078/2285 train_time:1508058ms step_avg:725.73ms
step:2079/2285 train_time:1508789ms step_avg:725.73ms
step:2080/2285 train_time:1509526ms step_avg:725.73ms
step:2081/2285 train_time:1510251ms step_avg:725.73ms
step:2082/2285 train_time:1510986ms step_avg:725.74ms
step:2083/2285 train_time:1511714ms step_avg:725.74ms
step:2084/2285 train_time:1512449ms step_avg:725.74ms
step:2085/2285 train_time:1513177ms step_avg:725.74ms
step:2086/2285 train_time:1513913ms step_avg:725.75ms
step:2087/2285 train_time:1514642ms step_avg:725.75ms
step:2088/2285 train_time:1515377ms step_avg:725.76ms
step:2089/2285 train_time:1516105ms step_avg:725.76ms
step:2090/2285 train_time:1516837ms step_avg:725.76ms
step:2091/2285 train_time:1517563ms step_avg:725.76ms
step:2092/2285 train_time:1518299ms step_avg:725.76ms
step:2093/2285 train_time:1519027ms step_avg:725.77ms
step:2094/2285 train_time:1519767ms step_avg:725.77ms
step:2095/2285 train_time:1520492ms step_avg:725.77ms
step:2096/2285 train_time:1521226ms step_avg:725.78ms
step:2097/2285 train_time:1521954ms step_avg:725.78ms
step:2098/2285 train_time:1522688ms step_avg:725.78ms
step:2099/2285 train_time:1523417ms step_avg:725.78ms
step:2100/2285 train_time:1524148ms step_avg:725.78ms
step:2100/2285 val_loss:3.4357 train_time:1524246ms step_avg:725.83ms
step:2101/2285 train_time:1524876ms step_avg:725.79ms
step:2102/2285 train_time:1525612ms step_avg:725.79ms
step:2103/2285 train_time:1526341ms step_avg:725.79ms
step:2104/2285 train_time:1527073ms step_avg:725.80ms
step:2105/2285 train_time:1527806ms step_avg:725.80ms
step:2106/2285 train_time:1528542ms step_avg:725.80ms
step:2107/2285 train_time:1529269ms step_avg:725.80ms
step:2108/2285 train_time:1530003ms step_avg:725.81ms
step:2109/2285 train_time:1530732ms step_avg:725.81ms
step:2110/2285 train_time:1531472ms step_avg:725.82ms
step:2111/2285 train_time:1532199ms step_avg:725.82ms
step:2112/2285 train_time:1532934ms step_avg:725.82ms
step:2113/2285 train_time:1533664ms step_avg:725.82ms
step:2114/2285 train_time:1534396ms step_avg:725.83ms
step:2115/2285 train_time:1535127ms step_avg:725.83ms
step:2116/2285 train_time:1535863ms step_avg:725.83ms
step:2117/2285 train_time:1536591ms step_avg:725.83ms
step:2118/2285 train_time:1537324ms step_avg:725.84ms
step:2119/2285 train_time:1538051ms step_avg:725.84ms
step:2120/2285 train_time:1538784ms step_avg:725.84ms
step:2121/2285 train_time:1539513ms step_avg:725.84ms
step:2122/2285 train_time:1540250ms step_avg:725.85ms
step:2123/2285 train_time:1540978ms step_avg:725.85ms
step:2124/2285 train_time:1541712ms step_avg:725.85ms
step:2125/2285 train_time:1542440ms step_avg:725.85ms
step:2126/2285 train_time:1543174ms step_avg:725.86ms
step:2127/2285 train_time:1543898ms step_avg:725.86ms
step:2128/2285 train_time:1544633ms step_avg:725.86ms
step:2129/2285 train_time:1545361ms step_avg:725.86ms
step:2130/2285 train_time:1546094ms step_avg:725.87ms
step:2131/2285 train_time:1546826ms step_avg:725.87ms
step:2132/2285 train_time:1547563ms step_avg:725.87ms
step:2133/2285 train_time:1548290ms step_avg:725.87ms
step:2134/2285 train_time:1549024ms step_avg:725.88ms
step:2135/2285 train_time:1549757ms step_avg:725.88ms
step:2136/2285 train_time:1550493ms step_avg:725.89ms
step:2137/2285 train_time:1551218ms step_avg:725.89ms
step:2138/2285 train_time:1551954ms step_avg:725.89ms
step:2139/2285 train_time:1552680ms step_avg:725.89ms
step:2140/2285 train_time:1553415ms step_avg:725.89ms
step:2141/2285 train_time:1554144ms step_avg:725.90ms
step:2142/2285 train_time:1554878ms step_avg:725.90ms
step:2143/2285 train_time:1555608ms step_avg:725.90ms
step:2144/2285 train_time:1556343ms step_avg:725.91ms
step:2145/2285 train_time:1557072ms step_avg:725.91ms
step:2146/2285 train_time:1557805ms step_avg:725.91ms
step:2147/2285 train_time:1558535ms step_avg:725.91ms
step:2148/2285 train_time:1559271ms step_avg:725.92ms
step:2149/2285 train_time:1560000ms step_avg:725.92ms
step:2150/2285 train_time:1560734ms step_avg:725.92ms
step:2151/2285 train_time:1561461ms step_avg:725.92ms
step:2152/2285 train_time:1562194ms step_avg:725.93ms
step:2153/2285 train_time:1562924ms step_avg:725.93ms
step:2154/2285 train_time:1563661ms step_avg:725.93ms
step:2155/2285 train_time:1564389ms step_avg:725.93ms
step:2156/2285 train_time:1565125ms step_avg:725.94ms
step:2157/2285 train_time:1565850ms step_avg:725.94ms
step:2158/2285 train_time:1566586ms step_avg:725.94ms
step:2159/2285 train_time:1567311ms step_avg:725.94ms
step:2160/2285 train_time:1568046ms step_avg:725.95ms
step:2161/2285 train_time:1568778ms step_avg:725.95ms
step:2162/2285 train_time:1569511ms step_avg:725.95ms
step:2163/2285 train_time:1570239ms step_avg:725.95ms
step:2164/2285 train_time:1570974ms step_avg:725.96ms
step:2165/2285 train_time:1571705ms step_avg:725.96ms
step:2166/2285 train_time:1572440ms step_avg:725.96ms
step:2167/2285 train_time:1573169ms step_avg:725.97ms
step:2168/2285 train_time:1573904ms step_avg:725.97ms
step:2169/2285 train_time:1574630ms step_avg:725.97ms
step:2170/2285 train_time:1575365ms step_avg:725.97ms
step:2171/2285 train_time:1576091ms step_avg:725.97ms
step:2172/2285 train_time:1576825ms step_avg:725.98ms
step:2173/2285 train_time:1577555ms step_avg:725.98ms
step:2174/2285 train_time:1578292ms step_avg:725.99ms
step:2175/2285 train_time:1579020ms step_avg:725.99ms
step:2176/2285 train_time:1579754ms step_avg:725.99ms
step:2177/2285 train_time:1580482ms step_avg:725.99ms
step:2178/2285 train_time:1581214ms step_avg:725.99ms
step:2179/2285 train_time:1581942ms step_avg:725.99ms
step:2180/2285 train_time:1582675ms step_avg:726.00ms
step:2181/2285 train_time:1583401ms step_avg:726.00ms
step:2182/2285 train_time:1584135ms step_avg:726.00ms
step:2183/2285 train_time:1584867ms step_avg:726.00ms
step:2184/2285 train_time:1585603ms step_avg:726.01ms
step:2185/2285 train_time:1586333ms step_avg:726.01ms
step:2186/2285 train_time:1587069ms step_avg:726.01ms
step:2187/2285 train_time:1587797ms step_avg:726.02ms
step:2188/2285 train_time:1588535ms step_avg:726.02ms
step:2189/2285 train_time:1589266ms step_avg:726.02ms
step:2190/2285 train_time:1589998ms step_avg:726.03ms
step:2191/2285 train_time:1590727ms step_avg:726.03ms
step:2192/2285 train_time:1591464ms step_avg:726.03ms
step:2193/2285 train_time:1592194ms step_avg:726.03ms
step:2194/2285 train_time:1592929ms step_avg:726.04ms
step:2195/2285 train_time:1593657ms step_avg:726.04ms
step:2196/2285 train_time:1594394ms step_avg:726.04ms
step:2197/2285 train_time:1595120ms step_avg:726.04ms
step:2198/2285 train_time:1595855ms step_avg:726.05ms
step:2199/2285 train_time:1596582ms step_avg:726.05ms
step:2200/2285 train_time:1597315ms step_avg:726.05ms
step:2200/2285 val_loss:3.4231 train_time:1597413ms step_avg:726.10ms
step:2201/2285 train_time:1598041ms step_avg:726.05ms
step:2202/2285 train_time:1598777ms step_avg:726.06ms
step:2203/2285 train_time:1599504ms step_avg:726.06ms
step:2204/2285 train_time:1600237ms step_avg:726.06ms
step:2205/2285 train_time:1600966ms step_avg:726.06ms
step:2206/2285 train_time:1601698ms step_avg:726.06ms
step:2207/2285 train_time:1602425ms step_avg:726.06ms
step:2208/2285 train_time:1603159ms step_avg:726.07ms
step:2209/2285 train_time:1603887ms step_avg:726.07ms
step:2210/2285 train_time:1604619ms step_avg:726.07ms
step:2211/2285 train_time:1605346ms step_avg:726.07ms
step:2212/2285 train_time:1606081ms step_avg:726.08ms
step:2213/2285 train_time:1606810ms step_avg:726.08ms
step:2214/2285 train_time:1607547ms step_avg:726.08ms
step:2215/2285 train_time:1608274ms step_avg:726.08ms
step:2216/2285 train_time:1609009ms step_avg:726.09ms
step:2217/2285 train_time:1609739ms step_avg:726.09ms
step:2218/2285 train_time:1610475ms step_avg:726.09ms
step:2219/2285 train_time:1611204ms step_avg:726.09ms
step:2220/2285 train_time:1611937ms step_avg:726.10ms
step:2221/2285 train_time:1612663ms step_avg:726.10ms
step:2222/2285 train_time:1613399ms step_avg:726.10ms
step:2223/2285 train_time:1614124ms step_avg:726.10ms
step:2224/2285 train_time:1614859ms step_avg:726.11ms
step:2225/2285 train_time:1615583ms step_avg:726.10ms
step:2226/2285 train_time:1616318ms step_avg:726.11ms
step:2227/2285 train_time:1617043ms step_avg:726.11ms
step:2228/2285 train_time:1617778ms step_avg:726.11ms
step:2229/2285 train_time:1618507ms step_avg:726.11ms
step:2230/2285 train_time:1619240ms step_avg:726.12ms
step:2231/2285 train_time:1619970ms step_avg:726.12ms
step:2232/2285 train_time:1620708ms step_avg:726.12ms
step:2233/2285 train_time:1621435ms step_avg:726.12ms
step:2234/2285 train_time:1622168ms step_avg:726.13ms
step:2235/2285 train_time:1622897ms step_avg:726.13ms
step:2236/2285 train_time:1623629ms step_avg:726.13ms
step:2237/2285 train_time:1624357ms step_avg:726.13ms
step:2238/2285 train_time:1625091ms step_avg:726.14ms
step:2239/2285 train_time:1625815ms step_avg:726.13ms
step:2240/2285 train_time:1626548ms step_avg:726.14ms
step:2241/2285 train_time:1627276ms step_avg:726.14ms
step:2242/2285 train_time:1628009ms step_avg:726.14ms
step:2243/2285 train_time:1628737ms step_avg:726.14ms
step:2244/2285 train_time:1629468ms step_avg:726.14ms
step:2245/2285 train_time:1630196ms step_avg:726.15ms
step:2246/2285 train_time:1630931ms step_avg:726.15ms
step:2247/2285 train_time:1631665ms step_avg:726.15ms
step:2248/2285 train_time:1632402ms step_avg:726.16ms
step:2249/2285 train_time:1633131ms step_avg:726.16ms
step:2250/2285 train_time:1633869ms step_avg:726.16ms
step:2251/2285 train_time:1634596ms step_avg:726.16ms
step:2252/2285 train_time:1635330ms step_avg:726.17ms
step:2253/2285 train_time:1636062ms step_avg:726.17ms
step:2254/2285 train_time:1636800ms step_avg:726.18ms
step:2255/2285 train_time:1637531ms step_avg:726.18ms
step:2256/2285 train_time:1638270ms step_avg:726.18ms
step:2257/2285 train_time:1639002ms step_avg:726.19ms
step:2258/2285 train_time:1639739ms step_avg:726.19ms
step:2259/2285 train_time:1640471ms step_avg:726.19ms
step:2260/2285 train_time:1641207ms step_avg:726.20ms
step:2261/2285 train_time:1641934ms step_avg:726.20ms
step:2262/2285 train_time:1642671ms step_avg:726.20ms
step:2263/2285 train_time:1643404ms step_avg:726.21ms
step:2264/2285 train_time:1644139ms step_avg:726.21ms
step:2265/2285 train_time:1644870ms step_avg:726.21ms
step:2266/2285 train_time:1645609ms step_avg:726.22ms
step:2267/2285 train_time:1646338ms step_avg:726.22ms
step:2268/2285 train_time:1647070ms step_avg:726.22ms
step:2269/2285 train_time:1647803ms step_avg:726.22ms
step:2270/2285 train_time:1648540ms step_avg:726.23ms
step:2271/2285 train_time:1649267ms step_avg:726.23ms
step:2272/2285 train_time:1650003ms step_avg:726.23ms
step:2273/2285 train_time:1650734ms step_avg:726.24ms
step:2274/2285 train_time:1651470ms step_avg:726.24ms
step:2275/2285 train_time:1652203ms step_avg:726.24ms
step:2276/2285 train_time:1652939ms step_avg:726.25ms
step:2277/2285 train_time:1653670ms step_avg:726.25ms
step:2278/2285 train_time:1654408ms step_avg:726.25ms
step:2279/2285 train_time:1655135ms step_avg:726.25ms
step:2280/2285 train_time:1655869ms step_avg:726.26ms
step:2281/2285 train_time:1656604ms step_avg:726.26ms
step:2282/2285 train_time:1657340ms step_avg:726.27ms
step:2283/2285 train_time:1658073ms step_avg:726.27ms
step:2284/2285 train_time:1658809ms step_avg:726.27ms
step:2285/2285 train_time:1659539ms step_avg:726.28ms
step:2285/2285 val_loss:3.4105 train_time:1659637ms step_avg:726.32ms
peak memory allocated: 30846 MiB reserved: 51940 MiB
