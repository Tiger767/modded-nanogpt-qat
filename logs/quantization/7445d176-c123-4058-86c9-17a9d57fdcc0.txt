import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# Travis' Quantization Code

# Weights
# 2 bit
ENABLE_MLP_QUANTIZATION = True
ENABLE_ATTN_QUANTIZATION = True
ENABLE_ATTN_PROJ_QUANTIZATION = True

# Activations
# MLP
# 8 bit
ENABLE_MLP_ACT_QUANTIZATION = True
ENABLE_MLP_PRE_ACT_QUANTIZATION = True
# 1/1.58 bit instead of 8 bit
ENABLE_EXTREME_MLP_ACT_QUANTIZATION = True
ENABLE_EXTREME_PRE_MLP_ACT_QUANTIZATION = True
# ATTN
ENABLE_ATTN_ACT_QUANTIZATION = True
ENABLE_ATTN_PRE_ACT_QUANTIZATION = True
# 1/1.58 bit instead of 8 bit
ENABLE_EXTREME_ATTN_ACT_QUANTIZATION = True
ENABLE_EXTREME_PRE_ATTN_ACT_QUANTIZATION = True


class round_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input):
        ctx.input = input
        return torch.round(input)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()
        return grad_input


class noisy_round_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input):
        ctx.input = input
        return torch.round(input + torch.rand_like(input) - .5)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()
        return grad_input


class round_grad_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input):
        ctx.input = input
        return torch.round(input)

    @staticmethod
    def backward(ctx, grad_output):
        l = 1
        k = 5
        #x = l * (1 + torch.sign(grad_output - l) * torch.abs(grad_output - l)**(1/k))
        x = ctx.input
        x = x - torch.round(x - l / 2)
        grad = 1 / k * torch.abs(x - l / 2)**(1/k - 1)
        grad = grad.clamp(min=-3, max=3)
        return grad * grad_output


class clamp_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input, min_val=-1.0, max_val=1.0):
        ctx.input = input
        return torch.clamp(input, min_val, max_val)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()
        return grad_input, None, None


def ternary_quantize_mean_special(
    x: torch.Tensor,
):
    min_max_value = x.abs().mean(dim=-1, keepdim=True).mean(dim=-2, keepdim=True).clamp(min=1e-5)
    quantized = clamp_pt.apply(round_grad_pt.apply(x / min_max_value))
    return min_max_value * quantized


def ternary_quantize(
    x: torch.Tensor,
):
    min_max_value = x.abs().mean().clamp(min=1e-5)
    quantized = clamp_pt.apply(round_grad_pt.apply(x / min_max_value))
    return min_max_value * quantized


def quantized_sigmoid(x):
    x = torch.nn.functional.sigmoid(x)
    return round_grad_pt.apply(x)


def quantized_tanh(x):
    x = torch.nn.functional.tanh(x)
    return round_grad_pt.apply(x)


def quantized_relu2(
    x: torch.Tensor,
    num_bits=8,
    eps=1e-5,
):
    # 1. Apply the ReLU-squared activation.
    # The result is guaranteed to be non-negative (>= 0).
    x = torch.nn.functional.relu(x)
    x = x.square()

    # 2. Find the maximum value along the inner dimension (last dim).
    # We clamp at eps to prevent division by zero if a row is all zeros.
    xmax = x.max(-1, keepdim=True).values.clamp(min=eps)

    # 3. Determine the number of quantization steps.
    # As requested: "num steps should be 2**num_bits - 1"
    num_steps = (2**num_bits - 1)

    # 4. Quantize the values.
    # The range is [0, xmax].
    # We scale x to [0, num_steps], round it, then scale it back.
    
    # Scale to [0, num_steps]
    scaled_x = x / xmax * num_steps
    
    # Round to the nearest integer step (using your custom grad function)
    rounded_x = round_grad_pt.apply(scaled_x)
    
    # Scale back to the original magnitude range [0, xmax]
    quantized_x = rounded_x / num_steps * xmax

    return quantized_x


def quantized_linear(
    x: torch.Tensor,
    num_bits=8,
    eps=1e-5,
):
    """
    Applies symmetric linear quantization to the input tensor x.
    Quantizes positive and negative values separately based on the
    per-channel (last dim) min/max.
    """
    
    # Determine the number of quantization steps for each side (pos/neg)
    # (2**num_bits - 1) total steps. We split them around zero.
    num_steps = (2**num_bits - 1) // 2

    # Create a mask for positive (>= 0) values
    pos = x >= 0

    # Find the per-channel min/max values
    # Clamp at eps to prevent division by zero
    xnmin = x.min(-1, keepdim=True).values.clamp(max=-eps)
    xpmax = x.max(-1, keepdim=True).values.clamp(min=eps)

    # Quantize the positive part:
    # Scale to [0, num_steps], round, and scale back to [0, xpmax]
    xp = round_grad_pt.apply(x / xpmax * num_steps) / num_steps * xpmax

    # Quantize the negative part:
    # Scale to [0, num_steps] (since x/xnmin is positive), round, 
    # and scale back to [xnmin, 0]
    xn = round_grad_pt.apply(x / xnmin * num_steps) / num_steps * xnmin

    # Combine the quantized positive and negative parts
    x = xp * pos + xn * (~pos)

    return x


class Quantize(nn.Module):
    def __init__(
        self,
        quantize_func,
        out_scalars=False,
        shape=None,
        device=None,
        dtype=None,
        *args,
        **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)
        self.quantize_func = quantize_func

        self._post_quantized_mode = False
        self._max_value = 1.0 #nn.Parameter(torch.tensor(1.0))

        self.out_scalars = None
        if out_scalars:
            self.out_scalars = nn.Parameter(
                torch.sign(
                    torch.randn((shape[0], 1, *shape[2:]), device=device, dtype=dtype)
                )
            )

    def forward(self, x):
        if self.quantize_func is None:
            return x
        if self._post_quantized_mode:
            return x * self._max_value
        if self.out_scalars is not None:
            return self.out_scalars * self.quantize_func(x)
        return self.quantize_func(x)

    def quantize(self, x, round=False, to_scalar=True, dtype=None):
        self._post_quantized_mode = True
        if self.quantize_func is None:
            return

        qx = self.quantize_func(x.data)
        self._max_value.data = torch.max(torch.abs(qx))
        qx = qx / self._max_value.data
        qx = qx.round() if round else qx
        qx = qx.to(dtype) if dtype is not None else qx
        if to_scalar:
            nu = torch.unique(qx.flatten())
            if len(nu) == 1:
                qx = nu[:1]
        x.data = qx


class QuantizedLinear(nn.Linear):
    def __init__(
        self, *args, weight_quantize_func=None, bias_quantize_func=None, **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)
        self.quantize_weight = Quantize(weight_quantize_func)
        self.quantize_bias = Quantize(bias_quantize_func)

    def forward(self, input):
        qw = self.quantize_weight(self.weight)
        qb = self.quantize_bias(self.bias)
        return F.linear(input, qw, qb)

    def quantize(self, dtype=None):
        self.quantize_weight.quantize(self.weight, dtype=dtype)
        self.quantize_bias.quantize(self.bias, dtype=dtype)


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # quantization
        self.quantize_qkvo_w = Quantize(ternary_quantize_mean_special) if ENABLE_ATTN_QUANTIZATION else lambda x: x
        self.quantize_qkvo_w2 = Quantize(ternary_quantize) if ENABLE_ATTN_PROJ_QUANTIZATION else lambda x: x
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        if ENABLE_ATTN_PRE_ACT_QUANTIZATION:
            if ENABLE_EXTREME_PRE_ATTN_ACT_QUANTIZATION:
                x = quantized_tanh(x)
            else:
                # TODO: use linear not relu2?
                x = quantized_linear(x, num_bits=8)

        q, k, v = F.linear(x, self.quantize_qkvo_w(self.qkvo_w.view(4, self.hdim, self.dim)[:3]).flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        if ENABLE_ATTN_ACT_QUANTIZATION:
            if ENABLE_EXTREME_ATTN_ACT_QUANTIZATION:
                q = quantized_tanh(q)
                k = quantized_tanh(k)
                #v = quantized_tanh(v, num_bits=8)  # can skip
            else:
                # TODO: use linear not relu2?
                q = quantized_linear(q, num_bits=8)
                k = quantized_linear(k, num_bits=8)
                #v = quantized_linear(v, num_bits=8)  # can skip

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.quantize_qkvo_w2(self.qkvo_w.view(4, self.hdim, self.dim)[3]).type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # quantization
        self.quantize_c_fc = Quantize(ternary_quantize) if ENABLE_MLP_QUANTIZATION else lambda x: x
        self.quantize_c_proj = Quantize(ternary_quantize) if ENABLE_MLP_QUANTIZATION else lambda x: x
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        if ENABLE_MLP_PRE_ACT_QUANTIZATION:
            if ENABLE_EXTREME_PRE_MLP_ACT_QUANTIZATION:
                x = quantized_tanh(x)
            else:
                x = quantized_linear(x, num_bits=8)
        x = F.linear(x, self.quantize_c_fc(self.c_fc).T.type_as(x))
        if ENABLE_MLP_ACT_QUANTIZATION:
            if ENABLE_EXTREME_MLP_ACT_QUANTIZATION:
                x = quantized_tanh(x) #quantized_sigmoid(x)
            else:
                x = quantized_relu2(x, num_bits=8)
        else:
            x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.quantize_c_fc(self.c_proj).type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 100 # 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = True
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.1+cu128 compiled for CUDA 12.8
Running Triton version 3.5.1
Thu Nov 13 13:11:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1183MiB /  81559MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            5553      C   ...nogpt-lowbit/venv/bin/python3       1174MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2285 train_time:540ms step_avg:540.12ms
step:2/2285 train_time:982ms step_avg:491.07ms
step:3/2285 train_time:1481ms step_avg:493.66ms
step:4/2285 train_time:1990ms step_avg:497.60ms
step:5/2285 train_time:2503ms step_avg:500.52ms
step:6/2285 train_time:3019ms step_avg:503.12ms
step:7/2285 train_time:3534ms step_avg:504.88ms
step:8/2285 train_time:4054ms step_avg:506.71ms
step:9/2285 train_time:4570ms step_avg:507.75ms
step:10/2285 train_time:5092ms step_avg:509.15ms
step:11/2285 train_time:5609ms step_avg:509.92ms
step:12/2285 train_time:6131ms step_avg:510.91ms
step:13/2285 train_time:6648ms step_avg:511.40ms
step:14/2285 train_time:7170ms step_avg:512.14ms
step:15/2285 train_time:7686ms step_avg:512.40ms
step:16/2285 train_time:8209ms step_avg:513.04ms
step:17/2285 train_time:8724ms step_avg:513.17ms
step:18/2285 train_time:9248ms step_avg:513.77ms
step:19/2285 train_time:9765ms step_avg:513.94ms
step:20/2285 train_time:10285ms step_avg:514.25ms
step:21/2285 train_time:10801ms step_avg:514.35ms
step:22/2285 train_time:11324ms step_avg:514.74ms
step:23/2285 train_time:11844ms step_avg:514.94ms
step:24/2285 train_time:12364ms step_avg:515.16ms
step:25/2285 train_time:12883ms step_avg:515.33ms
step:26/2285 train_time:13405ms step_avg:515.56ms
step:27/2285 train_time:13921ms step_avg:515.61ms
step:28/2285 train_time:14446ms step_avg:515.92ms
step:29/2285 train_time:14962ms step_avg:515.93ms
step:30/2285 train_time:15487ms step_avg:516.25ms
step:31/2285 train_time:16006ms step_avg:516.32ms
step:32/2285 train_time:16530ms step_avg:516.57ms
step:33/2285 train_time:17047ms step_avg:516.57ms
step:34/2285 train_time:17567ms step_avg:516.69ms
step:35/2285 train_time:18083ms step_avg:516.66ms
step:36/2285 train_time:18605ms step_avg:516.81ms
step:37/2285 train_time:19121ms step_avg:516.79ms
step:38/2285 train_time:19643ms step_avg:516.91ms
step:39/2285 train_time:20161ms step_avg:516.96ms
step:40/2285 train_time:20687ms step_avg:517.17ms
step:41/2285 train_time:21204ms step_avg:517.16ms
step:42/2285 train_time:21728ms step_avg:517.34ms
step:43/2285 train_time:22247ms step_avg:517.37ms
step:44/2285 train_time:22769ms step_avg:517.48ms
step:45/2285 train_time:23288ms step_avg:517.50ms
step:46/2285 train_time:23808ms step_avg:517.57ms
step:47/2285 train_time:24325ms step_avg:517.56ms
step:48/2285 train_time:24847ms step_avg:517.65ms
step:49/2285 train_time:25368ms step_avg:517.71ms
step:50/2285 train_time:25892ms step_avg:517.83ms
step:51/2285 train_time:26410ms step_avg:517.83ms
step:52/2285 train_time:26934ms step_avg:517.96ms
step:53/2285 train_time:27451ms step_avg:517.94ms
step:54/2285 train_time:27977ms step_avg:518.10ms
step:55/2285 train_time:28498ms step_avg:518.14ms
step:56/2285 train_time:29022ms step_avg:518.25ms
step:57/2285 train_time:29542ms step_avg:518.28ms
step:58/2285 train_time:30067ms step_avg:518.40ms
step:59/2285 train_time:30584ms step_avg:518.37ms
step:60/2285 train_time:31109ms step_avg:518.48ms
step:61/2285 train_time:31627ms step_avg:518.48ms
step:62/2285 train_time:32155ms step_avg:518.63ms
step:63/2285 train_time:32676ms step_avg:518.67ms
step:64/2285 train_time:33202ms step_avg:518.79ms
step:65/2285 train_time:33720ms step_avg:518.77ms
step:66/2285 train_time:34247ms step_avg:518.90ms
step:67/2285 train_time:34766ms step_avg:518.89ms
step:68/2285 train_time:35290ms step_avg:518.98ms
step:69/2285 train_time:35806ms step_avg:518.93ms
step:70/2285 train_time:36332ms step_avg:519.03ms
step:71/2285 train_time:36848ms step_avg:518.99ms
step:72/2285 train_time:37370ms step_avg:519.03ms
step:73/2285 train_time:37890ms step_avg:519.04ms
step:74/2285 train_time:38412ms step_avg:519.08ms
step:75/2285 train_time:38930ms step_avg:519.07ms
step:76/2285 train_time:39455ms step_avg:519.14ms
step:77/2285 train_time:39973ms step_avg:519.13ms
step:78/2285 train_time:40498ms step_avg:519.20ms
step:79/2285 train_time:41019ms step_avg:519.22ms
step:80/2285 train_time:41547ms step_avg:519.33ms
step:81/2285 train_time:42064ms step_avg:519.31ms
step:82/2285 train_time:42592ms step_avg:519.42ms
step:83/2285 train_time:43109ms step_avg:519.39ms
step:84/2285 train_time:43637ms step_avg:519.49ms
step:85/2285 train_time:44158ms step_avg:519.51ms
step:86/2285 train_time:44687ms step_avg:519.61ms
step:87/2285 train_time:45206ms step_avg:519.61ms
step:88/2285 train_time:45730ms step_avg:519.66ms
step:89/2285 train_time:46251ms step_avg:519.67ms
step:90/2285 train_time:46774ms step_avg:519.71ms
step:91/2285 train_time:47294ms step_avg:519.71ms
step:92/2285 train_time:47819ms step_avg:519.78ms
step:93/2285 train_time:48339ms step_avg:519.77ms
step:94/2285 train_time:48862ms step_avg:519.81ms
step:95/2285 train_time:49382ms step_avg:519.81ms
step:96/2285 train_time:49908ms step_avg:519.87ms
step:97/2285 train_time:50427ms step_avg:519.87ms
step:98/2285 train_time:50954ms step_avg:519.94ms
step:99/2285 train_time:51473ms step_avg:519.93ms
step:100/2285 train_time:52001ms step_avg:520.01ms
step:100/2285 val_loss:5.2726 train_time:52065ms step_avg:520.65ms
step:101/2285 train_time:52530ms step_avg:520.10ms
step:102/2285 train_time:53053ms step_avg:520.13ms
step:103/2285 train_time:53573ms step_avg:520.13ms
step:104/2285 train_time:54098ms step_avg:520.17ms
step:105/2285 train_time:54621ms step_avg:520.20ms
step:106/2285 train_time:55144ms step_avg:520.23ms
step:107/2285 train_time:55667ms step_avg:520.25ms
step:108/2285 train_time:56191ms step_avg:520.29ms
step:109/2285 train_time:56712ms step_avg:520.30ms
step:110/2285 train_time:57241ms step_avg:520.37ms
step:111/2285 train_time:57761ms step_avg:520.37ms
step:112/2285 train_time:58287ms step_avg:520.42ms
step:113/2285 train_time:58808ms step_avg:520.42ms
step:114/2285 train_time:59335ms step_avg:520.48ms
step:115/2285 train_time:59854ms step_avg:520.47ms
step:116/2285 train_time:60385ms step_avg:520.56ms
step:117/2285 train_time:60906ms step_avg:520.56ms
step:118/2285 train_time:61433ms step_avg:520.61ms
step:119/2285 train_time:61952ms step_avg:520.61ms
step:120/2285 train_time:62481ms step_avg:520.68ms
step:121/2285 train_time:62999ms step_avg:520.65ms
step:122/2285 train_time:63527ms step_avg:520.71ms
step:123/2285 train_time:64047ms step_avg:520.71ms
step:124/2285 train_time:64574ms step_avg:520.75ms
step:125/2285 train_time:65092ms step_avg:520.74ms
step:126/2285 train_time:65619ms step_avg:520.79ms
step:127/2285 train_time:66140ms step_avg:520.79ms
step:128/2285 train_time:66667ms step_avg:520.84ms
step:129/2285 train_time:67186ms step_avg:520.83ms
step:130/2285 train_time:67711ms step_avg:520.85ms
step:131/2285 train_time:68229ms step_avg:520.83ms
step:132/2285 train_time:68755ms step_avg:520.87ms
step:133/2285 train_time:69277ms step_avg:520.88ms
step:134/2285 train_time:69805ms step_avg:520.93ms
step:135/2285 train_time:70325ms step_avg:520.93ms
step:136/2285 train_time:70851ms step_avg:520.97ms
step:137/2285 train_time:71369ms step_avg:520.94ms
step:138/2285 train_time:71893ms step_avg:520.96ms
step:139/2285 train_time:72414ms step_avg:520.97ms
step:140/2285 train_time:72942ms step_avg:521.01ms
step:141/2285 train_time:73460ms step_avg:521.00ms
step:142/2285 train_time:73984ms step_avg:521.01ms
step:143/2285 train_time:74505ms step_avg:521.02ms
step:144/2285 train_time:75030ms step_avg:521.04ms
step:145/2285 train_time:75549ms step_avg:521.03ms
step:146/2285 train_time:76076ms step_avg:521.07ms
step:147/2285 train_time:76597ms step_avg:521.07ms
step:148/2285 train_time:77122ms step_avg:521.09ms
step:149/2285 train_time:77640ms step_avg:521.07ms
step:150/2285 train_time:78165ms step_avg:521.10ms
step:151/2285 train_time:78686ms step_avg:521.10ms
step:152/2285 train_time:79209ms step_avg:521.11ms
step:153/2285 train_time:79727ms step_avg:521.09ms
step:154/2285 train_time:80252ms step_avg:521.11ms
step:155/2285 train_time:80770ms step_avg:521.10ms
step:156/2285 train_time:81293ms step_avg:521.11ms
step:157/2285 train_time:81812ms step_avg:521.10ms
step:158/2285 train_time:82336ms step_avg:521.11ms
step:159/2285 train_time:82857ms step_avg:521.11ms
step:160/2285 train_time:83383ms step_avg:521.14ms
step:161/2285 train_time:83904ms step_avg:521.14ms
step:162/2285 train_time:84426ms step_avg:521.15ms
step:163/2285 train_time:84945ms step_avg:521.14ms
step:164/2285 train_time:85470ms step_avg:521.16ms
step:165/2285 train_time:85988ms step_avg:521.14ms
step:166/2285 train_time:86515ms step_avg:521.17ms
step:167/2285 train_time:87037ms step_avg:521.18ms
step:168/2285 train_time:87560ms step_avg:521.19ms
step:169/2285 train_time:88084ms step_avg:521.21ms
step:170/2285 train_time:88608ms step_avg:521.23ms
step:171/2285 train_time:89129ms step_avg:521.22ms
step:172/2285 train_time:89654ms step_avg:521.24ms
step:173/2285 train_time:90173ms step_avg:521.23ms
step:174/2285 train_time:90702ms step_avg:521.27ms
step:175/2285 train_time:91221ms step_avg:521.27ms
step:176/2285 train_time:91744ms step_avg:521.27ms
step:177/2285 train_time:92262ms step_avg:521.25ms
step:178/2285 train_time:92792ms step_avg:521.31ms
step:179/2285 train_time:93312ms step_avg:521.30ms
step:180/2285 train_time:93837ms step_avg:521.32ms
step:181/2285 train_time:94359ms step_avg:521.32ms
step:182/2285 train_time:94885ms step_avg:521.34ms
step:183/2285 train_time:95408ms step_avg:521.35ms
step:184/2285 train_time:95936ms step_avg:521.39ms
step:185/2285 train_time:96456ms step_avg:521.38ms
step:186/2285 train_time:96981ms step_avg:521.40ms
step:187/2285 train_time:97503ms step_avg:521.41ms
step:188/2285 train_time:98027ms step_avg:521.42ms
step:189/2285 train_time:98547ms step_avg:521.41ms
step:190/2285 train_time:99072ms step_avg:521.43ms
step:191/2285 train_time:99593ms step_avg:521.43ms
step:192/2285 train_time:100121ms step_avg:521.46ms
step:193/2285 train_time:100644ms step_avg:521.47ms
step:194/2285 train_time:101172ms step_avg:521.50ms
step:195/2285 train_time:101692ms step_avg:521.50ms
step:196/2285 train_time:102218ms step_avg:521.52ms
step:197/2285 train_time:102739ms step_avg:521.52ms
step:198/2285 train_time:103271ms step_avg:521.57ms
step:199/2285 train_time:103791ms step_avg:521.56ms
step:200/2285 train_time:104322ms step_avg:521.61ms
step:200/2285 val_loss:4.9157 train_time:104386ms step_avg:521.93ms
step:201/2285 train_time:104844ms step_avg:521.61ms
step:202/2285 train_time:105371ms step_avg:521.64ms
step:203/2285 train_time:105889ms step_avg:521.62ms
step:204/2285 train_time:106414ms step_avg:521.64ms
step:205/2285 train_time:106934ms step_avg:521.63ms
step:206/2285 train_time:107457ms step_avg:521.64ms
step:207/2285 train_time:107976ms step_avg:521.62ms
step:208/2285 train_time:108503ms step_avg:521.65ms
step:209/2285 train_time:109022ms step_avg:521.63ms
step:210/2285 train_time:109544ms step_avg:521.64ms
step:211/2285 train_time:110061ms step_avg:521.62ms
step:212/2285 train_time:110583ms step_avg:521.62ms
step:213/2285 train_time:111102ms step_avg:521.61ms
step:214/2285 train_time:111625ms step_avg:521.61ms
step:215/2285 train_time:112145ms step_avg:521.60ms
step:216/2285 train_time:112670ms step_avg:521.62ms
step:217/2285 train_time:113189ms step_avg:521.61ms
step:218/2285 train_time:113716ms step_avg:521.63ms
step:219/2285 train_time:114235ms step_avg:521.62ms
step:220/2285 train_time:114761ms step_avg:521.64ms
step:221/2285 train_time:115282ms step_avg:521.64ms
step:222/2285 train_time:115810ms step_avg:521.67ms
step:223/2285 train_time:116328ms step_avg:521.65ms
step:224/2285 train_time:116855ms step_avg:521.67ms
step:225/2285 train_time:117376ms step_avg:521.67ms
step:226/2285 train_time:117897ms step_avg:521.67ms
step:227/2285 train_time:118420ms step_avg:521.67ms
step:228/2285 train_time:118945ms step_avg:521.69ms
step:229/2285 train_time:119466ms step_avg:521.69ms
step:230/2285 train_time:119988ms step_avg:521.69ms
step:231/2285 train_time:120509ms step_avg:521.68ms
step:232/2285 train_time:121034ms step_avg:521.70ms
step:233/2285 train_time:121553ms step_avg:521.69ms
step:234/2285 train_time:122074ms step_avg:521.69ms
step:235/2285 train_time:122596ms step_avg:521.69ms
step:236/2285 train_time:123122ms step_avg:521.70ms
step:237/2285 train_time:123640ms step_avg:521.69ms
step:238/2285 train_time:124166ms step_avg:521.71ms
step:239/2285 train_time:124687ms step_avg:521.70ms
step:240/2285 train_time:125212ms step_avg:521.72ms
step:241/2285 train_time:125732ms step_avg:521.71ms
step:242/2285 train_time:126257ms step_avg:521.72ms
step:243/2285 train_time:126779ms step_avg:521.72ms
step:244/2285 train_time:127303ms step_avg:521.73ms
step:245/2285 train_time:127827ms step_avg:521.74ms
step:246/2285 train_time:128350ms step_avg:521.75ms
step:247/2285 train_time:128872ms step_avg:521.75ms
step:248/2285 train_time:129392ms step_avg:521.74ms
step:249/2285 train_time:129914ms step_avg:521.74ms
step:250/2285 train_time:130434ms step_avg:521.74ms
step:251/2285 train_time:130957ms step_avg:521.74ms
step:252/2285 train_time:131479ms step_avg:521.74ms
step:253/2285 train_time:132002ms step_avg:521.75ms
step:254/2285 train_time:132528ms step_avg:521.76ms
step:255/2285 train_time:133050ms step_avg:521.76ms
step:256/2285 train_time:133576ms step_avg:521.78ms
step:257/2285 train_time:134099ms step_avg:521.78ms
step:258/2285 train_time:134626ms step_avg:521.81ms
step:259/2285 train_time:135147ms step_avg:521.80ms
step:260/2285 train_time:135671ms step_avg:521.81ms
step:261/2285 train_time:136189ms step_avg:521.80ms
step:262/2285 train_time:136715ms step_avg:521.81ms
step:263/2285 train_time:137236ms step_avg:521.81ms
step:264/2285 train_time:137761ms step_avg:521.82ms
step:265/2285 train_time:138282ms step_avg:521.82ms
step:266/2285 train_time:138809ms step_avg:521.84ms
step:267/2285 train_time:139330ms step_avg:521.83ms
step:268/2285 train_time:139855ms step_avg:521.85ms
step:269/2285 train_time:140373ms step_avg:521.83ms
step:270/2285 train_time:140898ms step_avg:521.85ms
step:271/2285 train_time:141417ms step_avg:521.83ms
step:272/2285 train_time:141946ms step_avg:521.86ms
step:273/2285 train_time:142466ms step_avg:521.85ms
step:274/2285 train_time:142989ms step_avg:521.86ms
step:275/2285 train_time:143507ms step_avg:521.84ms
step:276/2285 train_time:144031ms step_avg:521.85ms
step:277/2285 train_time:144550ms step_avg:521.84ms
step:278/2285 train_time:145075ms step_avg:521.85ms
step:279/2285 train_time:145598ms step_avg:521.86ms
step:280/2285 train_time:146126ms step_avg:521.88ms
step:281/2285 train_time:146646ms step_avg:521.87ms
step:282/2285 train_time:147174ms step_avg:521.89ms
step:283/2285 train_time:147695ms step_avg:521.89ms
step:284/2285 train_time:148220ms step_avg:521.90ms
step:285/2285 train_time:148744ms step_avg:521.91ms
step:286/2285 train_time:149272ms step_avg:521.93ms
step:287/2285 train_time:149792ms step_avg:521.92ms
step:288/2285 train_time:150315ms step_avg:521.93ms
step:289/2285 train_time:150836ms step_avg:521.92ms
step:290/2285 train_time:151362ms step_avg:521.94ms
step:291/2285 train_time:151883ms step_avg:521.94ms
step:292/2285 train_time:152408ms step_avg:521.95ms
step:293/2285 train_time:152930ms step_avg:521.95ms
step:294/2285 train_time:153454ms step_avg:521.95ms
step:295/2285 train_time:153974ms step_avg:521.95ms
step:296/2285 train_time:154499ms step_avg:521.96ms
step:297/2285 train_time:155021ms step_avg:521.96ms
step:298/2285 train_time:155543ms step_avg:521.95ms
step:299/2285 train_time:156068ms step_avg:521.97ms
step:300/2285 train_time:156588ms step_avg:521.96ms
step:300/2285 val_loss:4.7401 train_time:156653ms step_avg:522.18ms
step:301/2285 train_time:157114ms step_avg:521.97ms
step:302/2285 train_time:157641ms step_avg:521.99ms
step:303/2285 train_time:158163ms step_avg:521.99ms
step:304/2285 train_time:158688ms step_avg:522.00ms
step:305/2285 train_time:159211ms step_avg:522.00ms
step:306/2285 train_time:159739ms step_avg:522.02ms
step:307/2285 train_time:160262ms step_avg:522.03ms
step:308/2285 train_time:160788ms step_avg:522.04ms
step:309/2285 train_time:161311ms step_avg:522.04ms
step:310/2285 train_time:161839ms step_avg:522.06ms
step:311/2285 train_time:162359ms step_avg:522.05ms
step:312/2285 train_time:162886ms step_avg:522.07ms
step:313/2285 train_time:163410ms step_avg:522.08ms
step:314/2285 train_time:163938ms step_avg:522.09ms
step:315/2285 train_time:164460ms step_avg:522.09ms
step:316/2285 train_time:164986ms step_avg:522.11ms
step:317/2285 train_time:165510ms step_avg:522.11ms
step:318/2285 train_time:166040ms step_avg:522.14ms
step:319/2285 train_time:166563ms step_avg:522.14ms
step:320/2285 train_time:167090ms step_avg:522.16ms
step:321/2285 train_time:167610ms step_avg:522.15ms
step:322/2285 train_time:168141ms step_avg:522.18ms
step:323/2285 train_time:168663ms step_avg:522.18ms
step:324/2285 train_time:169190ms step_avg:522.19ms
step:325/2285 train_time:169713ms step_avg:522.19ms
step:326/2285 train_time:170241ms step_avg:522.21ms
step:327/2285 train_time:170761ms step_avg:522.21ms
step:328/2285 train_time:171290ms step_avg:522.23ms
step:329/2285 train_time:171812ms step_avg:522.22ms
step:330/2285 train_time:172341ms step_avg:522.25ms
step:331/2285 train_time:172862ms step_avg:522.24ms
step:332/2285 train_time:173394ms step_avg:522.27ms
step:333/2285 train_time:173917ms step_avg:522.27ms
step:334/2285 train_time:174448ms step_avg:522.30ms
step:335/2285 train_time:174971ms step_avg:522.30ms
step:336/2285 train_time:175498ms step_avg:522.31ms
step:337/2285 train_time:176019ms step_avg:522.31ms
step:338/2285 train_time:176546ms step_avg:522.33ms
step:339/2285 train_time:177069ms step_avg:522.33ms
step:340/2285 train_time:177601ms step_avg:522.36ms
step:341/2285 train_time:178122ms step_avg:522.35ms
step:342/2285 train_time:178652ms step_avg:522.37ms
step:343/2285 train_time:179173ms step_avg:522.37ms
step:344/2285 train_time:179702ms step_avg:522.39ms
step:345/2285 train_time:180223ms step_avg:522.39ms
step:346/2285 train_time:180751ms step_avg:522.40ms
step:347/2285 train_time:181276ms step_avg:522.41ms
step:348/2285 train_time:181803ms step_avg:522.42ms
step:349/2285 train_time:182326ms step_avg:522.42ms
step:350/2285 train_time:182855ms step_avg:522.44ms
step:351/2285 train_time:183376ms step_avg:522.44ms
step:352/2285 train_time:183901ms step_avg:522.45ms
step:353/2285 train_time:184425ms step_avg:522.45ms
step:354/2285 train_time:184952ms step_avg:522.46ms
step:355/2285 train_time:185474ms step_avg:522.46ms
step:356/2285 train_time:186000ms step_avg:522.47ms
step:357/2285 train_time:186522ms step_avg:522.47ms
step:358/2285 train_time:187049ms step_avg:522.48ms
step:359/2285 train_time:187572ms step_avg:522.48ms
step:360/2285 train_time:188104ms step_avg:522.51ms
step:361/2285 train_time:188625ms step_avg:522.51ms
step:362/2285 train_time:189152ms step_avg:522.52ms
step:363/2285 train_time:189673ms step_avg:522.52ms
step:364/2285 train_time:190201ms step_avg:522.53ms
step:365/2285 train_time:190721ms step_avg:522.52ms
step:366/2285 train_time:191247ms step_avg:522.53ms
step:367/2285 train_time:191769ms step_avg:522.53ms
step:368/2285 train_time:192299ms step_avg:522.55ms
step:369/2285 train_time:192824ms step_avg:522.56ms
step:370/2285 train_time:193354ms step_avg:522.58ms
step:371/2285 train_time:193878ms step_avg:522.58ms
step:372/2285 train_time:194403ms step_avg:522.59ms
step:373/2285 train_time:194923ms step_avg:522.58ms
step:374/2285 train_time:195452ms step_avg:522.60ms
step:375/2285 train_time:195973ms step_avg:522.59ms
step:376/2285 train_time:196498ms step_avg:522.60ms
step:377/2285 train_time:197019ms step_avg:522.60ms
step:378/2285 train_time:197546ms step_avg:522.61ms
step:379/2285 train_time:198068ms step_avg:522.61ms
step:380/2285 train_time:198597ms step_avg:522.62ms
step:381/2285 train_time:199120ms step_avg:522.62ms
step:382/2285 train_time:199648ms step_avg:522.64ms
step:383/2285 train_time:200170ms step_avg:522.64ms
step:384/2285 train_time:200696ms step_avg:522.65ms
step:385/2285 train_time:201219ms step_avg:522.65ms
step:386/2285 train_time:201746ms step_avg:522.66ms
step:387/2285 train_time:202267ms step_avg:522.65ms
step:388/2285 train_time:202793ms step_avg:522.66ms
step:389/2285 train_time:203315ms step_avg:522.66ms
step:390/2285 train_time:203843ms step_avg:522.67ms
step:391/2285 train_time:204364ms step_avg:522.67ms
step:392/2285 train_time:204894ms step_avg:522.69ms
step:393/2285 train_time:205413ms step_avg:522.68ms
step:394/2285 train_time:205942ms step_avg:522.69ms
step:395/2285 train_time:206464ms step_avg:522.69ms
step:396/2285 train_time:206993ms step_avg:522.71ms
step:397/2285 train_time:207514ms step_avg:522.71ms
step:398/2285 train_time:208043ms step_avg:522.72ms
step:399/2285 train_time:208568ms step_avg:522.73ms
step:400/2285 train_time:209093ms step_avg:522.73ms
step:400/2285 val_loss:4.6023 train_time:209159ms step_avg:522.90ms
step:401/2285 train_time:209619ms step_avg:522.74ms
step:402/2285 train_time:210148ms step_avg:522.76ms
step:403/2285 train_time:210670ms step_avg:522.75ms
step:404/2285 train_time:211198ms step_avg:522.77ms
step:405/2285 train_time:211721ms step_avg:522.77ms
step:406/2285 train_time:212245ms step_avg:522.77ms
step:407/2285 train_time:212767ms step_avg:522.77ms
step:408/2285 train_time:213293ms step_avg:522.78ms
step:409/2285 train_time:213818ms step_avg:522.78ms
step:410/2285 train_time:214349ms step_avg:522.80ms
step:411/2285 train_time:214868ms step_avg:522.79ms
step:412/2285 train_time:215395ms step_avg:522.80ms
step:413/2285 train_time:215916ms step_avg:522.80ms
step:414/2285 train_time:216442ms step_avg:522.81ms
step:415/2285 train_time:216965ms step_avg:522.81ms
step:416/2285 train_time:217495ms step_avg:522.82ms
step:417/2285 train_time:218016ms step_avg:522.82ms
step:418/2285 train_time:218545ms step_avg:522.83ms
step:419/2285 train_time:219066ms step_avg:522.83ms
step:420/2285 train_time:219591ms step_avg:522.84ms
step:421/2285 train_time:220112ms step_avg:522.83ms
step:422/2285 train_time:220638ms step_avg:522.84ms
step:423/2285 train_time:221161ms step_avg:522.84ms
step:424/2285 train_time:221690ms step_avg:522.85ms
step:425/2285 train_time:222210ms step_avg:522.85ms
step:426/2285 train_time:222735ms step_avg:522.85ms
step:427/2285 train_time:223261ms step_avg:522.86ms
step:428/2285 train_time:223787ms step_avg:522.87ms
step:429/2285 train_time:224307ms step_avg:522.86ms
step:430/2285 train_time:224833ms step_avg:522.87ms
step:431/2285 train_time:225354ms step_avg:522.86ms
step:432/2285 train_time:225883ms step_avg:522.88ms
step:433/2285 train_time:226406ms step_avg:522.88ms
step:434/2285 train_time:226934ms step_avg:522.89ms
step:435/2285 train_time:227458ms step_avg:522.89ms
step:436/2285 train_time:227985ms step_avg:522.90ms
step:437/2285 train_time:228505ms step_avg:522.90ms
step:438/2285 train_time:229031ms step_avg:522.90ms
step:439/2285 train_time:229556ms step_avg:522.91ms
step:440/2285 train_time:230084ms step_avg:522.92ms
step:441/2285 train_time:230605ms step_avg:522.91ms
step:442/2285 train_time:231133ms step_avg:522.92ms
step:443/2285 train_time:231654ms step_avg:522.92ms
step:444/2285 train_time:232179ms step_avg:522.93ms
step:445/2285 train_time:232700ms step_avg:522.92ms
step:446/2285 train_time:233228ms step_avg:522.93ms
step:447/2285 train_time:233746ms step_avg:522.92ms
step:448/2285 train_time:234274ms step_avg:522.93ms
step:449/2285 train_time:234796ms step_avg:522.93ms
step:450/2285 train_time:235326ms step_avg:522.95ms
step:451/2285 train_time:235846ms step_avg:522.94ms
step:452/2285 train_time:236373ms step_avg:522.95ms
step:453/2285 train_time:236891ms step_avg:522.94ms
step:454/2285 train_time:237419ms step_avg:522.95ms
step:455/2285 train_time:237940ms step_avg:522.94ms
step:456/2285 train_time:238467ms step_avg:522.95ms
step:457/2285 train_time:238989ms step_avg:522.95ms
step:458/2285 train_time:239514ms step_avg:522.96ms
step:459/2285 train_time:240038ms step_avg:522.96ms
step:460/2285 train_time:240567ms step_avg:522.97ms
step:461/2285 train_time:241090ms step_avg:522.97ms
step:462/2285 train_time:241615ms step_avg:522.98ms
step:463/2285 train_time:242140ms step_avg:522.98ms
step:464/2285 train_time:242665ms step_avg:522.99ms
step:465/2285 train_time:243189ms step_avg:522.99ms
step:466/2285 train_time:243716ms step_avg:522.99ms
step:467/2285 train_time:244243ms step_avg:523.01ms
step:468/2285 train_time:244767ms step_avg:523.01ms
step:469/2285 train_time:245289ms step_avg:523.00ms
step:470/2285 train_time:245814ms step_avg:523.01ms
step:471/2285 train_time:246339ms step_avg:523.01ms
step:472/2285 train_time:246868ms step_avg:523.02ms
step:473/2285 train_time:247393ms step_avg:523.03ms
step:474/2285 train_time:247918ms step_avg:523.03ms
step:475/2285 train_time:248443ms step_avg:523.04ms
step:476/2285 train_time:248970ms step_avg:523.05ms
step:477/2285 train_time:249492ms step_avg:523.04ms
step:478/2285 train_time:250016ms step_avg:523.05ms
step:479/2285 train_time:250537ms step_avg:523.04ms
step:480/2285 train_time:251065ms step_avg:523.05ms
step:481/2285 train_time:251586ms step_avg:523.05ms
step:482/2285 train_time:252112ms step_avg:523.05ms
step:483/2285 train_time:252635ms step_avg:523.05ms
step:484/2285 train_time:253161ms step_avg:523.06ms
step:485/2285 train_time:253683ms step_avg:523.06ms
step:486/2285 train_time:254208ms step_avg:523.06ms
step:487/2285 train_time:254730ms step_avg:523.06ms
step:488/2285 train_time:255256ms step_avg:523.07ms
step:489/2285 train_time:255778ms step_avg:523.06ms
step:490/2285 train_time:256310ms step_avg:523.08ms
step:491/2285 train_time:256831ms step_avg:523.08ms
step:492/2285 train_time:257360ms step_avg:523.09ms
step:493/2285 train_time:257882ms step_avg:523.09ms
step:494/2285 train_time:258409ms step_avg:523.10ms
step:495/2285 train_time:258928ms step_avg:523.09ms
step:496/2285 train_time:259453ms step_avg:523.09ms
step:497/2285 train_time:259976ms step_avg:523.09ms
step:498/2285 train_time:260505ms step_avg:523.10ms
step:499/2285 train_time:261026ms step_avg:523.10ms
step:500/2285 train_time:261554ms step_avg:523.11ms
step:500/2285 val_loss:4.4887 train_time:261619ms step_avg:523.24ms
step:501/2285 train_time:262077ms step_avg:523.11ms
step:502/2285 train_time:262605ms step_avg:523.12ms
step:503/2285 train_time:263125ms step_avg:523.11ms
step:504/2285 train_time:263651ms step_avg:523.12ms
step:505/2285 train_time:264176ms step_avg:523.12ms
step:506/2285 train_time:264701ms step_avg:523.13ms
step:507/2285 train_time:265220ms step_avg:523.12ms
step:508/2285 train_time:265749ms step_avg:523.13ms
step:509/2285 train_time:266271ms step_avg:523.13ms
step:510/2285 train_time:266801ms step_avg:523.14ms
step:511/2285 train_time:267320ms step_avg:523.13ms
step:512/2285 train_time:267847ms step_avg:523.14ms
step:513/2285 train_time:268366ms step_avg:523.13ms
step:514/2285 train_time:268895ms step_avg:523.14ms
step:515/2285 train_time:269419ms step_avg:523.14ms
step:516/2285 train_time:269944ms step_avg:523.15ms
step:517/2285 train_time:270465ms step_avg:523.14ms
step:518/2285 train_time:270995ms step_avg:523.16ms
step:519/2285 train_time:271517ms step_avg:523.15ms
step:520/2285 train_time:272046ms step_avg:523.16ms
step:521/2285 train_time:272565ms step_avg:523.16ms
step:522/2285 train_time:273095ms step_avg:523.17ms
step:523/2285 train_time:273616ms step_avg:523.17ms
step:524/2285 train_time:274142ms step_avg:523.17ms
step:525/2285 train_time:274664ms step_avg:523.17ms
step:526/2285 train_time:275191ms step_avg:523.18ms
step:527/2285 train_time:275712ms step_avg:523.17ms
step:528/2285 train_time:276238ms step_avg:523.18ms
step:529/2285 train_time:276758ms step_avg:523.17ms
step:530/2285 train_time:277287ms step_avg:523.18ms
step:531/2285 train_time:277810ms step_avg:523.18ms
step:532/2285 train_time:278337ms step_avg:523.19ms
step:533/2285 train_time:278859ms step_avg:523.19ms
step:534/2285 train_time:279384ms step_avg:523.19ms
step:535/2285 train_time:279906ms step_avg:523.19ms
step:536/2285 train_time:280434ms step_avg:523.20ms
step:537/2285 train_time:280959ms step_avg:523.20ms
step:538/2285 train_time:281489ms step_avg:523.21ms
step:539/2285 train_time:282014ms step_avg:523.22ms
step:540/2285 train_time:282543ms step_avg:523.23ms
step:541/2285 train_time:283063ms step_avg:523.22ms
step:542/2285 train_time:283591ms step_avg:523.23ms
step:543/2285 train_time:284111ms step_avg:523.22ms
step:544/2285 train_time:284638ms step_avg:523.23ms
step:545/2285 train_time:285159ms step_avg:523.23ms
step:546/2285 train_time:285688ms step_avg:523.24ms
step:547/2285 train_time:286208ms step_avg:523.23ms
step:548/2285 train_time:286739ms step_avg:523.25ms
step:549/2285 train_time:287260ms step_avg:523.24ms
step:550/2285 train_time:287787ms step_avg:523.25ms
step:551/2285 train_time:288310ms step_avg:523.25ms
step:552/2285 train_time:288839ms step_avg:523.26ms
step:553/2285 train_time:289358ms step_avg:523.25ms
step:554/2285 train_time:289882ms step_avg:523.25ms
step:555/2285 train_time:290405ms step_avg:523.25ms
step:556/2285 train_time:290933ms step_avg:523.26ms
step:557/2285 train_time:291457ms step_avg:523.26ms
step:558/2285 train_time:291983ms step_avg:523.27ms
step:559/2285 train_time:292504ms step_avg:523.26ms
step:560/2285 train_time:293033ms step_avg:523.27ms
step:561/2285 train_time:293555ms step_avg:523.27ms
step:562/2285 train_time:294084ms step_avg:523.28ms
step:563/2285 train_time:294604ms step_avg:523.28ms
step:564/2285 train_time:295133ms step_avg:523.28ms
step:565/2285 train_time:295655ms step_avg:523.28ms
step:566/2285 train_time:296183ms step_avg:523.29ms
step:567/2285 train_time:296702ms step_avg:523.28ms
step:568/2285 train_time:297230ms step_avg:523.29ms
step:569/2285 train_time:297753ms step_avg:523.29ms
step:570/2285 train_time:298280ms step_avg:523.30ms
step:571/2285 train_time:298801ms step_avg:523.29ms
step:572/2285 train_time:299332ms step_avg:523.31ms
step:573/2285 train_time:299853ms step_avg:523.30ms
step:574/2285 train_time:300381ms step_avg:523.31ms
step:575/2285 train_time:300904ms step_avg:523.31ms
step:576/2285 train_time:301432ms step_avg:523.32ms
step:577/2285 train_time:301953ms step_avg:523.32ms
step:578/2285 train_time:302484ms step_avg:523.33ms
step:579/2285 train_time:303005ms step_avg:523.32ms
step:580/2285 train_time:303534ms step_avg:523.34ms
step:581/2285 train_time:304056ms step_avg:523.33ms
step:582/2285 train_time:304583ms step_avg:523.34ms
step:583/2285 train_time:305103ms step_avg:523.33ms
step:584/2285 train_time:305634ms step_avg:523.35ms
step:585/2285 train_time:306151ms step_avg:523.34ms
step:586/2285 train_time:306681ms step_avg:523.35ms
step:587/2285 train_time:307203ms step_avg:523.34ms
step:588/2285 train_time:307730ms step_avg:523.35ms
step:589/2285 train_time:308249ms step_avg:523.34ms
step:590/2285 train_time:308779ms step_avg:523.35ms
step:591/2285 train_time:309302ms step_avg:523.35ms
step:592/2285 train_time:309828ms step_avg:523.36ms
step:593/2285 train_time:310350ms step_avg:523.36ms
step:594/2285 train_time:310877ms step_avg:523.36ms
step:595/2285 train_time:311401ms step_avg:523.36ms
step:596/2285 train_time:311929ms step_avg:523.37ms
step:597/2285 train_time:312453ms step_avg:523.37ms
step:598/2285 train_time:312978ms step_avg:523.37ms
step:599/2285 train_time:313499ms step_avg:523.37ms
step:600/2285 train_time:314027ms step_avg:523.38ms
step:600/2285 val_loss:4.4109 train_time:314091ms step_avg:523.49ms
step:601/2285 train_time:314549ms step_avg:523.38ms
step:602/2285 train_time:315074ms step_avg:523.38ms
step:603/2285 train_time:315596ms step_avg:523.38ms
step:604/2285 train_time:316124ms step_avg:523.38ms
step:605/2285 train_time:316644ms step_avg:523.38ms
step:606/2285 train_time:317176ms step_avg:523.39ms
step:607/2285 train_time:317699ms step_avg:523.39ms
step:608/2285 train_time:318229ms step_avg:523.40ms
step:609/2285 train_time:318749ms step_avg:523.40ms
step:610/2285 train_time:319277ms step_avg:523.41ms
step:611/2285 train_time:319798ms step_avg:523.40ms
step:612/2285 train_time:320326ms step_avg:523.41ms
step:613/2285 train_time:320848ms step_avg:523.41ms
step:614/2285 train_time:321379ms step_avg:523.42ms
step:615/2285 train_time:321901ms step_avg:523.42ms
step:616/2285 train_time:322428ms step_avg:523.42ms
step:617/2285 train_time:322951ms step_avg:523.42ms
step:618/2285 train_time:323478ms step_avg:523.43ms
step:619/2285 train_time:324003ms step_avg:523.43ms
step:620/2285 train_time:324528ms step_avg:523.43ms
step:621/2285 train_time:325050ms step_avg:523.43ms
step:622/2285 train_time:325580ms step_avg:523.44ms
step:623/2285 train_time:326101ms step_avg:523.44ms
step:624/2285 train_time:326626ms step_avg:523.44ms
step:625/2285 train_time:327146ms step_avg:523.43ms
step:626/2285 train_time:327673ms step_avg:523.44ms
step:627/2285 train_time:328195ms step_avg:523.44ms
step:628/2285 train_time:328722ms step_avg:523.44ms
step:629/2285 train_time:329244ms step_avg:523.44ms
step:630/2285 train_time:329772ms step_avg:523.45ms
step:631/2285 train_time:330293ms step_avg:523.44ms
step:632/2285 train_time:330818ms step_avg:523.45ms
step:633/2285 train_time:331337ms step_avg:523.44ms
step:634/2285 train_time:331860ms step_avg:523.44ms
step:635/2285 train_time:332382ms step_avg:523.44ms
step:636/2285 train_time:332907ms step_avg:523.44ms
step:637/2285 train_time:333429ms step_avg:523.44ms
step:638/2285 train_time:333959ms step_avg:523.45ms
step:639/2285 train_time:334480ms step_avg:523.44ms
step:640/2285 train_time:335006ms step_avg:523.45ms
step:641/2285 train_time:335527ms step_avg:523.44ms
step:642/2285 train_time:336053ms step_avg:523.45ms
step:643/2285 train_time:336573ms step_avg:523.44ms
step:644/2285 train_time:337102ms step_avg:523.45ms
step:645/2285 train_time:337624ms step_avg:523.45ms
step:646/2285 train_time:338151ms step_avg:523.45ms
step:647/2285 train_time:338671ms step_avg:523.45ms
step:648/2285 train_time:339201ms step_avg:523.46ms
step:649/2285 train_time:339722ms step_avg:523.45ms
step:650/2285 train_time:340250ms step_avg:523.46ms
step:651/2285 train_time:340769ms step_avg:523.46ms
step:652/2285 train_time:341299ms step_avg:523.46ms
step:653/2285 train_time:341819ms step_avg:523.46ms
step:654/2285 train_time:342342ms step_avg:523.46ms
step:655/2285 train_time:342864ms step_avg:523.46ms
step:656/2285 train_time:343389ms step_avg:523.46ms
step:657/2285 train_time:343910ms step_avg:523.46ms
step:658/2285 train_time:344438ms step_avg:523.46ms
step:659/2285 train_time:344960ms step_avg:523.46ms
step:660/2285 train_time:345491ms step_avg:523.47ms
step:661/2285 train_time:346015ms step_avg:523.47ms
step:662/2285 train_time:346539ms step_avg:523.47ms
step:663/2285 train_time:347063ms step_avg:523.47ms
step:664/2285 train_time:347588ms step_avg:523.48ms
step:665/2285 train_time:348109ms step_avg:523.47ms
step:666/2285 train_time:348637ms step_avg:523.48ms
step:667/2285 train_time:349160ms step_avg:523.48ms
step:668/2285 train_time:349683ms step_avg:523.48ms
step:669/2285 train_time:350205ms step_avg:523.47ms
step:670/2285 train_time:350730ms step_avg:523.48ms
step:671/2285 train_time:351251ms step_avg:523.47ms
step:672/2285 train_time:351776ms step_avg:523.48ms
step:673/2285 train_time:352296ms step_avg:523.47ms
step:674/2285 train_time:352821ms step_avg:523.47ms
step:675/2285 train_time:353344ms step_avg:523.47ms
step:676/2285 train_time:353870ms step_avg:523.48ms
step:677/2285 train_time:354397ms step_avg:523.48ms
step:678/2285 train_time:354924ms step_avg:523.49ms
step:679/2285 train_time:355447ms step_avg:523.49ms
step:680/2285 train_time:355973ms step_avg:523.49ms
step:681/2285 train_time:356495ms step_avg:523.49ms
step:682/2285 train_time:357019ms step_avg:523.49ms
step:683/2285 train_time:357545ms step_avg:523.49ms
step:684/2285 train_time:358072ms step_avg:523.50ms
step:685/2285 train_time:358595ms step_avg:523.50ms
step:686/2285 train_time:359121ms step_avg:523.50ms
step:687/2285 train_time:359642ms step_avg:523.50ms
step:688/2285 train_time:360169ms step_avg:523.50ms
step:689/2285 train_time:360690ms step_avg:523.50ms
step:690/2285 train_time:361218ms step_avg:523.50ms
step:691/2285 train_time:361739ms step_avg:523.50ms
step:692/2285 train_time:362265ms step_avg:523.50ms
step:693/2285 train_time:362785ms step_avg:523.50ms
step:694/2285 train_time:363313ms step_avg:523.51ms
step:695/2285 train_time:363834ms step_avg:523.50ms
step:696/2285 train_time:364366ms step_avg:523.52ms
step:697/2285 train_time:364887ms step_avg:523.51ms
step:698/2285 train_time:365414ms step_avg:523.52ms
step:699/2285 train_time:365934ms step_avg:523.51ms
step:700/2285 train_time:366462ms step_avg:523.52ms
step:700/2285 val_loss:4.3397 train_time:366527ms step_avg:523.61ms
step:701/2285 train_time:366987ms step_avg:523.52ms
step:702/2285 train_time:367513ms step_avg:523.52ms
step:703/2285 train_time:368038ms step_avg:523.52ms
step:704/2285 train_time:368567ms step_avg:523.53ms
step:705/2285 train_time:369089ms step_avg:523.53ms
step:706/2285 train_time:369617ms step_avg:523.54ms
step:707/2285 train_time:370139ms step_avg:523.53ms
step:708/2285 train_time:370672ms step_avg:523.55ms
step:709/2285 train_time:371192ms step_avg:523.54ms
step:710/2285 train_time:371720ms step_avg:523.55ms
step:711/2285 train_time:372241ms step_avg:523.55ms
step:712/2285 train_time:372770ms step_avg:523.55ms
step:713/2285 train_time:373294ms step_avg:523.55ms
step:714/2285 train_time:373821ms step_avg:523.56ms
step:715/2285 train_time:374344ms step_avg:523.56ms
step:716/2285 train_time:374873ms step_avg:523.57ms
step:717/2285 train_time:375391ms step_avg:523.56ms
step:718/2285 train_time:375917ms step_avg:523.56ms
step:719/2285 train_time:376436ms step_avg:523.55ms
step:720/2285 train_time:376966ms step_avg:523.56ms
step:721/2285 train_time:377487ms step_avg:523.56ms
step:722/2285 train_time:378018ms step_avg:523.57ms
step:723/2285 train_time:378539ms step_avg:523.57ms
step:724/2285 train_time:379068ms step_avg:523.57ms
step:725/2285 train_time:379586ms step_avg:523.57ms
step:726/2285 train_time:380114ms step_avg:523.57ms
step:727/2285 train_time:380637ms step_avg:523.57ms
step:728/2285 train_time:381164ms step_avg:523.58ms
step:729/2285 train_time:381688ms step_avg:523.58ms
step:730/2285 train_time:382213ms step_avg:523.58ms
step:731/2285 train_time:382735ms step_avg:523.58ms
step:732/2285 train_time:383264ms step_avg:523.58ms
step:733/2285 train_time:383791ms step_avg:523.59ms
step:734/2285 train_time:384321ms step_avg:523.60ms
step:735/2285 train_time:384846ms step_avg:523.60ms
step:736/2285 train_time:385374ms step_avg:523.61ms
step:737/2285 train_time:385895ms step_avg:523.60ms
step:738/2285 train_time:386425ms step_avg:523.61ms
step:739/2285 train_time:386947ms step_avg:523.61ms
step:740/2285 train_time:387473ms step_avg:523.61ms
step:741/2285 train_time:387997ms step_avg:523.61ms
step:742/2285 train_time:388527ms step_avg:523.62ms
step:743/2285 train_time:389049ms step_avg:523.62ms
step:744/2285 train_time:389574ms step_avg:523.62ms
step:745/2285 train_time:390098ms step_avg:523.62ms
step:746/2285 train_time:390625ms step_avg:523.63ms
step:747/2285 train_time:391145ms step_avg:523.62ms
step:748/2285 train_time:391671ms step_avg:523.62ms
step:749/2285 train_time:392198ms step_avg:523.63ms
step:750/2285 train_time:392731ms step_avg:523.64ms
step:751/2285 train_time:393258ms step_avg:523.65ms
step:752/2285 train_time:393795ms step_avg:523.66ms
step:753/2285 train_time:394322ms step_avg:523.67ms
step:754/2285 train_time:394856ms step_avg:523.68ms
step:755/2285 train_time:395384ms step_avg:523.69ms
step:756/2285 train_time:395914ms step_avg:523.70ms
step:757/2285 train_time:396446ms step_avg:523.71ms
step:758/2285 train_time:396978ms step_avg:523.72ms
step:759/2285 train_time:397506ms step_avg:523.72ms
step:760/2285 train_time:398041ms step_avg:523.74ms
step:761/2285 train_time:398569ms step_avg:523.74ms
step:762/2285 train_time:399104ms step_avg:523.76ms
step:763/2285 train_time:399634ms step_avg:523.77ms
step:764/2285 train_time:400166ms step_avg:523.78ms
step:765/2285 train_time:400694ms step_avg:523.78ms
step:766/2285 train_time:401227ms step_avg:523.80ms
step:767/2285 train_time:401756ms step_avg:523.80ms
step:768/2285 train_time:402292ms step_avg:523.82ms
step:769/2285 train_time:402819ms step_avg:523.82ms
step:770/2285 train_time:403353ms step_avg:523.84ms
step:771/2285 train_time:403882ms step_avg:523.84ms
step:772/2285 train_time:404415ms step_avg:523.85ms
step:773/2285 train_time:404947ms step_avg:523.86ms
step:774/2285 train_time:405480ms step_avg:523.88ms
step:775/2285 train_time:406008ms step_avg:523.88ms
step:776/2285 train_time:406542ms step_avg:523.89ms
step:777/2285 train_time:407069ms step_avg:523.90ms
step:778/2285 train_time:407603ms step_avg:523.91ms
step:779/2285 train_time:408129ms step_avg:523.91ms
step:780/2285 train_time:408667ms step_avg:523.93ms
step:781/2285 train_time:409194ms step_avg:523.94ms
step:782/2285 train_time:409731ms step_avg:523.95ms
step:783/2285 train_time:410259ms step_avg:523.96ms
step:784/2285 train_time:410789ms step_avg:523.97ms
step:785/2285 train_time:411321ms step_avg:523.98ms
step:786/2285 train_time:411851ms step_avg:523.98ms
step:787/2285 train_time:412377ms step_avg:523.99ms
step:788/2285 train_time:412909ms step_avg:524.00ms
step:789/2285 train_time:413437ms step_avg:524.00ms
step:790/2285 train_time:413969ms step_avg:524.01ms
step:791/2285 train_time:414498ms step_avg:524.02ms
step:792/2285 train_time:415032ms step_avg:524.03ms
step:793/2285 train_time:415558ms step_avg:524.03ms
step:794/2285 train_time:416090ms step_avg:524.04ms
step:795/2285 train_time:416617ms step_avg:524.05ms
step:796/2285 train_time:417151ms step_avg:524.06ms
step:797/2285 train_time:417680ms step_avg:524.07ms
step:798/2285 train_time:418212ms step_avg:524.07ms
step:799/2285 train_time:418739ms step_avg:524.08ms
step:800/2285 train_time:419274ms step_avg:524.09ms
step:800/2285 val_loss:4.2713 train_time:419340ms step_avg:524.17ms
step:801/2285 train_time:419805ms step_avg:524.10ms
step:802/2285 train_time:420343ms step_avg:524.12ms
step:803/2285 train_time:420871ms step_avg:524.12ms
step:804/2285 train_time:421403ms step_avg:524.13ms
step:805/2285 train_time:421928ms step_avg:524.13ms
step:806/2285 train_time:422463ms step_avg:524.15ms
step:807/2285 train_time:422993ms step_avg:524.15ms
step:808/2285 train_time:423527ms step_avg:524.17ms
step:809/2285 train_time:424051ms step_avg:524.17ms
step:810/2285 train_time:424584ms step_avg:524.18ms
step:811/2285 train_time:425111ms step_avg:524.18ms
step:812/2285 train_time:425643ms step_avg:524.19ms
step:813/2285 train_time:426170ms step_avg:524.19ms
step:814/2285 train_time:426701ms step_avg:524.20ms
step:815/2285 train_time:427229ms step_avg:524.21ms
step:816/2285 train_time:427762ms step_avg:524.22ms
step:817/2285 train_time:428290ms step_avg:524.22ms
step:818/2285 train_time:428823ms step_avg:524.23ms
step:819/2285 train_time:429350ms step_avg:524.24ms
step:820/2285 train_time:429883ms step_avg:524.25ms
step:821/2285 train_time:430415ms step_avg:524.26ms
step:822/2285 train_time:430947ms step_avg:524.27ms
step:823/2285 train_time:431475ms step_avg:524.27ms
step:824/2285 train_time:432008ms step_avg:524.28ms
step:825/2285 train_time:432540ms step_avg:524.29ms
step:826/2285 train_time:433072ms step_avg:524.30ms
step:827/2285 train_time:433599ms step_avg:524.30ms
step:828/2285 train_time:434132ms step_avg:524.31ms
step:829/2285 train_time:434661ms step_avg:524.32ms
step:830/2285 train_time:435195ms step_avg:524.33ms
step:831/2285 train_time:435724ms step_avg:524.34ms
step:832/2285 train_time:436259ms step_avg:524.35ms
step:833/2285 train_time:436786ms step_avg:524.35ms
step:834/2285 train_time:437315ms step_avg:524.36ms
step:835/2285 train_time:437847ms step_avg:524.37ms
step:836/2285 train_time:438380ms step_avg:524.38ms
step:837/2285 train_time:438908ms step_avg:524.38ms
step:838/2285 train_time:439442ms step_avg:524.39ms
step:839/2285 train_time:439970ms step_avg:524.40ms
step:840/2285 train_time:440504ms step_avg:524.41ms
step:841/2285 train_time:441033ms step_avg:524.41ms
step:842/2285 train_time:441566ms step_avg:524.42ms
step:843/2285 train_time:442094ms step_avg:524.43ms
step:844/2285 train_time:442628ms step_avg:524.44ms
step:845/2285 train_time:443155ms step_avg:524.44ms
step:846/2285 train_time:443690ms step_avg:524.46ms
step:847/2285 train_time:444215ms step_avg:524.46ms
step:848/2285 train_time:444753ms step_avg:524.47ms
step:849/2285 train_time:445279ms step_avg:524.47ms
step:850/2285 train_time:445814ms step_avg:524.49ms
step:851/2285 train_time:446340ms step_avg:524.49ms
step:852/2285 train_time:446872ms step_avg:524.50ms
step:853/2285 train_time:447396ms step_avg:524.50ms
step:854/2285 train_time:447925ms step_avg:524.50ms
step:855/2285 train_time:448452ms step_avg:524.51ms
step:856/2285 train_time:448983ms step_avg:524.51ms
step:857/2285 train_time:449512ms step_avg:524.52ms
step:858/2285 train_time:450047ms step_avg:524.53ms
step:859/2285 train_time:450577ms step_avg:524.54ms
step:860/2285 train_time:451113ms step_avg:524.55ms
step:861/2285 train_time:451643ms step_avg:524.56ms
step:862/2285 train_time:452172ms step_avg:524.56ms
step:863/2285 train_time:452700ms step_avg:524.57ms
step:864/2285 train_time:453235ms step_avg:524.58ms
step:865/2285 train_time:453763ms step_avg:524.58ms
step:866/2285 train_time:454297ms step_avg:524.59ms
step:867/2285 train_time:454826ms step_avg:524.60ms
step:868/2285 train_time:455358ms step_avg:524.61ms
step:869/2285 train_time:455887ms step_avg:524.61ms
step:870/2285 train_time:456427ms step_avg:524.63ms
step:871/2285 train_time:456953ms step_avg:524.63ms
step:872/2285 train_time:457486ms step_avg:524.64ms
step:873/2285 train_time:458015ms step_avg:524.65ms
step:874/2285 train_time:458549ms step_avg:524.66ms
step:875/2285 train_time:459078ms step_avg:524.66ms
step:876/2285 train_time:459614ms step_avg:524.67ms
step:877/2285 train_time:460144ms step_avg:524.68ms
step:878/2285 train_time:460674ms step_avg:524.69ms
step:879/2285 train_time:461203ms step_avg:524.69ms
step:880/2285 train_time:461740ms step_avg:524.70ms
step:881/2285 train_time:462268ms step_avg:524.71ms
step:882/2285 train_time:462801ms step_avg:524.72ms
step:883/2285 train_time:463327ms step_avg:524.72ms
step:884/2285 train_time:463862ms step_avg:524.73ms
step:885/2285 train_time:464387ms step_avg:524.73ms
step:886/2285 train_time:464922ms step_avg:524.74ms
step:887/2285 train_time:465450ms step_avg:524.75ms
step:888/2285 train_time:465983ms step_avg:524.76ms
step:889/2285 train_time:466512ms step_avg:524.76ms
step:890/2285 train_time:467048ms step_avg:524.77ms
step:891/2285 train_time:467574ms step_avg:524.77ms
step:892/2285 train_time:468110ms step_avg:524.79ms
step:893/2285 train_time:468639ms step_avg:524.79ms
step:894/2285 train_time:469175ms step_avg:524.80ms
step:895/2285 train_time:469708ms step_avg:524.81ms
step:896/2285 train_time:470243ms step_avg:524.82ms
step:897/2285 train_time:470768ms step_avg:524.83ms
step:898/2285 train_time:471300ms step_avg:524.83ms
step:899/2285 train_time:471827ms step_avg:524.84ms
step:900/2285 train_time:472360ms step_avg:524.84ms
step:900/2285 val_loss:4.2225 train_time:472424ms step_avg:524.92ms
step:901/2285 train_time:472893ms step_avg:524.85ms
step:902/2285 train_time:473423ms step_avg:524.86ms
step:903/2285 train_time:473949ms step_avg:524.86ms
step:904/2285 train_time:474481ms step_avg:524.87ms
step:905/2285 train_time:475011ms step_avg:524.87ms
step:906/2285 train_time:475543ms step_avg:524.88ms
step:907/2285 train_time:476074ms step_avg:524.89ms
step:908/2285 train_time:476607ms step_avg:524.90ms
step:909/2285 train_time:477136ms step_avg:524.90ms
step:910/2285 train_time:477668ms step_avg:524.91ms
step:911/2285 train_time:478199ms step_avg:524.92ms
step:912/2285 train_time:478733ms step_avg:524.93ms
step:913/2285 train_time:479261ms step_avg:524.93ms
step:914/2285 train_time:479797ms step_avg:524.94ms
step:915/2285 train_time:480324ms step_avg:524.94ms
step:916/2285 train_time:480860ms step_avg:524.96ms
step:917/2285 train_time:481386ms step_avg:524.96ms
step:918/2285 train_time:481922ms step_avg:524.97ms
step:919/2285 train_time:482448ms step_avg:524.97ms
step:920/2285 train_time:482985ms step_avg:524.98ms
step:921/2285 train_time:483510ms step_avg:524.98ms
step:922/2285 train_time:484050ms step_avg:525.00ms
step:923/2285 train_time:484577ms step_avg:525.00ms
step:924/2285 train_time:485115ms step_avg:525.02ms
step:925/2285 train_time:485644ms step_avg:525.02ms
step:926/2285 train_time:486179ms step_avg:525.03ms
step:927/2285 train_time:486707ms step_avg:525.03ms
step:928/2285 train_time:487242ms step_avg:525.04ms
step:929/2285 train_time:487771ms step_avg:525.05ms
step:930/2285 train_time:488305ms step_avg:525.06ms
step:931/2285 train_time:488835ms step_avg:525.06ms
step:932/2285 train_time:489367ms step_avg:525.07ms
step:933/2285 train_time:489891ms step_avg:525.07ms
step:934/2285 train_time:490426ms step_avg:525.08ms
step:935/2285 train_time:490954ms step_avg:525.08ms
step:936/2285 train_time:491486ms step_avg:525.09ms
step:937/2285 train_time:492013ms step_avg:525.09ms
step:938/2285 train_time:492548ms step_avg:525.10ms
step:939/2285 train_time:493075ms step_avg:525.11ms
step:940/2285 train_time:493608ms step_avg:525.12ms
step:941/2285 train_time:494139ms step_avg:525.12ms
step:942/2285 train_time:494673ms step_avg:525.13ms
step:943/2285 train_time:495201ms step_avg:525.13ms
step:944/2285 train_time:495733ms step_avg:525.14ms
step:945/2285 train_time:496262ms step_avg:525.14ms
step:946/2285 train_time:496796ms step_avg:525.15ms
step:947/2285 train_time:497326ms step_avg:525.16ms
step:948/2285 train_time:497860ms step_avg:525.17ms
step:949/2285 train_time:498388ms step_avg:525.17ms
step:950/2285 train_time:498922ms step_avg:525.18ms
step:951/2285 train_time:499450ms step_avg:525.18ms
step:952/2285 train_time:499981ms step_avg:525.19ms
step:953/2285 train_time:500506ms step_avg:525.19ms
step:954/2285 train_time:501039ms step_avg:525.20ms
step:955/2285 train_time:501567ms step_avg:525.20ms
step:956/2285 train_time:502099ms step_avg:525.21ms
step:957/2285 train_time:502626ms step_avg:525.21ms
step:958/2285 train_time:503161ms step_avg:525.22ms
step:959/2285 train_time:503689ms step_avg:525.22ms
step:960/2285 train_time:504224ms step_avg:525.23ms
step:961/2285 train_time:504752ms step_avg:525.24ms
step:962/2285 train_time:505289ms step_avg:525.25ms
step:963/2285 train_time:505816ms step_avg:525.25ms
step:964/2285 train_time:506349ms step_avg:525.26ms
step:965/2285 train_time:506878ms step_avg:525.26ms
step:966/2285 train_time:507412ms step_avg:525.27ms
step:967/2285 train_time:507940ms step_avg:525.27ms
step:968/2285 train_time:508474ms step_avg:525.28ms
step:969/2285 train_time:509004ms step_avg:525.29ms
step:970/2285 train_time:509541ms step_avg:525.30ms
step:971/2285 train_time:510067ms step_avg:525.30ms
step:972/2285 train_time:510600ms step_avg:525.31ms
step:973/2285 train_time:511128ms step_avg:525.31ms
step:974/2285 train_time:511662ms step_avg:525.32ms
step:975/2285 train_time:512187ms step_avg:525.32ms
step:976/2285 train_time:512723ms step_avg:525.33ms
step:977/2285 train_time:513252ms step_avg:525.33ms
step:978/2285 train_time:513787ms step_avg:525.34ms
step:979/2285 train_time:514316ms step_avg:525.35ms
step:980/2285 train_time:514849ms step_avg:525.36ms
step:981/2285 train_time:515380ms step_avg:525.36ms
step:982/2285 train_time:515914ms step_avg:525.37ms
step:983/2285 train_time:516441ms step_avg:525.37ms
step:984/2285 train_time:516973ms step_avg:525.38ms
step:985/2285 train_time:517501ms step_avg:525.38ms
step:986/2285 train_time:518031ms step_avg:525.39ms
step:987/2285 train_time:518562ms step_avg:525.39ms
step:988/2285 train_time:519094ms step_avg:525.40ms
step:989/2285 train_time:519623ms step_avg:525.40ms
step:990/2285 train_time:520154ms step_avg:525.41ms
step:991/2285 train_time:520682ms step_avg:525.41ms
step:992/2285 train_time:521220ms step_avg:525.42ms
step:993/2285 train_time:521745ms step_avg:525.42ms
step:994/2285 train_time:522280ms step_avg:525.43ms
step:995/2285 train_time:522809ms step_avg:525.44ms
step:996/2285 train_time:523345ms step_avg:525.45ms
step:997/2285 train_time:523877ms step_avg:525.45ms
step:998/2285 train_time:524409ms step_avg:525.46ms
step:999/2285 train_time:524937ms step_avg:525.46ms
step:1000/2285 train_time:525472ms step_avg:525.47ms
step:1000/2285 val_loss:4.1768 train_time:525537ms step_avg:525.54ms
step:1001/2285 train_time:526010ms step_avg:525.48ms
step:1002/2285 train_time:526541ms step_avg:525.49ms
step:1003/2285 train_time:527073ms step_avg:525.50ms
step:1004/2285 train_time:527599ms step_avg:525.50ms
step:1005/2285 train_time:528132ms step_avg:525.50ms
step:1006/2285 train_time:528663ms step_avg:525.51ms
step:1007/2285 train_time:529190ms step_avg:525.51ms
step:1008/2285 train_time:529720ms step_avg:525.52ms
step:1009/2285 train_time:530249ms step_avg:525.52ms
step:1010/2285 train_time:530782ms step_avg:525.53ms
step:1011/2285 train_time:531314ms step_avg:525.53ms
step:1012/2285 train_time:531846ms step_avg:525.54ms
step:1013/2285 train_time:532374ms step_avg:525.54ms
step:1014/2285 train_time:532907ms step_avg:525.55ms
step:1015/2285 train_time:533436ms step_avg:525.55ms
step:1016/2285 train_time:533967ms step_avg:525.56ms
step:1017/2285 train_time:534498ms step_avg:525.56ms
step:1018/2285 train_time:535028ms step_avg:525.57ms
step:1019/2285 train_time:535555ms step_avg:525.57ms
step:1020/2285 train_time:536084ms step_avg:525.57ms
step:1021/2285 train_time:536611ms step_avg:525.57ms
step:1022/2285 train_time:537141ms step_avg:525.58ms
step:1023/2285 train_time:537668ms step_avg:525.58ms
step:1024/2285 train_time:538199ms step_avg:525.58ms
step:1025/2285 train_time:538727ms step_avg:525.59ms
step:1026/2285 train_time:539258ms step_avg:525.59ms
step:1027/2285 train_time:539784ms step_avg:525.59ms
step:1028/2285 train_time:540314ms step_avg:525.60ms
step:1029/2285 train_time:540843ms step_avg:525.60ms
step:1030/2285 train_time:541376ms step_avg:525.61ms
step:1031/2285 train_time:541904ms step_avg:525.61ms
step:1032/2285 train_time:542435ms step_avg:525.62ms
step:1033/2285 train_time:542963ms step_avg:525.62ms
step:1034/2285 train_time:543494ms step_avg:525.62ms
step:1035/2285 train_time:544022ms step_avg:525.62ms
step:1036/2285 train_time:544553ms step_avg:525.63ms
step:1037/2285 train_time:545082ms step_avg:525.63ms
step:1038/2285 train_time:545612ms step_avg:525.64ms
step:1039/2285 train_time:546138ms step_avg:525.64ms
step:1040/2285 train_time:546673ms step_avg:525.65ms
step:1041/2285 train_time:547198ms step_avg:525.65ms
step:1042/2285 train_time:547727ms step_avg:525.65ms
step:1043/2285 train_time:548257ms step_avg:525.65ms
step:1044/2285 train_time:548789ms step_avg:525.66ms
step:1045/2285 train_time:549319ms step_avg:525.66ms
step:1046/2285 train_time:549850ms step_avg:525.67ms
step:1047/2285 train_time:550380ms step_avg:525.67ms
step:1048/2285 train_time:550911ms step_avg:525.68ms
step:1049/2285 train_time:551438ms step_avg:525.68ms
step:1050/2285 train_time:551970ms step_avg:525.69ms
step:1051/2285 train_time:552496ms step_avg:525.69ms
step:1052/2285 train_time:553026ms step_avg:525.69ms
step:1053/2285 train_time:553557ms step_avg:525.70ms
step:1054/2285 train_time:554087ms step_avg:525.70ms
step:1055/2285 train_time:554617ms step_avg:525.70ms
step:1056/2285 train_time:555146ms step_avg:525.71ms
step:1057/2285 train_time:555675ms step_avg:525.71ms
step:1058/2285 train_time:556209ms step_avg:525.72ms
step:1059/2285 train_time:556737ms step_avg:525.72ms
step:1060/2285 train_time:557271ms step_avg:525.73ms
step:1061/2285 train_time:557799ms step_avg:525.73ms
step:1062/2285 train_time:558327ms step_avg:525.73ms
step:1063/2285 train_time:558858ms step_avg:525.74ms
step:1064/2285 train_time:559392ms step_avg:525.74ms
step:1065/2285 train_time:559920ms step_avg:525.75ms
step:1066/2285 train_time:560451ms step_avg:525.75ms
step:1067/2285 train_time:560982ms step_avg:525.76ms
step:1068/2285 train_time:561513ms step_avg:525.76ms
step:1069/2285 train_time:562040ms step_avg:525.76ms
step:1070/2285 train_time:562572ms step_avg:525.77ms
step:1071/2285 train_time:563098ms step_avg:525.77ms
step:1072/2285 train_time:563627ms step_avg:525.77ms
step:1073/2285 train_time:564155ms step_avg:525.77ms
step:1074/2285 train_time:564684ms step_avg:525.78ms
step:1075/2285 train_time:565217ms step_avg:525.78ms
step:1076/2285 train_time:565750ms step_avg:525.79ms
step:1077/2285 train_time:566279ms step_avg:525.79ms
step:1078/2285 train_time:566809ms step_avg:525.80ms
step:1079/2285 train_time:567333ms step_avg:525.80ms
step:1080/2285 train_time:567865ms step_avg:525.80ms
step:1081/2285 train_time:568395ms step_avg:525.80ms
step:1082/2285 train_time:568927ms step_avg:525.81ms
step:1083/2285 train_time:569457ms step_avg:525.81ms
step:1084/2285 train_time:569987ms step_avg:525.82ms
step:1085/2285 train_time:570514ms step_avg:525.82ms
step:1086/2285 train_time:571049ms step_avg:525.83ms
step:1087/2285 train_time:571577ms step_avg:525.83ms
step:1088/2285 train_time:572110ms step_avg:525.84ms
step:1089/2285 train_time:572638ms step_avg:525.84ms
step:1090/2285 train_time:573170ms step_avg:525.84ms
step:1091/2285 train_time:573698ms step_avg:525.85ms
step:1092/2285 train_time:574227ms step_avg:525.85ms
step:1093/2285 train_time:574756ms step_avg:525.85ms
step:1094/2285 train_time:575287ms step_avg:525.86ms
step:1095/2285 train_time:575814ms step_avg:525.86ms
step:1096/2285 train_time:576347ms step_avg:525.86ms
step:1097/2285 train_time:576874ms step_avg:525.87ms
step:1098/2285 train_time:577412ms step_avg:525.88ms
step:1099/2285 train_time:577935ms step_avg:525.87ms
step:1100/2285 train_time:578468ms step_avg:525.88ms
step:1100/2285 val_loss:4.1720 train_time:578534ms step_avg:525.94ms
step:1101/2285 train_time:578999ms step_avg:525.88ms
step:1102/2285 train_time:579533ms step_avg:525.89ms
step:1103/2285 train_time:580058ms step_avg:525.89ms
step:1104/2285 train_time:580592ms step_avg:525.90ms
step:1105/2285 train_time:581123ms step_avg:525.90ms
step:1106/2285 train_time:581656ms step_avg:525.91ms
step:1107/2285 train_time:582185ms step_avg:525.91ms
step:1108/2285 train_time:582721ms step_avg:525.92ms
step:1109/2285 train_time:583245ms step_avg:525.92ms
step:1110/2285 train_time:583782ms step_avg:525.93ms
step:1111/2285 train_time:584311ms step_avg:525.93ms
step:1112/2285 train_time:584846ms step_avg:525.94ms
step:1113/2285 train_time:585372ms step_avg:525.94ms
step:1114/2285 train_time:585907ms step_avg:525.95ms
step:1115/2285 train_time:586436ms step_avg:525.95ms
step:1116/2285 train_time:586970ms step_avg:525.96ms
step:1117/2285 train_time:587501ms step_avg:525.96ms
step:1118/2285 train_time:588030ms step_avg:525.97ms
step:1119/2285 train_time:588560ms step_avg:525.97ms
step:1120/2285 train_time:589091ms step_avg:525.97ms
step:1121/2285 train_time:589620ms step_avg:525.98ms
step:1122/2285 train_time:590152ms step_avg:525.98ms
step:1123/2285 train_time:590680ms step_avg:525.98ms
step:1124/2285 train_time:591213ms step_avg:525.99ms
step:1125/2285 train_time:591741ms step_avg:525.99ms
step:1126/2285 train_time:592273ms step_avg:526.00ms
step:1127/2285 train_time:592804ms step_avg:526.00ms
step:1128/2285 train_time:593333ms step_avg:526.00ms
step:1129/2285 train_time:593861ms step_avg:526.01ms
step:1130/2285 train_time:594393ms step_avg:526.01ms
step:1131/2285 train_time:594924ms step_avg:526.02ms
step:1132/2285 train_time:595456ms step_avg:526.02ms
step:1133/2285 train_time:595984ms step_avg:526.02ms
step:1134/2285 train_time:596515ms step_avg:526.03ms
step:1135/2285 train_time:597045ms step_avg:526.03ms
step:1136/2285 train_time:597573ms step_avg:526.03ms
step:1137/2285 train_time:598101ms step_avg:526.03ms
step:1138/2285 train_time:598631ms step_avg:526.04ms
step:1139/2285 train_time:599163ms step_avg:526.04ms
step:1140/2285 train_time:599695ms step_avg:526.05ms
step:1141/2285 train_time:600221ms step_avg:526.05ms
step:1142/2285 train_time:600749ms step_avg:526.05ms
step:1143/2285 train_time:601279ms step_avg:526.05ms
step:1144/2285 train_time:601810ms step_avg:526.06ms
step:1145/2285 train_time:602334ms step_avg:526.06ms
step:1146/2285 train_time:602866ms step_avg:526.06ms
step:1147/2285 train_time:603392ms step_avg:526.06ms
step:1148/2285 train_time:603925ms step_avg:526.07ms
step:1149/2285 train_time:604456ms step_avg:526.07ms
step:1150/2285 train_time:604990ms step_avg:526.08ms
step:1151/2285 train_time:605516ms step_avg:526.08ms
step:1152/2285 train_time:606048ms step_avg:526.08ms
step:1153/2285 train_time:606575ms step_avg:526.08ms
step:1154/2285 train_time:607108ms step_avg:526.09ms
step:1155/2285 train_time:607632ms step_avg:526.09ms
step:1156/2285 train_time:608168ms step_avg:526.10ms
step:1157/2285 train_time:608696ms step_avg:526.10ms
step:1158/2285 train_time:609228ms step_avg:526.10ms
step:1159/2285 train_time:609749ms step_avg:526.10ms
step:1160/2285 train_time:610281ms step_avg:526.10ms
step:1161/2285 train_time:610815ms step_avg:526.11ms
step:1162/2285 train_time:611348ms step_avg:526.12ms
step:1163/2285 train_time:611876ms step_avg:526.12ms
step:1164/2285 train_time:612409ms step_avg:526.12ms
step:1165/2285 train_time:612935ms step_avg:526.12ms
step:1166/2285 train_time:613470ms step_avg:526.13ms
step:1167/2285 train_time:614000ms step_avg:526.13ms
step:1168/2285 train_time:614535ms step_avg:526.14ms
step:1169/2285 train_time:615061ms step_avg:526.14ms
step:1170/2285 train_time:615593ms step_avg:526.15ms
step:1171/2285 train_time:616120ms step_avg:526.15ms
step:1172/2285 train_time:616652ms step_avg:526.15ms
step:1173/2285 train_time:617179ms step_avg:526.15ms
step:1174/2285 train_time:617714ms step_avg:526.16ms
step:1175/2285 train_time:618245ms step_avg:526.17ms
step:1176/2285 train_time:618773ms step_avg:526.17ms
step:1177/2285 train_time:619301ms step_avg:526.17ms
step:1178/2285 train_time:619837ms step_avg:526.18ms
step:1179/2285 train_time:620365ms step_avg:526.18ms
step:1180/2285 train_time:620901ms step_avg:526.19ms
step:1181/2285 train_time:621428ms step_avg:526.19ms
step:1182/2285 train_time:621964ms step_avg:526.20ms
step:1183/2285 train_time:622487ms step_avg:526.19ms
step:1184/2285 train_time:623023ms step_avg:526.20ms
step:1185/2285 train_time:623550ms step_avg:526.20ms
step:1186/2285 train_time:624087ms step_avg:526.21ms
step:1187/2285 train_time:624614ms step_avg:526.21ms
step:1188/2285 train_time:625149ms step_avg:526.22ms
step:1189/2285 train_time:625679ms step_avg:526.22ms
step:1190/2285 train_time:626214ms step_avg:526.23ms
step:1191/2285 train_time:626742ms step_avg:526.23ms
step:1192/2285 train_time:627275ms step_avg:526.24ms
step:1193/2285 train_time:627802ms step_avg:526.24ms
step:1194/2285 train_time:628332ms step_avg:526.24ms
step:1195/2285 train_time:628857ms step_avg:526.24ms
step:1196/2285 train_time:629389ms step_avg:526.25ms
step:1197/2285 train_time:629917ms step_avg:526.25ms
step:1198/2285 train_time:630452ms step_avg:526.25ms
step:1199/2285 train_time:630979ms step_avg:526.25ms
step:1200/2285 train_time:631512ms step_avg:526.26ms
step:1200/2285 val_loss:4.1083 train_time:631577ms step_avg:526.31ms
step:1201/2285 train_time:632040ms step_avg:526.26ms
step:1202/2285 train_time:632573ms step_avg:526.27ms
step:1203/2285 train_time:633103ms step_avg:526.27ms
step:1204/2285 train_time:633632ms step_avg:526.27ms
step:1205/2285 train_time:634159ms step_avg:526.27ms
step:1206/2285 train_time:634690ms step_avg:526.28ms
step:1207/2285 train_time:635219ms step_avg:526.28ms
step:1208/2285 train_time:635749ms step_avg:526.28ms
step:1209/2285 train_time:636276ms step_avg:526.28ms
step:1210/2285 train_time:636811ms step_avg:526.29ms
step:1211/2285 train_time:637343ms step_avg:526.29ms
step:1212/2285 train_time:637875ms step_avg:526.30ms
step:1213/2285 train_time:638403ms step_avg:526.30ms
step:1214/2285 train_time:638938ms step_avg:526.31ms
step:1215/2285 train_time:639462ms step_avg:526.31ms
step:1216/2285 train_time:639994ms step_avg:526.31ms
step:1217/2285 train_time:640523ms step_avg:526.31ms
step:1218/2285 train_time:641056ms step_avg:526.32ms
step:1219/2285 train_time:641583ms step_avg:526.32ms
step:1220/2285 train_time:642113ms step_avg:526.32ms
step:1221/2285 train_time:642641ms step_avg:526.32ms
step:1222/2285 train_time:643172ms step_avg:526.33ms
step:1223/2285 train_time:643700ms step_avg:526.33ms
step:1224/2285 train_time:644234ms step_avg:526.33ms
step:1225/2285 train_time:644761ms step_avg:526.34ms
step:1226/2285 train_time:645295ms step_avg:526.34ms
step:1227/2285 train_time:645825ms step_avg:526.35ms
step:1228/2285 train_time:646358ms step_avg:526.35ms
step:1229/2285 train_time:646886ms step_avg:526.35ms
step:1230/2285 train_time:647418ms step_avg:526.36ms
step:1231/2285 train_time:647948ms step_avg:526.36ms
step:1232/2285 train_time:648477ms step_avg:526.36ms
step:1233/2285 train_time:649005ms step_avg:526.36ms
step:1234/2285 train_time:649535ms step_avg:526.37ms
step:1235/2285 train_time:650064ms step_avg:526.37ms
step:1236/2285 train_time:650595ms step_avg:526.37ms
step:1237/2285 train_time:651121ms step_avg:526.37ms
step:1238/2285 train_time:651654ms step_avg:526.38ms
step:1239/2285 train_time:652188ms step_avg:526.38ms
step:1240/2285 train_time:652721ms step_avg:526.39ms
step:1241/2285 train_time:653246ms step_avg:526.39ms
step:1242/2285 train_time:653776ms step_avg:526.39ms
step:1243/2285 train_time:654304ms step_avg:526.39ms
step:1244/2285 train_time:654839ms step_avg:526.40ms
step:1245/2285 train_time:655370ms step_avg:526.40ms
step:1246/2285 train_time:655900ms step_avg:526.40ms
step:1247/2285 train_time:656430ms step_avg:526.41ms
step:1248/2285 train_time:656962ms step_avg:526.41ms
step:1249/2285 train_time:657494ms step_avg:526.42ms
step:1250/2285 train_time:658026ms step_avg:526.42ms
step:1251/2285 train_time:658553ms step_avg:526.42ms
step:1252/2285 train_time:659087ms step_avg:526.43ms
step:1253/2285 train_time:659617ms step_avg:526.43ms
step:1254/2285 train_time:660151ms step_avg:526.44ms
step:1255/2285 train_time:660681ms step_avg:526.44ms
step:1256/2285 train_time:661212ms step_avg:526.44ms
step:1257/2285 train_time:661741ms step_avg:526.44ms
step:1258/2285 train_time:662270ms step_avg:526.45ms
step:1259/2285 train_time:662796ms step_avg:526.45ms
step:1260/2285 train_time:663326ms step_avg:526.45ms
step:1261/2285 train_time:663858ms step_avg:526.45ms
step:1262/2285 train_time:664389ms step_avg:526.46ms
step:1263/2285 train_time:664916ms step_avg:526.46ms
step:1264/2285 train_time:665448ms step_avg:526.46ms
step:1265/2285 train_time:665976ms step_avg:526.46ms
step:1266/2285 train_time:666510ms step_avg:526.47ms
step:1267/2285 train_time:667035ms step_avg:526.47ms
step:1268/2285 train_time:667569ms step_avg:526.47ms
step:1269/2285 train_time:668095ms step_avg:526.47ms
step:1270/2285 train_time:668629ms step_avg:526.48ms
step:1271/2285 train_time:669152ms step_avg:526.48ms
step:1272/2285 train_time:669688ms step_avg:526.48ms
step:1273/2285 train_time:670212ms step_avg:526.48ms
step:1274/2285 train_time:670747ms step_avg:526.49ms
step:1275/2285 train_time:671275ms step_avg:526.49ms
step:1276/2285 train_time:671809ms step_avg:526.50ms
step:1277/2285 train_time:672339ms step_avg:526.50ms
step:1278/2285 train_time:672873ms step_avg:526.50ms
step:1279/2285 train_time:673402ms step_avg:526.51ms
step:1280/2285 train_time:673935ms step_avg:526.51ms
step:1281/2285 train_time:674464ms step_avg:526.51ms
step:1282/2285 train_time:674997ms step_avg:526.52ms
step:1283/2285 train_time:675527ms step_avg:526.52ms
step:1284/2285 train_time:676059ms step_avg:526.53ms
step:1285/2285 train_time:676584ms step_avg:526.52ms
step:1286/2285 train_time:677116ms step_avg:526.53ms
step:1287/2285 train_time:677644ms step_avg:526.53ms
step:1288/2285 train_time:678177ms step_avg:526.53ms
step:1289/2285 train_time:678703ms step_avg:526.53ms
step:1290/2285 train_time:679236ms step_avg:526.54ms
step:1291/2285 train_time:679762ms step_avg:526.54ms
step:1292/2285 train_time:680291ms step_avg:526.54ms
step:1293/2285 train_time:680820ms step_avg:526.54ms
step:1294/2285 train_time:681349ms step_avg:526.54ms
step:1295/2285 train_time:681877ms step_avg:526.55ms
step:1296/2285 train_time:682412ms step_avg:526.55ms
step:1297/2285 train_time:682940ms step_avg:526.55ms
step:1298/2285 train_time:683470ms step_avg:526.56ms
step:1299/2285 train_time:683996ms step_avg:526.56ms
step:1300/2285 train_time:684526ms step_avg:526.56ms
step:1300/2285 val_loss:4.0730 train_time:684591ms step_avg:526.61ms
step:1301/2285 train_time:685056ms step_avg:526.56ms
step:1302/2285 train_time:685588ms step_avg:526.57ms
step:1303/2285 train_time:686115ms step_avg:526.57ms
step:1304/2285 train_time:686647ms step_avg:526.57ms
step:1305/2285 train_time:687176ms step_avg:526.57ms
step:1306/2285 train_time:687712ms step_avg:526.58ms
step:1307/2285 train_time:688236ms step_avg:526.58ms
step:1308/2285 train_time:688770ms step_avg:526.58ms
step:1309/2285 train_time:689293ms step_avg:526.58ms
step:1310/2285 train_time:689827ms step_avg:526.59ms
step:1311/2285 train_time:690351ms step_avg:526.58ms
step:1312/2285 train_time:690887ms step_avg:526.59ms
step:1313/2285 train_time:691413ms step_avg:526.59ms
step:1314/2285 train_time:691949ms step_avg:526.60ms
step:1315/2285 train_time:692474ms step_avg:526.60ms
step:1316/2285 train_time:693008ms step_avg:526.60ms
step:1317/2285 train_time:693538ms step_avg:526.60ms
step:1318/2285 train_time:694072ms step_avg:526.61ms
step:1319/2285 train_time:694600ms step_avg:526.61ms
step:1320/2285 train_time:695134ms step_avg:526.62ms
step:1321/2285 train_time:695664ms step_avg:526.62ms
step:1322/2285 train_time:696198ms step_avg:526.62ms
step:1323/2285 train_time:696727ms step_avg:526.63ms
step:1324/2285 train_time:697264ms step_avg:526.63ms
step:1325/2285 train_time:697793ms step_avg:526.64ms
step:1326/2285 train_time:698328ms step_avg:526.64ms
step:1327/2285 train_time:698856ms step_avg:526.64ms
step:1328/2285 train_time:699387ms step_avg:526.65ms
step:1329/2285 train_time:699912ms step_avg:526.65ms
step:1330/2285 train_time:700445ms step_avg:526.65ms
step:1331/2285 train_time:700974ms step_avg:526.65ms
step:1332/2285 train_time:701507ms step_avg:526.66ms
step:1333/2285 train_time:702031ms step_avg:526.65ms
step:1334/2285 train_time:702564ms step_avg:526.66ms
step:1335/2285 train_time:703093ms step_avg:526.66ms
step:1336/2285 train_time:703625ms step_avg:526.67ms
step:1337/2285 train_time:704151ms step_avg:526.67ms
step:1338/2285 train_time:704685ms step_avg:526.67ms
step:1339/2285 train_time:705210ms step_avg:526.67ms
step:1340/2285 train_time:705742ms step_avg:526.67ms
step:1341/2285 train_time:706268ms step_avg:526.67ms
step:1342/2285 train_time:706806ms step_avg:526.68ms
step:1343/2285 train_time:707337ms step_avg:526.68ms
step:1344/2285 train_time:707868ms step_avg:526.69ms
step:1345/2285 train_time:708397ms step_avg:526.69ms
step:1346/2285 train_time:708930ms step_avg:526.69ms
step:1347/2285 train_time:709454ms step_avg:526.69ms
step:1348/2285 train_time:709987ms step_avg:526.70ms
step:1349/2285 train_time:710515ms step_avg:526.70ms
step:1350/2285 train_time:711049ms step_avg:526.70ms
step:1351/2285 train_time:711573ms step_avg:526.70ms
step:1352/2285 train_time:712106ms step_avg:526.71ms
step:1353/2285 train_time:712629ms step_avg:526.70ms
step:1354/2285 train_time:713160ms step_avg:526.71ms
step:1355/2285 train_time:713686ms step_avg:526.71ms
step:1356/2285 train_time:714220ms step_avg:526.71ms
step:1357/2285 train_time:714747ms step_avg:526.71ms
step:1358/2285 train_time:715280ms step_avg:526.72ms
step:1359/2285 train_time:715806ms step_avg:526.72ms
step:1360/2285 train_time:716341ms step_avg:526.72ms
step:1361/2285 train_time:716870ms step_avg:526.72ms
step:1362/2285 train_time:717403ms step_avg:526.73ms
step:1363/2285 train_time:717930ms step_avg:526.73ms
step:1364/2285 train_time:718464ms step_avg:526.73ms
step:1365/2285 train_time:718989ms step_avg:526.73ms
step:1366/2285 train_time:719523ms step_avg:526.74ms
step:1367/2285 train_time:720053ms step_avg:526.74ms
step:1368/2285 train_time:720585ms step_avg:526.74ms
step:1369/2285 train_time:721109ms step_avg:526.74ms
step:1370/2285 train_time:721640ms step_avg:526.74ms
step:1371/2285 train_time:722168ms step_avg:526.75ms
step:1372/2285 train_time:722705ms step_avg:526.75ms
step:1373/2285 train_time:723234ms step_avg:526.75ms
step:1374/2285 train_time:723772ms step_avg:526.76ms
step:1375/2285 train_time:724301ms step_avg:526.76ms
step:1376/2285 train_time:724834ms step_avg:526.77ms
step:1377/2285 train_time:725366ms step_avg:526.77ms
step:1378/2285 train_time:725898ms step_avg:526.78ms
step:1379/2285 train_time:726426ms step_avg:526.78ms
step:1380/2285 train_time:726960ms step_avg:526.78ms
step:1381/2285 train_time:727488ms step_avg:526.78ms
step:1382/2285 train_time:728020ms step_avg:526.79ms
step:1383/2285 train_time:728546ms step_avg:526.79ms
step:1384/2285 train_time:729076ms step_avg:526.79ms
step:1385/2285 train_time:729602ms step_avg:526.79ms
step:1386/2285 train_time:730133ms step_avg:526.79ms
step:1387/2285 train_time:730662ms step_avg:526.79ms
step:1388/2285 train_time:731196ms step_avg:526.80ms
step:1389/2285 train_time:731725ms step_avg:526.80ms
step:1390/2285 train_time:732258ms step_avg:526.80ms
step:1391/2285 train_time:732783ms step_avg:526.80ms
step:1392/2285 train_time:733317ms step_avg:526.81ms
step:1393/2285 train_time:733845ms step_avg:526.81ms
step:1394/2285 train_time:734380ms step_avg:526.82ms
step:1395/2285 train_time:734906ms step_avg:526.81ms
step:1396/2285 train_time:735440ms step_avg:526.82ms
step:1397/2285 train_time:735970ms step_avg:526.82ms
step:1398/2285 train_time:736502ms step_avg:526.83ms
step:1399/2285 train_time:737031ms step_avg:526.83ms
step:1400/2285 train_time:737563ms step_avg:526.83ms
step:1400/2285 val_loss:4.0481 train_time:737628ms step_avg:526.88ms
step:1401/2285 train_time:738091ms step_avg:526.83ms
step:1402/2285 train_time:738625ms step_avg:526.84ms
step:1403/2285 train_time:739153ms step_avg:526.84ms
step:1404/2285 train_time:739690ms step_avg:526.84ms
step:1405/2285 train_time:740221ms step_avg:526.85ms
step:1406/2285 train_time:740753ms step_avg:526.85ms
step:1407/2285 train_time:741282ms step_avg:526.85ms
step:1408/2285 train_time:741815ms step_avg:526.86ms
step:1409/2285 train_time:742344ms step_avg:526.86ms
step:1410/2285 train_time:742876ms step_avg:526.86ms
step:1411/2285 train_time:743403ms step_avg:526.86ms
step:1412/2285 train_time:743936ms step_avg:526.87ms
step:1413/2285 train_time:744462ms step_avg:526.87ms
step:1414/2285 train_time:744993ms step_avg:526.87ms
step:1415/2285 train_time:745519ms step_avg:526.87ms
step:1416/2285 train_time:746054ms step_avg:526.87ms
step:1417/2285 train_time:746582ms step_avg:526.87ms
step:1418/2285 train_time:747117ms step_avg:526.88ms
step:1419/2285 train_time:747647ms step_avg:526.88ms
step:1420/2285 train_time:748184ms step_avg:526.89ms
step:1421/2285 train_time:748711ms step_avg:526.89ms
step:1422/2285 train_time:749242ms step_avg:526.89ms
step:1423/2285 train_time:749767ms step_avg:526.89ms
step:1424/2285 train_time:750298ms step_avg:526.89ms
step:1425/2285 train_time:750827ms step_avg:526.90ms
step:1426/2285 train_time:751364ms step_avg:526.90ms
step:1427/2285 train_time:751890ms step_avg:526.90ms
step:1428/2285 train_time:752428ms step_avg:526.91ms
step:1429/2285 train_time:752957ms step_avg:526.91ms
step:1430/2285 train_time:753490ms step_avg:526.92ms
step:1431/2285 train_time:754022ms step_avg:526.92ms
step:1432/2285 train_time:754551ms step_avg:526.92ms
step:1433/2285 train_time:755080ms step_avg:526.92ms
step:1434/2285 train_time:755612ms step_avg:526.93ms
step:1435/2285 train_time:756143ms step_avg:526.93ms
step:1436/2285 train_time:756674ms step_avg:526.93ms
step:1437/2285 train_time:757203ms step_avg:526.93ms
step:1438/2285 train_time:757732ms step_avg:526.93ms
step:1439/2285 train_time:758257ms step_avg:526.93ms
step:1440/2285 train_time:758788ms step_avg:526.94ms
step:1441/2285 train_time:759318ms step_avg:526.94ms
step:1442/2285 train_time:759851ms step_avg:526.94ms
step:1443/2285 train_time:760378ms step_avg:526.94ms
step:1444/2285 train_time:760909ms step_avg:526.95ms
step:1445/2285 train_time:761438ms step_avg:526.95ms
step:1446/2285 train_time:761971ms step_avg:526.95ms
step:1447/2285 train_time:762498ms step_avg:526.95ms
step:1448/2285 train_time:763032ms step_avg:526.96ms
step:1449/2285 train_time:763560ms step_avg:526.96ms
step:1450/2285 train_time:764096ms step_avg:526.96ms
step:1451/2285 train_time:764623ms step_avg:526.96ms
step:1452/2285 train_time:765153ms step_avg:526.96ms
step:1453/2285 train_time:765682ms step_avg:526.97ms
step:1454/2285 train_time:766217ms step_avg:526.97ms
step:1455/2285 train_time:766743ms step_avg:526.97ms
step:1456/2285 train_time:767274ms step_avg:526.97ms
step:1457/2285 train_time:767800ms step_avg:526.97ms
step:1458/2285 train_time:768334ms step_avg:526.98ms
step:1459/2285 train_time:768862ms step_avg:526.98ms
step:1460/2285 train_time:769399ms step_avg:526.99ms
step:1461/2285 train_time:769928ms step_avg:526.99ms
step:1462/2285 train_time:770457ms step_avg:526.99ms
step:1463/2285 train_time:770983ms step_avg:526.99ms
step:1464/2285 train_time:771515ms step_avg:526.99ms
step:1465/2285 train_time:772044ms step_avg:526.99ms
step:1466/2285 train_time:772576ms step_avg:527.00ms
step:1467/2285 train_time:773104ms step_avg:527.00ms
step:1468/2285 train_time:773634ms step_avg:527.00ms
step:1469/2285 train_time:774165ms step_avg:527.00ms
step:1470/2285 train_time:774695ms step_avg:527.00ms
step:1471/2285 train_time:775224ms step_avg:527.00ms
step:1472/2285 train_time:775753ms step_avg:527.01ms
step:1473/2285 train_time:776285ms step_avg:527.01ms
step:1474/2285 train_time:776815ms step_avg:527.01ms
step:1475/2285 train_time:777343ms step_avg:527.01ms
step:1476/2285 train_time:777874ms step_avg:527.01ms
step:1477/2285 train_time:778404ms step_avg:527.02ms
step:1478/2285 train_time:778935ms step_avg:527.02ms
step:1479/2285 train_time:779464ms step_avg:527.02ms
step:1480/2285 train_time:779996ms step_avg:527.02ms
step:1481/2285 train_time:780525ms step_avg:527.03ms
step:1482/2285 train_time:781056ms step_avg:527.03ms
step:1483/2285 train_time:781584ms step_avg:527.03ms
step:1484/2285 train_time:782116ms step_avg:527.03ms
step:1485/2285 train_time:782646ms step_avg:527.03ms
step:1486/2285 train_time:783181ms step_avg:527.04ms
step:1487/2285 train_time:783706ms step_avg:527.04ms
step:1488/2285 train_time:784239ms step_avg:527.04ms
step:1489/2285 train_time:784769ms step_avg:527.04ms
step:1490/2285 train_time:785303ms step_avg:527.05ms
step:1491/2285 train_time:785831ms step_avg:527.05ms
step:1492/2285 train_time:786364ms step_avg:527.05ms
step:1493/2285 train_time:786889ms step_avg:527.05ms
step:1494/2285 train_time:787425ms step_avg:527.06ms
step:1495/2285 train_time:787956ms step_avg:527.06ms
step:1496/2285 train_time:788489ms step_avg:527.06ms
step:1497/2285 train_time:789016ms step_avg:527.06ms
step:1498/2285 train_time:789556ms step_avg:527.07ms
step:1499/2285 train_time:790082ms step_avg:527.07ms
step:1500/2285 train_time:790618ms step_avg:527.08ms
step:1500/2285 val_loss:4.0207 train_time:790683ms step_avg:527.12ms
step:1501/2285 train_time:791149ms step_avg:527.08ms
step:1502/2285 train_time:791686ms step_avg:527.09ms
step:1503/2285 train_time:792219ms step_avg:527.09ms
step:1504/2285 train_time:792754ms step_avg:527.10ms
step:1505/2285 train_time:793281ms step_avg:527.10ms
step:1506/2285 train_time:793817ms step_avg:527.10ms
step:1507/2285 train_time:794347ms step_avg:527.10ms
step:1508/2285 train_time:794881ms step_avg:527.11ms
step:1509/2285 train_time:795409ms step_avg:527.11ms
step:1510/2285 train_time:795945ms step_avg:527.12ms
step:1511/2285 train_time:796476ms step_avg:527.12ms
step:1512/2285 train_time:797012ms step_avg:527.12ms
step:1513/2285 train_time:797544ms step_avg:527.13ms
step:1514/2285 train_time:798082ms step_avg:527.13ms
step:1515/2285 train_time:798613ms step_avg:527.14ms
step:1516/2285 train_time:799150ms step_avg:527.14ms
step:1517/2285 train_time:799683ms step_avg:527.15ms
step:1518/2285 train_time:800219ms step_avg:527.15ms
step:1519/2285 train_time:800753ms step_avg:527.16ms
step:1520/2285 train_time:801288ms step_avg:527.16ms
step:1521/2285 train_time:801822ms step_avg:527.17ms
step:1522/2285 train_time:802355ms step_avg:527.17ms
step:1523/2285 train_time:802886ms step_avg:527.17ms
step:1524/2285 train_time:803424ms step_avg:527.18ms
step:1525/2285 train_time:803958ms step_avg:527.19ms
step:1526/2285 train_time:804493ms step_avg:527.19ms
step:1527/2285 train_time:805025ms step_avg:527.19ms
step:1528/2285 train_time:805557ms step_avg:527.20ms
step:1529/2285 train_time:806089ms step_avg:527.20ms
step:1530/2285 train_time:806624ms step_avg:527.21ms
step:1531/2285 train_time:807155ms step_avg:527.21ms
step:1532/2285 train_time:807689ms step_avg:527.21ms
step:1533/2285 train_time:808222ms step_avg:527.22ms
step:1534/2285 train_time:808756ms step_avg:527.22ms
step:1535/2285 train_time:809287ms step_avg:527.22ms
step:1536/2285 train_time:809827ms step_avg:527.23ms
step:1537/2285 train_time:810358ms step_avg:527.23ms
step:1538/2285 train_time:810893ms step_avg:527.24ms
step:1539/2285 train_time:811425ms step_avg:527.24ms
step:1540/2285 train_time:811965ms step_avg:527.25ms
step:1541/2285 train_time:812494ms step_avg:527.25ms
step:1542/2285 train_time:813032ms step_avg:527.26ms
step:1543/2285 train_time:813562ms step_avg:527.26ms
step:1544/2285 train_time:814096ms step_avg:527.26ms
step:1545/2285 train_time:814627ms step_avg:527.27ms
step:1546/2285 train_time:815163ms step_avg:527.27ms
step:1547/2285 train_time:815694ms step_avg:527.28ms
step:1548/2285 train_time:816230ms step_avg:527.28ms
step:1549/2285 train_time:816762ms step_avg:527.28ms
step:1550/2285 train_time:817295ms step_avg:527.29ms
step:1551/2285 train_time:817825ms step_avg:527.29ms
step:1552/2285 train_time:818359ms step_avg:527.29ms
step:1553/2285 train_time:818889ms step_avg:527.30ms
step:1554/2285 train_time:819428ms step_avg:527.30ms
step:1555/2285 train_time:819956ms step_avg:527.30ms
step:1556/2285 train_time:820493ms step_avg:527.31ms
step:1557/2285 train_time:821025ms step_avg:527.31ms
step:1558/2285 train_time:821560ms step_avg:527.32ms
step:1559/2285 train_time:822089ms step_avg:527.32ms
step:1560/2285 train_time:822625ms step_avg:527.32ms
step:1561/2285 train_time:823157ms step_avg:527.33ms
step:1562/2285 train_time:823690ms step_avg:527.33ms
step:1563/2285 train_time:824220ms step_avg:527.33ms
step:1564/2285 train_time:824751ms step_avg:527.33ms
step:1565/2285 train_time:825282ms step_avg:527.34ms
step:1566/2285 train_time:825819ms step_avg:527.34ms
step:1567/2285 train_time:826351ms step_avg:527.35ms
step:1568/2285 train_time:826887ms step_avg:527.35ms
step:1569/2285 train_time:827417ms step_avg:527.35ms
step:1570/2285 train_time:827955ms step_avg:527.36ms
step:1571/2285 train_time:828485ms step_avg:527.36ms
step:1572/2285 train_time:829019ms step_avg:527.37ms
step:1573/2285 train_time:829550ms step_avg:527.37ms
step:1574/2285 train_time:830087ms step_avg:527.37ms
step:1575/2285 train_time:830621ms step_avg:527.38ms
step:1576/2285 train_time:831154ms step_avg:527.38ms
step:1577/2285 train_time:831688ms step_avg:527.39ms
step:1578/2285 train_time:832225ms step_avg:527.39ms
step:1579/2285 train_time:832757ms step_avg:527.40ms
step:1580/2285 train_time:833293ms step_avg:527.40ms
step:1581/2285 train_time:833824ms step_avg:527.40ms
step:1582/2285 train_time:834357ms step_avg:527.41ms
step:1583/2285 train_time:834890ms step_avg:527.41ms
step:1584/2285 train_time:835428ms step_avg:527.42ms
step:1585/2285 train_time:835955ms step_avg:527.42ms
step:1586/2285 train_time:836492ms step_avg:527.42ms
step:1587/2285 train_time:837021ms step_avg:527.42ms
step:1588/2285 train_time:837559ms step_avg:527.43ms
step:1589/2285 train_time:838086ms step_avg:527.43ms
step:1590/2285 train_time:838624ms step_avg:527.44ms
step:1591/2285 train_time:839149ms step_avg:527.44ms
step:1592/2285 train_time:839684ms step_avg:527.44ms
step:1593/2285 train_time:840213ms step_avg:527.44ms
step:1594/2285 train_time:840747ms step_avg:527.44ms
step:1595/2285 train_time:841278ms step_avg:527.45ms
step:1596/2285 train_time:841815ms step_avg:527.45ms
step:1597/2285 train_time:842347ms step_avg:527.46ms
step:1598/2285 train_time:842880ms step_avg:527.46ms
step:1599/2285 train_time:843411ms step_avg:527.46ms
step:1600/2285 train_time:843949ms step_avg:527.47ms
step:1600/2285 val_loss:3.9874 train_time:844013ms step_avg:527.51ms
step:1601/2285 train_time:844483ms step_avg:527.47ms
step:1602/2285 train_time:845022ms step_avg:527.48ms
step:1603/2285 train_time:845551ms step_avg:527.48ms
step:1604/2285 train_time:846084ms step_avg:527.48ms
step:1605/2285 train_time:846616ms step_avg:527.49ms
step:1606/2285 train_time:847147ms step_avg:527.49ms
step:1607/2285 train_time:847675ms step_avg:527.49ms
step:1608/2285 train_time:848213ms step_avg:527.50ms
step:1609/2285 train_time:848750ms step_avg:527.50ms
step:1610/2285 train_time:849277ms step_avg:527.50ms
step:1611/2285 train_time:849811ms step_avg:527.51ms
step:1612/2285 train_time:850345ms step_avg:527.51ms
step:1613/2285 train_time:850870ms step_avg:527.51ms
step:1614/2285 train_time:851408ms step_avg:527.51ms
step:1615/2285 train_time:851940ms step_avg:527.52ms
step:1616/2285 train_time:852479ms step_avg:527.52ms
step:1617/2285 train_time:853012ms step_avg:527.53ms
step:1618/2285 train_time:853554ms step_avg:527.54ms
step:1619/2285 train_time:854082ms step_avg:527.54ms
step:1620/2285 train_time:854620ms step_avg:527.54ms
step:1621/2285 train_time:855155ms step_avg:527.55ms
step:1622/2285 train_time:855688ms step_avg:527.55ms
step:1623/2285 train_time:856217ms step_avg:527.55ms
step:1624/2285 train_time:856752ms step_avg:527.56ms
step:1625/2285 train_time:857280ms step_avg:527.56ms
step:1626/2285 train_time:857814ms step_avg:527.56ms
step:1627/2285 train_time:858347ms step_avg:527.56ms
step:1628/2285 train_time:858884ms step_avg:527.57ms
step:1629/2285 train_time:859414ms step_avg:527.57ms
step:1630/2285 train_time:859955ms step_avg:527.58ms
step:1631/2285 train_time:860485ms step_avg:527.58ms
step:1632/2285 train_time:861021ms step_avg:527.59ms
step:1633/2285 train_time:861552ms step_avg:527.59ms
step:1634/2285 train_time:862091ms step_avg:527.60ms
step:1635/2285 train_time:862625ms step_avg:527.60ms
step:1636/2285 train_time:863159ms step_avg:527.60ms
step:1637/2285 train_time:863693ms step_avg:527.61ms
step:1638/2285 train_time:864225ms step_avg:527.61ms
step:1639/2285 train_time:864762ms step_avg:527.62ms
step:1640/2285 train_time:865295ms step_avg:527.62ms
step:1641/2285 train_time:865828ms step_avg:527.62ms
step:1642/2285 train_time:866365ms step_avg:527.63ms
step:1643/2285 train_time:866895ms step_avg:527.63ms
step:1644/2285 train_time:867432ms step_avg:527.64ms
step:1645/2285 train_time:867964ms step_avg:527.64ms
step:1646/2285 train_time:868499ms step_avg:527.64ms
step:1647/2285 train_time:869030ms step_avg:527.64ms
step:1648/2285 train_time:869563ms step_avg:527.65ms
step:1649/2285 train_time:870092ms step_avg:527.65ms
step:1650/2285 train_time:870631ms step_avg:527.65ms
step:1651/2285 train_time:871164ms step_avg:527.66ms
step:1652/2285 train_time:871700ms step_avg:527.66ms
step:1653/2285 train_time:872227ms step_avg:527.66ms
step:1654/2285 train_time:872763ms step_avg:527.67ms
step:1655/2285 train_time:873291ms step_avg:527.67ms
step:1656/2285 train_time:873825ms step_avg:527.67ms
step:1657/2285 train_time:874352ms step_avg:527.67ms
step:1658/2285 train_time:874886ms step_avg:527.68ms
step:1659/2285 train_time:875417ms step_avg:527.68ms
step:1660/2285 train_time:875950ms step_avg:527.68ms
step:1661/2285 train_time:876479ms step_avg:527.68ms
step:1662/2285 train_time:877015ms step_avg:527.69ms
step:1663/2285 train_time:877541ms step_avg:527.69ms
step:1664/2285 train_time:878076ms step_avg:527.69ms
step:1665/2285 train_time:878607ms step_avg:527.69ms
step:1666/2285 train_time:879145ms step_avg:527.70ms
step:1667/2285 train_time:879675ms step_avg:527.70ms
step:1668/2285 train_time:880212ms step_avg:527.70ms
step:1669/2285 train_time:880747ms step_avg:527.71ms
step:1670/2285 train_time:881279ms step_avg:527.71ms
step:1671/2285 train_time:881811ms step_avg:527.71ms
step:1672/2285 train_time:882349ms step_avg:527.72ms
step:1673/2285 train_time:882876ms step_avg:527.72ms
step:1674/2285 train_time:883410ms step_avg:527.72ms
step:1675/2285 train_time:883938ms step_avg:527.72ms
step:1676/2285 train_time:884475ms step_avg:527.73ms
step:1677/2285 train_time:885002ms step_avg:527.73ms
step:1678/2285 train_time:885537ms step_avg:527.73ms
step:1679/2285 train_time:886069ms step_avg:527.74ms
step:1680/2285 train_time:886606ms step_avg:527.74ms
step:1681/2285 train_time:887135ms step_avg:527.74ms
step:1682/2285 train_time:887667ms step_avg:527.75ms
step:1683/2285 train_time:888198ms step_avg:527.75ms
step:1684/2285 train_time:888736ms step_avg:527.75ms
step:1685/2285 train_time:889271ms step_avg:527.76ms
step:1686/2285 train_time:889805ms step_avg:527.76ms
step:1687/2285 train_time:890336ms step_avg:527.76ms
step:1688/2285 train_time:890871ms step_avg:527.77ms
step:1689/2285 train_time:891401ms step_avg:527.77ms
step:1690/2285 train_time:891941ms step_avg:527.78ms
step:1691/2285 train_time:892474ms step_avg:527.78ms
step:1692/2285 train_time:893007ms step_avg:527.78ms
step:1693/2285 train_time:893538ms step_avg:527.78ms
step:1694/2285 train_time:894077ms step_avg:527.79ms
step:1695/2285 train_time:894606ms step_avg:527.79ms
step:1696/2285 train_time:895145ms step_avg:527.80ms
step:1697/2285 train_time:895674ms step_avg:527.80ms
step:1698/2285 train_time:896208ms step_avg:527.80ms
step:1699/2285 train_time:896736ms step_avg:527.80ms
step:1700/2285 train_time:897271ms step_avg:527.81ms
step:1700/2285 val_loss:3.9649 train_time:897336ms step_avg:527.84ms
step:1701/2285 train_time:897802ms step_avg:527.81ms
step:1702/2285 train_time:898339ms step_avg:527.81ms
step:1703/2285 train_time:898868ms step_avg:527.81ms
step:1704/2285 train_time:899405ms step_avg:527.82ms
step:1705/2285 train_time:899936ms step_avg:527.82ms
step:1706/2285 train_time:900476ms step_avg:527.83ms
step:1707/2285 train_time:901002ms step_avg:527.83ms
step:1708/2285 train_time:901539ms step_avg:527.83ms
step:1709/2285 train_time:902071ms step_avg:527.84ms
step:1710/2285 train_time:902604ms step_avg:527.84ms
step:1711/2285 train_time:903137ms step_avg:527.84ms
step:1712/2285 train_time:903669ms step_avg:527.84ms
step:1713/2285 train_time:904200ms step_avg:527.85ms
step:1714/2285 train_time:904739ms step_avg:527.85ms
step:1715/2285 train_time:905273ms step_avg:527.86ms
step:1716/2285 train_time:905811ms step_avg:527.86ms
step:1717/2285 train_time:906340ms step_avg:527.86ms
step:1718/2285 train_time:906876ms step_avg:527.87ms
step:1719/2285 train_time:907408ms step_avg:527.87ms
step:1720/2285 train_time:907946ms step_avg:527.88ms
step:1721/2285 train_time:908476ms step_avg:527.88ms
step:1722/2285 train_time:909013ms step_avg:527.88ms
step:1723/2285 train_time:909543ms step_avg:527.88ms
step:1724/2285 train_time:910082ms step_avg:527.89ms
step:1725/2285 train_time:910614ms step_avg:527.89ms
step:1726/2285 train_time:911149ms step_avg:527.90ms
step:1727/2285 train_time:911678ms step_avg:527.90ms
step:1728/2285 train_time:912217ms step_avg:527.90ms
step:1729/2285 train_time:912748ms step_avg:527.91ms
step:1730/2285 train_time:913287ms step_avg:527.91ms
step:1731/2285 train_time:913814ms step_avg:527.91ms
step:1732/2285 train_time:914354ms step_avg:527.92ms
step:1733/2285 train_time:914886ms step_avg:527.92ms
step:1734/2285 train_time:915422ms step_avg:527.93ms
step:1735/2285 train_time:915953ms step_avg:527.93ms
step:1736/2285 train_time:916493ms step_avg:527.93ms
step:1737/2285 train_time:917025ms step_avg:527.94ms
step:1738/2285 train_time:917560ms step_avg:527.94ms
step:1739/2285 train_time:918090ms step_avg:527.94ms
step:1740/2285 train_time:918625ms step_avg:527.95ms
step:1741/2285 train_time:919155ms step_avg:527.95ms
step:1742/2285 train_time:919693ms step_avg:527.95ms
step:1743/2285 train_time:920220ms step_avg:527.95ms
step:1744/2285 train_time:920758ms step_avg:527.96ms
step:1745/2285 train_time:921287ms step_avg:527.96ms
step:1746/2285 train_time:921824ms step_avg:527.96ms
step:1747/2285 train_time:922355ms step_avg:527.97ms
step:1748/2285 train_time:922888ms step_avg:527.97ms
step:1749/2285 train_time:923418ms step_avg:527.97ms
step:1750/2285 train_time:923954ms step_avg:527.97ms
step:1751/2285 train_time:924482ms step_avg:527.97ms
step:1752/2285 train_time:925014ms step_avg:527.98ms
step:1753/2285 train_time:925546ms step_avg:527.98ms
step:1754/2285 train_time:926079ms step_avg:527.98ms
step:1755/2285 train_time:926606ms step_avg:527.98ms
step:1756/2285 train_time:927142ms step_avg:527.99ms
step:1757/2285 train_time:927675ms step_avg:527.99ms
step:1758/2285 train_time:928207ms step_avg:527.99ms
step:1759/2285 train_time:928740ms step_avg:527.99ms
step:1760/2285 train_time:929275ms step_avg:528.00ms
step:1761/2285 train_time:929804ms step_avg:528.00ms
step:1762/2285 train_time:930341ms step_avg:528.00ms
step:1763/2285 train_time:930870ms step_avg:528.00ms
step:1764/2285 train_time:931408ms step_avg:528.01ms
step:1765/2285 train_time:931938ms step_avg:528.01ms
step:1766/2285 train_time:932472ms step_avg:528.01ms
step:1767/2285 train_time:933001ms step_avg:528.01ms
step:1768/2285 train_time:933534ms step_avg:528.02ms
step:1769/2285 train_time:934068ms step_avg:528.02ms
step:1770/2285 train_time:934601ms step_avg:528.02ms
step:1771/2285 train_time:935134ms step_avg:528.03ms
step:1772/2285 train_time:935669ms step_avg:528.03ms
step:1773/2285 train_time:936200ms step_avg:528.03ms
step:1774/2285 train_time:936738ms step_avg:528.04ms
step:1775/2285 train_time:937268ms step_avg:528.04ms
step:1776/2285 train_time:937803ms step_avg:528.04ms
step:1777/2285 train_time:938332ms step_avg:528.04ms
step:1778/2285 train_time:938865ms step_avg:528.05ms
step:1779/2285 train_time:939398ms step_avg:528.05ms
step:1780/2285 train_time:939933ms step_avg:528.05ms
step:1781/2285 train_time:940461ms step_avg:528.05ms
step:1782/2285 train_time:940994ms step_avg:528.06ms
step:1783/2285 train_time:941520ms step_avg:528.05ms
step:1784/2285 train_time:942054ms step_avg:528.06ms
step:1785/2285 train_time:942586ms step_avg:528.06ms
step:1786/2285 train_time:943123ms step_avg:528.06ms
step:1787/2285 train_time:943651ms step_avg:528.06ms
step:1788/2285 train_time:944189ms step_avg:528.07ms
step:1789/2285 train_time:944720ms step_avg:528.07ms
step:1790/2285 train_time:945255ms step_avg:528.08ms
step:1791/2285 train_time:945785ms step_avg:528.08ms
step:1792/2285 train_time:946321ms step_avg:528.08ms
step:1793/2285 train_time:946852ms step_avg:528.08ms
step:1794/2285 train_time:947389ms step_avg:528.09ms
step:1795/2285 train_time:947920ms step_avg:528.09ms
step:1796/2285 train_time:948454ms step_avg:528.09ms
step:1797/2285 train_time:948982ms step_avg:528.09ms
step:1798/2285 train_time:949532ms step_avg:528.10ms
step:1799/2285 train_time:950063ms step_avg:528.11ms
step:1800/2285 train_time:950593ms step_avg:528.11ms
step:1800/2285 val_loss:3.9446 train_time:950658ms step_avg:528.14ms
step:1801/2285 train_time:951168ms step_avg:528.13ms
step:1802/2285 train_time:951750ms step_avg:528.16ms
step:1803/2285 train_time:952354ms step_avg:528.21ms
step:1804/2285 train_time:952953ms step_avg:528.24ms
step:1805/2285 train_time:953529ms step_avg:528.27ms
step:1806/2285 train_time:954106ms step_avg:528.30ms
step:1807/2285 train_time:954727ms step_avg:528.35ms
step:1808/2285 train_time:955314ms step_avg:528.38ms
step:1809/2285 train_time:955885ms step_avg:528.41ms
step:1810/2285 train_time:956477ms step_avg:528.44ms
step:1811/2285 train_time:957090ms step_avg:528.49ms
step:1812/2285 train_time:957671ms step_avg:528.52ms
step:1813/2285 train_time:958259ms step_avg:528.55ms
step:1814/2285 train_time:958838ms step_avg:528.58ms
step:1815/2285 train_time:959457ms step_avg:528.63ms
step:1816/2285 train_time:960049ms step_avg:528.66ms
step:1817/2285 train_time:960626ms step_avg:528.69ms
step:1818/2285 train_time:961205ms step_avg:528.72ms
step:1819/2285 train_time:961818ms step_avg:528.76ms
step:1820/2285 train_time:962409ms step_avg:528.80ms
step:1821/2285 train_time:962984ms step_avg:528.82ms
step:1822/2285 train_time:963564ms step_avg:528.85ms
step:1823/2285 train_time:964179ms step_avg:528.90ms
step:1824/2285 train_time:964773ms step_avg:528.93ms
step:1825/2285 train_time:965344ms step_avg:528.96ms
step:1826/2285 train_time:965921ms step_avg:528.98ms
step:1827/2285 train_time:966535ms step_avg:529.03ms
step:1828/2285 train_time:967127ms step_avg:529.06ms
step:1829/2285 train_time:967703ms step_avg:529.09ms
step:1830/2285 train_time:968279ms step_avg:529.11ms
step:1831/2285 train_time:968894ms step_avg:529.16ms
step:1832/2285 train_time:969488ms step_avg:529.20ms
step:1833/2285 train_time:970059ms step_avg:529.22ms
step:1834/2285 train_time:970639ms step_avg:529.25ms
step:1835/2285 train_time:971255ms step_avg:529.29ms
step:1836/2285 train_time:971850ms step_avg:529.33ms
step:1837/2285 train_time:972422ms step_avg:529.35ms
step:1838/2285 train_time:973003ms step_avg:529.38ms
step:1839/2285 train_time:973616ms step_avg:529.43ms
step:1840/2285 train_time:974209ms step_avg:529.46ms
step:1841/2285 train_time:974783ms step_avg:529.49ms
step:1842/2285 train_time:975360ms step_avg:529.51ms
step:1843/2285 train_time:975988ms step_avg:529.56ms
step:1844/2285 train_time:976569ms step_avg:529.59ms
step:1845/2285 train_time:977100ms step_avg:529.59ms
step:1846/2285 train_time:977636ms step_avg:529.60ms
step:1847/2285 train_time:978172ms step_avg:529.60ms
step:1848/2285 train_time:978752ms step_avg:529.63ms
step:1849/2285 train_time:979335ms step_avg:529.66ms
step:1850/2285 train_time:979950ms step_avg:529.70ms
step:1851/2285 train_time:980525ms step_avg:529.73ms
step:1852/2285 train_time:981130ms step_avg:529.77ms
step:1853/2285 train_time:981702ms step_avg:529.79ms
step:1854/2285 train_time:982310ms step_avg:529.83ms
step:1855/2285 train_time:982909ms step_avg:529.87ms
step:1856/2285 train_time:983485ms step_avg:529.90ms
step:1857/2285 train_time:984073ms step_avg:529.93ms
step:1858/2285 train_time:984691ms step_avg:529.97ms
step:1859/2285 train_time:985265ms step_avg:530.00ms
step:1860/2285 train_time:985843ms step_avg:530.02ms
step:1861/2285 train_time:986460ms step_avg:530.07ms
step:1862/2285 train_time:987044ms step_avg:530.10ms
step:1863/2285 train_time:987618ms step_avg:530.12ms
step:1864/2285 train_time:988196ms step_avg:530.15ms
step:1865/2285 train_time:988820ms step_avg:530.20ms
step:1866/2285 train_time:989402ms step_avg:530.23ms
step:1867/2285 train_time:989975ms step_avg:530.25ms
step:1868/2285 train_time:990597ms step_avg:530.30ms
step:1869/2285 train_time:991181ms step_avg:530.33ms
step:1870/2285 train_time:991759ms step_avg:530.35ms
step:1871/2285 train_time:992331ms step_avg:530.37ms
step:1872/2285 train_time:992961ms step_avg:530.43ms
step:1873/2285 train_time:993533ms step_avg:530.45ms
step:1874/2285 train_time:994111ms step_avg:530.48ms
step:1875/2285 train_time:994721ms step_avg:530.52ms
step:1876/2285 train_time:995311ms step_avg:530.55ms
step:1877/2285 train_time:995887ms step_avg:530.57ms
step:1878/2285 train_time:996470ms step_avg:530.60ms
step:1879/2285 train_time:997087ms step_avg:530.65ms
step:1880/2285 train_time:997667ms step_avg:530.67ms
step:1881/2285 train_time:998243ms step_avg:530.70ms
step:1882/2285 train_time:998870ms step_avg:530.75ms
step:1883/2285 train_time:999443ms step_avg:530.77ms
step:1884/2285 train_time:1000019ms step_avg:530.80ms
step:1885/2285 train_time:1000591ms step_avg:530.82ms
step:1886/2285 train_time:1001222ms step_avg:530.87ms
step:1887/2285 train_time:1001795ms step_avg:530.89ms
step:1888/2285 train_time:1002377ms step_avg:530.92ms
step:1889/2285 train_time:1003001ms step_avg:530.97ms
step:1890/2285 train_time:1003583ms step_avg:531.00ms
step:1891/2285 train_time:1004161ms step_avg:531.02ms
step:1892/2285 train_time:1004762ms step_avg:531.06ms
step:1893/2285 train_time:1005365ms step_avg:531.10ms
step:1894/2285 train_time:1005946ms step_avg:531.12ms
step:1895/2285 train_time:1006519ms step_avg:531.14ms
step:1896/2285 train_time:1007146ms step_avg:531.20ms
step:1897/2285 train_time:1007724ms step_avg:531.22ms
step:1898/2285 train_time:1008308ms step_avg:531.25ms
step:1899/2285 train_time:1008929ms step_avg:531.29ms
step:1900/2285 train_time:1009507ms step_avg:531.32ms
step:1900/2285 val_loss:3.9242 train_time:1009572ms step_avg:531.35ms
step:1901/2285 train_time:1010084ms step_avg:531.34ms
step:1902/2285 train_time:1010664ms step_avg:531.37ms
step:1903/2285 train_time:1011263ms step_avg:531.40ms
step:1904/2285 train_time:1011867ms step_avg:531.44ms
step:1905/2285 train_time:1012439ms step_avg:531.46ms
step:1906/2285 train_time:1013043ms step_avg:531.50ms
step:1907/2285 train_time:1013641ms step_avg:531.54ms
step:1908/2285 train_time:1014220ms step_avg:531.56ms
step:1909/2285 train_time:1014824ms step_avg:531.60ms
step:1910/2285 train_time:1015378ms step_avg:531.61ms
step:1911/2285 train_time:1016003ms step_avg:531.66ms
step:1912/2285 train_time:1016578ms step_avg:531.68ms
step:1913/2285 train_time:1017153ms step_avg:531.71ms
step:1914/2285 train_time:1017779ms step_avg:531.75ms
step:1915/2285 train_time:1018354ms step_avg:531.78ms
step:1916/2285 train_time:1018938ms step_avg:531.81ms
step:1917/2285 train_time:1019562ms step_avg:531.85ms
step:1918/2285 train_time:1020139ms step_avg:531.88ms
step:1919/2285 train_time:1020713ms step_avg:531.90ms
step:1920/2285 train_time:1021343ms step_avg:531.95ms
step:1921/2285 train_time:1021922ms step_avg:531.97ms
step:1922/2285 train_time:1022524ms step_avg:532.01ms
step:1923/2285 train_time:1023126ms step_avg:532.05ms
step:1924/2285 train_time:1023705ms step_avg:532.07ms
step:1925/2285 train_time:1024313ms step_avg:532.11ms
step:1926/2285 train_time:1024909ms step_avg:532.14ms
step:1927/2285 train_time:1025480ms step_avg:532.16ms
step:1928/2285 train_time:1026105ms step_avg:532.21ms
step:1929/2285 train_time:1026686ms step_avg:532.24ms
step:1930/2285 train_time:1027264ms step_avg:532.26ms
step:1931/2285 train_time:1027888ms step_avg:532.31ms
step:1932/2285 train_time:1028462ms step_avg:532.33ms
step:1933/2285 train_time:1029036ms step_avg:532.35ms
step:1934/2285 train_time:1029662ms step_avg:532.40ms
step:1935/2285 train_time:1030240ms step_avg:532.42ms
step:1936/2285 train_time:1030816ms step_avg:532.45ms
step:1937/2285 train_time:1031433ms step_avg:532.49ms
step:1938/2285 train_time:1032020ms step_avg:532.52ms
step:1939/2285 train_time:1032594ms step_avg:532.54ms
step:1940/2285 train_time:1033219ms step_avg:532.59ms
step:1941/2285 train_time:1033797ms step_avg:532.61ms
step:1942/2285 train_time:1034372ms step_avg:532.63ms
step:1943/2285 train_time:1034992ms step_avg:532.68ms
step:1944/2285 train_time:1035573ms step_avg:532.70ms
step:1945/2285 train_time:1036150ms step_avg:532.72ms
step:1946/2285 train_time:1036775ms step_avg:532.77ms
step:1947/2285 train_time:1037347ms step_avg:532.79ms
step:1948/2285 train_time:1037929ms step_avg:532.82ms
step:1949/2285 train_time:1038551ms step_avg:532.86ms
step:1950/2285 train_time:1039131ms step_avg:532.89ms
step:1951/2285 train_time:1039706ms step_avg:532.91ms
step:1952/2285 train_time:1040336ms step_avg:532.96ms
step:1953/2285 train_time:1040910ms step_avg:532.98ms
step:1954/2285 train_time:1041490ms step_avg:533.00ms
step:1955/2285 train_time:1042113ms step_avg:533.05ms
step:1956/2285 train_time:1042690ms step_avg:533.07ms
step:1957/2285 train_time:1043265ms step_avg:533.09ms
step:1958/2285 train_time:1043893ms step_avg:533.14ms
step:1959/2285 train_time:1044468ms step_avg:533.16ms
step:1960/2285 train_time:1045047ms step_avg:533.19ms
step:1961/2285 train_time:1045670ms step_avg:533.23ms
step:1962/2285 train_time:1046250ms step_avg:533.26ms
step:1963/2285 train_time:1046857ms step_avg:533.29ms
step:1964/2285 train_time:1047457ms step_avg:533.33ms
step:1965/2285 train_time:1048030ms step_avg:533.35ms
step:1966/2285 train_time:1048658ms step_avg:533.40ms
step:1967/2285 train_time:1049232ms step_avg:533.42ms
step:1968/2285 train_time:1049811ms step_avg:533.44ms
step:1969/2285 train_time:1050438ms step_avg:533.49ms
step:1970/2285 train_time:1051015ms step_avg:533.51ms
step:1971/2285 train_time:1051590ms step_avg:533.53ms
step:1972/2285 train_time:1052217ms step_avg:533.58ms
step:1973/2285 train_time:1052791ms step_avg:533.60ms
step:1974/2285 train_time:1053371ms step_avg:533.62ms
step:1975/2285 train_time:1053996ms step_avg:533.67ms
step:1976/2285 train_time:1054575ms step_avg:533.69ms
step:1977/2285 train_time:1055157ms step_avg:533.72ms
step:1978/2285 train_time:1055782ms step_avg:533.76ms
step:1979/2285 train_time:1056355ms step_avg:533.78ms
step:1980/2285 train_time:1056960ms step_avg:533.82ms
step:1981/2285 train_time:1057560ms step_avg:533.85ms
step:1982/2285 train_time:1058137ms step_avg:533.87ms
step:1983/2285 train_time:1058689ms step_avg:533.88ms
step:1984/2285 train_time:1059271ms step_avg:533.91ms
step:1985/2285 train_time:1059890ms step_avg:533.95ms
step:1986/2285 train_time:1060467ms step_avg:533.97ms
step:1987/2285 train_time:1061052ms step_avg:534.00ms
step:1988/2285 train_time:1061668ms step_avg:534.04ms
step:1989/2285 train_time:1062246ms step_avg:534.06ms
step:1990/2285 train_time:1062850ms step_avg:534.10ms
step:1991/2285 train_time:1063448ms step_avg:534.13ms
step:1992/2285 train_time:1064028ms step_avg:534.15ms
step:1993/2285 train_time:1064648ms step_avg:534.19ms
step:1994/2285 train_time:1065229ms step_avg:534.22ms
step:1995/2285 train_time:1065803ms step_avg:534.24ms
step:1996/2285 train_time:1066428ms step_avg:534.28ms
step:1997/2285 train_time:1067000ms step_avg:534.30ms
step:1998/2285 train_time:1067581ms step_avg:534.32ms
step:1999/2285 train_time:1068204ms step_avg:534.37ms
step:2000/2285 train_time:1068783ms step_avg:534.39ms
step:2000/2285 val_loss:3.9072 train_time:1068847ms step_avg:534.42ms
step:2001/2285 train_time:1069359ms step_avg:534.41ms
step:2002/2285 train_time:1069951ms step_avg:534.44ms
step:2003/2285 train_time:1070566ms step_avg:534.48ms
step:2004/2285 train_time:1071150ms step_avg:534.51ms
step:2005/2285 train_time:1071757ms step_avg:534.54ms
step:2006/2285 train_time:1072355ms step_avg:534.57ms
step:2007/2285 train_time:1072926ms step_avg:534.59ms
step:2008/2285 train_time:1073554ms step_avg:534.64ms
step:2009/2285 train_time:1074127ms step_avg:534.66ms
step:2010/2285 train_time:1074718ms step_avg:534.69ms
step:2011/2285 train_time:1075331ms step_avg:534.72ms
step:2012/2285 train_time:1075909ms step_avg:534.75ms
step:2013/2285 train_time:1076498ms step_avg:534.77ms
step:2014/2285 train_time:1077113ms step_avg:534.81ms
step:2015/2285 train_time:1077687ms step_avg:534.83ms
step:2016/2285 train_time:1078279ms step_avg:534.86ms
step:2017/2285 train_time:1078888ms step_avg:534.90ms
step:2018/2285 train_time:1079465ms step_avg:534.92ms
step:2019/2285 train_time:1080053ms step_avg:534.94ms
step:2020/2285 train_time:1080673ms step_avg:534.99ms
step:2021/2285 train_time:1081248ms step_avg:535.01ms
step:2022/2285 train_time:1081876ms step_avg:535.05ms
step:2023/2285 train_time:1082450ms step_avg:535.07ms
step:2024/2285 train_time:1083028ms step_avg:535.09ms
step:2025/2285 train_time:1083651ms step_avg:535.14ms
step:2026/2285 train_time:1084230ms step_avg:535.16ms
step:2027/2285 train_time:1084802ms step_avg:535.18ms
step:2028/2285 train_time:1085434ms step_avg:535.22ms
step:2029/2285 train_time:1086006ms step_avg:535.24ms
step:2030/2285 train_time:1086583ms step_avg:535.26ms
step:2031/2285 train_time:1087192ms step_avg:535.30ms
step:2032/2285 train_time:1087781ms step_avg:535.33ms
step:2033/2285 train_time:1088356ms step_avg:535.34ms
step:2034/2285 train_time:1088986ms step_avg:535.39ms
step:2035/2285 train_time:1089560ms step_avg:535.41ms
step:2036/2285 train_time:1090139ms step_avg:535.43ms
step:2037/2285 train_time:1090764ms step_avg:535.48ms
step:2038/2285 train_time:1091342ms step_avg:535.50ms
step:2039/2285 train_time:1091915ms step_avg:535.51ms
step:2040/2285 train_time:1092544ms step_avg:535.56ms
step:2041/2285 train_time:1093117ms step_avg:535.58ms
step:2042/2285 train_time:1093691ms step_avg:535.60ms
step:2043/2285 train_time:1094288ms step_avg:535.63ms
step:2044/2285 train_time:1094874ms step_avg:535.65ms
step:2045/2285 train_time:1095445ms step_avg:535.67ms
step:2046/2285 train_time:1096025ms step_avg:535.69ms
step:2047/2285 train_time:1096651ms step_avg:535.74ms
step:2048/2285 train_time:1097231ms step_avg:535.76ms
step:2049/2285 train_time:1097808ms step_avg:535.78ms
step:2050/2285 train_time:1098436ms step_avg:535.82ms
step:2051/2285 train_time:1099011ms step_avg:535.84ms
step:2052/2285 train_time:1099590ms step_avg:535.86ms
step:2053/2285 train_time:1100214ms step_avg:535.91ms
step:2054/2285 train_time:1100797ms step_avg:535.93ms
step:2055/2285 train_time:1101368ms step_avg:535.95ms
step:2056/2285 train_time:1101993ms step_avg:535.99ms
step:2057/2285 train_time:1102567ms step_avg:536.01ms
step:2058/2285 train_time:1103145ms step_avg:536.03ms
step:2059/2285 train_time:1103768ms step_avg:536.07ms
step:2060/2285 train_time:1104345ms step_avg:536.09ms
step:2061/2285 train_time:1104923ms step_avg:536.11ms
step:2062/2285 train_time:1105551ms step_avg:536.15ms
step:2063/2285 train_time:1106128ms step_avg:536.17ms
step:2064/2285 train_time:1106702ms step_avg:536.19ms
step:2065/2285 train_time:1107328ms step_avg:536.24ms
step:2066/2285 train_time:1107903ms step_avg:536.26ms
step:2067/2285 train_time:1108480ms step_avg:536.27ms
step:2068/2285 train_time:1109109ms step_avg:536.32ms
step:2069/2285 train_time:1109685ms step_avg:536.34ms
step:2070/2285 train_time:1110263ms step_avg:536.36ms
step:2071/2285 train_time:1110890ms step_avg:536.40ms
step:2072/2285 train_time:1111465ms step_avg:536.42ms
step:2073/2285 train_time:1112045ms step_avg:536.44ms
step:2074/2285 train_time:1112669ms step_avg:536.48ms
step:2075/2285 train_time:1113245ms step_avg:536.50ms
step:2076/2285 train_time:1113822ms step_avg:536.52ms
step:2077/2285 train_time:1114450ms step_avg:536.57ms
step:2078/2285 train_time:1115027ms step_avg:536.59ms
step:2079/2285 train_time:1115608ms step_avg:536.61ms
step:2080/2285 train_time:1116232ms step_avg:536.65ms
step:2081/2285 train_time:1116807ms step_avg:536.67ms
step:2082/2285 train_time:1117382ms step_avg:536.69ms
step:2083/2285 train_time:1118008ms step_avg:536.73ms
step:2084/2285 train_time:1118590ms step_avg:536.75ms
step:2085/2285 train_time:1119167ms step_avg:536.77ms
step:2086/2285 train_time:1119792ms step_avg:536.81ms
step:2087/2285 train_time:1120366ms step_avg:536.83ms
step:2088/2285 train_time:1120959ms step_avg:536.86ms
step:2089/2285 train_time:1121569ms step_avg:536.89ms
step:2090/2285 train_time:1122144ms step_avg:536.91ms
step:2091/2285 train_time:1122733ms step_avg:536.94ms
step:2092/2285 train_time:1123348ms step_avg:536.97ms
step:2093/2285 train_time:1123923ms step_avg:536.99ms
step:2094/2285 train_time:1124527ms step_avg:537.02ms
step:2095/2285 train_time:1125125ms step_avg:537.05ms
step:2096/2285 train_time:1125704ms step_avg:537.07ms
step:2097/2285 train_time:1126317ms step_avg:537.11ms
step:2098/2285 train_time:1126909ms step_avg:537.13ms
step:2099/2285 train_time:1127483ms step_avg:537.15ms
step:2100/2285 train_time:1128108ms step_avg:537.19ms
step:2100/2285 val_loss:3.8902 train_time:1128173ms step_avg:537.23ms
step:2101/2285 train_time:1128683ms step_avg:537.21ms
step:2102/2285 train_time:1129261ms step_avg:537.23ms
step:2103/2285 train_time:1129836ms step_avg:537.25ms
step:2104/2285 train_time:1130457ms step_avg:537.29ms
step:2105/2285 train_time:1131039ms step_avg:537.31ms
step:2106/2285 train_time:1131620ms step_avg:537.33ms
step:2107/2285 train_time:1132192ms step_avg:537.35ms
step:2108/2285 train_time:1132812ms step_avg:537.39ms
step:2109/2285 train_time:1133540ms step_avg:537.48ms
step:2110/2285 train_time:1134309ms step_avg:537.59ms
step:2111/2285 train_time:1134996ms step_avg:537.66ms
step:2112/2285 train_time:1135741ms step_avg:537.76ms
step:2113/2285 train_time:1136407ms step_avg:537.82ms
step:2114/2285 train_time:1137147ms step_avg:537.91ms
step:2115/2285 train_time:1137831ms step_avg:537.98ms
step:2116/2285 train_time:1138579ms step_avg:538.08ms
step:2117/2285 train_time:1139255ms step_avg:538.15ms
step:2118/2285 train_time:1139984ms step_avg:538.24ms
step:2119/2285 train_time:1140697ms step_avg:538.32ms
step:2120/2285 train_time:1141391ms step_avg:538.39ms
step:2121/2285 train_time:1142117ms step_avg:538.48ms
step:2122/2285 train_time:1142821ms step_avg:538.56ms
step:2123/2285 train_time:1143546ms step_avg:538.65ms
step:2124/2285 train_time:1144275ms step_avg:538.74ms
step:2125/2285 train_time:1144986ms step_avg:538.82ms
step:2126/2285 train_time:1145709ms step_avg:538.90ms
step:2127/2285 train_time:1146402ms step_avg:538.98ms
step:2128/2285 train_time:1147121ms step_avg:539.06ms
step:2129/2285 train_time:1147816ms step_avg:539.13ms
step:2130/2285 train_time:1148540ms step_avg:539.22ms
step:2131/2285 train_time:1149252ms step_avg:539.30ms
step:2132/2285 train_time:1149956ms step_avg:539.38ms
step:2133/2285 train_time:1150667ms step_avg:539.46ms
step:2134/2285 train_time:1151373ms step_avg:539.54ms
step:2135/2285 train_time:1152104ms step_avg:539.63ms
step:2136/2285 train_time:1152786ms step_avg:539.69ms
step:2137/2285 train_time:1153512ms step_avg:539.78ms
step:2138/2285 train_time:1154193ms step_avg:539.85ms
step:2139/2285 train_time:1154916ms step_avg:539.93ms
step:2140/2285 train_time:1155596ms step_avg:540.00ms
step:2141/2285 train_time:1156324ms step_avg:540.09ms
step:2142/2285 train_time:1157009ms step_avg:540.15ms
step:2143/2285 train_time:1157730ms step_avg:540.24ms
step:2144/2285 train_time:1158419ms step_avg:540.31ms
step:2145/2285 train_time:1159149ms step_avg:540.40ms
step:2146/2285 train_time:1159831ms step_avg:540.46ms
step:2147/2285 train_time:1160559ms step_avg:540.55ms
step:2148/2285 train_time:1161273ms step_avg:540.63ms
step:2149/2285 train_time:1161974ms step_avg:540.70ms
step:2150/2285 train_time:1162694ms step_avg:540.79ms
step:2151/2285 train_time:1163408ms step_avg:540.87ms
step:2152/2285 train_time:1164107ms step_avg:540.94ms
step:2153/2285 train_time:1164839ms step_avg:541.03ms
step:2154/2285 train_time:1165524ms step_avg:541.10ms
step:2155/2285 train_time:1166272ms step_avg:541.19ms
step:2156/2285 train_time:1166947ms step_avg:541.26ms
step:2157/2285 train_time:1167705ms step_avg:541.36ms
step:2158/2285 train_time:1168394ms step_avg:541.42ms
step:2159/2285 train_time:1169123ms step_avg:541.51ms
step:2160/2285 train_time:1169821ms step_avg:541.58ms
step:2161/2285 train_time:1170536ms step_avg:541.66ms
step:2162/2285 train_time:1171268ms step_avg:541.75ms
step:2163/2285 train_time:1171954ms step_avg:541.82ms
step:2164/2285 train_time:1172724ms step_avg:541.92ms
step:2165/2285 train_time:1173368ms step_avg:541.97ms
step:2166/2285 train_time:1174138ms step_avg:542.08ms
step:2167/2285 train_time:1174786ms step_avg:542.13ms
step:2168/2285 train_time:1175546ms step_avg:542.23ms
step:2169/2285 train_time:1176187ms step_avg:542.27ms
step:2170/2285 train_time:1176955ms step_avg:542.38ms
step:2171/2285 train_time:1177597ms step_avg:542.42ms
step:2172/2285 train_time:1178296ms step_avg:542.49ms
step:2173/2285 train_time:1179013ms step_avg:542.57ms
step:2174/2285 train_time:1179727ms step_avg:542.65ms
step:2175/2285 train_time:1180424ms step_avg:542.72ms
step:2176/2285 train_time:1181155ms step_avg:542.81ms
step:2177/2285 train_time:1181833ms step_avg:542.87ms
step:2178/2285 train_time:1182553ms step_avg:542.95ms
step:2179/2285 train_time:1183238ms step_avg:543.02ms
step:2180/2285 train_time:1183968ms step_avg:543.10ms
step:2181/2285 train_time:1184647ms step_avg:543.17ms
step:2182/2285 train_time:1185410ms step_avg:543.27ms
step:2183/2285 train_time:1186077ms step_avg:543.32ms
step:2184/2285 train_time:1186758ms step_avg:543.39ms
step:2185/2285 train_time:1187484ms step_avg:543.47ms
step:2186/2285 train_time:1188185ms step_avg:543.54ms
step:2187/2285 train_time:1188845ms step_avg:543.60ms
step:2188/2285 train_time:1189593ms step_avg:543.69ms
step:2189/2285 train_time:1190260ms step_avg:543.75ms
step:2190/2285 train_time:1191001ms step_avg:543.84ms
step:2191/2285 train_time:1191680ms step_avg:543.90ms
step:2192/2285 train_time:1192409ms step_avg:543.98ms
step:2193/2285 train_time:1193113ms step_avg:544.05ms
step:2194/2285 train_time:1193830ms step_avg:544.13ms
step:2195/2285 train_time:1194572ms step_avg:544.22ms
step:2196/2285 train_time:1195238ms step_avg:544.28ms
step:2197/2285 train_time:1195979ms step_avg:544.37ms
step:2198/2285 train_time:1196706ms step_avg:544.45ms
step:2199/2285 train_time:1197398ms step_avg:544.52ms
step:2200/2285 train_time:1198178ms step_avg:544.63ms
step:2200/2285 val_loss:3.8766 train_time:1198247ms step_avg:544.66ms
step:2201/2285 train_time:1198942ms step_avg:544.73ms
step:2202/2285 train_time:1199825ms step_avg:544.88ms
step:2203/2285 train_time:1200644ms step_avg:545.00ms
step:2204/2285 train_time:1201494ms step_avg:545.14ms
step:2205/2285 train_time:1202327ms step_avg:545.27ms
step:2206/2285 train_time:1203234ms step_avg:545.44ms
step:2207/2285 train_time:1204107ms step_avg:545.59ms
step:2208/2285 train_time:1204911ms step_avg:545.70ms
step:2209/2285 train_time:1205772ms step_avg:545.85ms
step:2210/2285 train_time:1206683ms step_avg:546.01ms
step:2211/2285 train_time:1207470ms step_avg:546.12ms
step:2212/2285 train_time:1208307ms step_avg:546.25ms
step:2213/2285 train_time:1209205ms step_avg:546.41ms
step:2214/2285 train_time:1210012ms step_avg:546.53ms
step:2215/2285 train_time:1210838ms step_avg:546.65ms
step:2216/2285 train_time:1211691ms step_avg:546.79ms
step:2217/2285 train_time:1212533ms step_avg:546.93ms
step:2218/2285 train_time:1213419ms step_avg:547.08ms
step:2219/2285 train_time:1214280ms step_avg:547.22ms
step:2220/2285 train_time:1215034ms step_avg:547.31ms
step:2221/2285 train_time:1215868ms step_avg:547.44ms
step:2222/2285 train_time:1216722ms step_avg:547.58ms
step:2223/2285 train_time:1217560ms step_avg:547.71ms
step:2224/2285 train_time:1218383ms step_avg:547.83ms
step:2225/2285 train_time:1219255ms step_avg:547.98ms
step:2226/2285 train_time:1220157ms step_avg:548.14ms
step:2227/2285 train_time:1220992ms step_avg:548.27ms
step:2228/2285 train_time:1221833ms step_avg:548.40ms
step:2229/2285 train_time:1222669ms step_avg:548.53ms
step:2230/2285 train_time:1223575ms step_avg:548.69ms
step:2231/2285 train_time:1224412ms step_avg:548.82ms
step:2232/2285 train_time:1225291ms step_avg:548.97ms
step:2233/2285 train_time:1226139ms step_avg:549.10ms
step:2234/2285 train_time:1226920ms step_avg:549.20ms
step:2235/2285 train_time:1227776ms step_avg:549.34ms
step:2236/2285 train_time:1228552ms step_avg:549.44ms
step:2237/2285 train_time:1229410ms step_avg:549.58ms
step:2238/2285 train_time:1230216ms step_avg:549.69ms
step:2239/2285 train_time:1231006ms step_avg:549.80ms
step:2240/2285 train_time:1231896ms step_avg:549.95ms
step:2241/2285 train_time:1232770ms step_avg:550.10ms
step:2242/2285 train_time:1233624ms step_avg:550.23ms
step:2243/2285 train_time:1234508ms step_avg:550.38ms
step:2244/2285 train_time:1235327ms step_avg:550.50ms
step:2245/2285 train_time:1236158ms step_avg:550.63ms
step:2246/2285 train_time:1236914ms step_avg:550.72ms
step:2247/2285 train_time:1237750ms step_avg:550.85ms
step:2248/2285 train_time:1238622ms step_avg:550.99ms
step:2249/2285 train_time:1239498ms step_avg:551.13ms
step:2250/2285 train_time:1240333ms step_avg:551.26ms
step:2251/2285 train_time:1241178ms step_avg:551.39ms
step:2252/2285 train_time:1242060ms step_avg:551.54ms
step:2253/2285 train_time:1242868ms step_avg:551.65ms
step:2254/2285 train_time:1243721ms step_avg:551.78ms
step:2255/2285 train_time:1244627ms step_avg:551.94ms
step:2256/2285 train_time:1245476ms step_avg:552.07ms
step:2257/2285 train_time:1246317ms step_avg:552.20ms
step:2258/2285 train_time:1247167ms step_avg:552.33ms
step:2259/2285 train_time:1248002ms step_avg:552.46ms
step:2260/2285 train_time:1248852ms step_avg:552.59ms
step:2261/2285 train_time:1249735ms step_avg:552.74ms
step:2262/2285 train_time:1250558ms step_avg:552.86ms
step:2263/2285 train_time:1251372ms step_avg:552.97ms
step:2264/2285 train_time:1252213ms step_avg:553.10ms
step:2265/2285 train_time:1252941ms step_avg:553.17ms
step:2266/2285 train_time:1253900ms step_avg:553.35ms
step:2267/2285 train_time:1254743ms step_avg:553.48ms
step:2268/2285 train_time:1255670ms step_avg:553.65ms
step:2269/2285 train_time:1256600ms step_avg:553.81ms
step:2270/2285 train_time:1257420ms step_avg:553.93ms
step:2271/2285 train_time:1258250ms step_avg:554.05ms
step:2272/2285 train_time:1259028ms step_avg:554.15ms
step:2273/2285 train_time:1260019ms step_avg:554.34ms
step:2274/2285 train_time:1260889ms step_avg:554.48ms
step:2275/2285 train_time:1261799ms step_avg:554.64ms
step:2276/2285 train_time:1262764ms step_avg:554.82ms
step:2277/2285 train_time:1263589ms step_avg:554.94ms
step:2278/2285 train_time:1264474ms step_avg:555.08ms
step:2279/2285 train_time:1265351ms step_avg:555.22ms
step:2280/2285 train_time:1266246ms step_avg:555.37ms
step:2281/2285 train_time:1267144ms step_avg:555.52ms
step:2282/2285 train_time:1267957ms step_avg:555.63ms
step:2283/2285 train_time:1268822ms step_avg:555.77ms
step:2284/2285 train_time:1269679ms step_avg:555.90ms
step:2285/2285 train_time:1270531ms step_avg:556.03ms
step:2285/2285 val_loss:3.8690 train_time:1270595ms step_avg:556.06ms
peak memory allocated: 29533 MiB reserved: 56044 MiB
