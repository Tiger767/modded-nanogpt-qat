import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# Travis' Quantization Code

# Weights
# 2 bit
ENABLE_MLP_QUANTIZATION = True
ENABLE_ATTN_QUANTIZATION = True

# Activations
# MLP
# 8 bit
ENABLE_MLP_ACT_QUANTIZATION = False
ENABLE_MLP_PRE_ACT_QUANTIZATION = False
# 1 bit instead of 8 bit
ENABLE_EXTREME_MLP_ACT_QUANTIZATION = False
ENABLE_EXTREME_PRE_MLP_ACT_QUANTIZATION = False
# ATTN
ENABLE_ATTN_ACT_QUANTIZATION = True
ENABLE_ATTN_PRE_ACT_QUANTIZATION = False
# 1 bit instead of 8 bit
ENABLE_EXTREME_ATTN_ACT_QUANTIZATION = True
ENABLE_EXTREME_PRE_ATTN_ACT_QUANTIZATION = False


class round_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input):
        ctx.input = input
        return torch.round(input)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()
        return grad_input


class noisy_round_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input):
        ctx.input = input
        return torch.round(input + torch.rand_like(input) - .5)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()
        return grad_input


class round_grad_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input):
        ctx.input = input
        return torch.round(input)

    @staticmethod
    def backward(ctx, grad_output):
        l = 1
        k = 5
        #x = l * (1 + torch.sign(grad_output - l) * torch.abs(grad_output - l)**(1/k))
        x = ctx.input
        x = x - torch.round(x - l / 2)
        grad = 1 / k * torch.abs(x - l / 2)**(1/k - 1)
        grad = grad.clamp(min=-3, max=3)
        return grad * grad_output


class clamp_pt(torch.autograd.function.InplaceFunction):
    @staticmethod
    def forward(ctx, input, min_val=-1.0, max_val=1.0):
        ctx.input = input
        return torch.clamp(input, min_val, max_val)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output.clone()
        return grad_input, None, None


# def ternary_quantize(
#     x: torch.Tensor,
# ):
#     min_max_value = x.abs().mean().clamp(min=1e-5)
#     # clamp_pt?
#     quantized = clamp_pt.apply(round_pt.apply(x / min_max_value))
#     return min_max_value * quantized


# def ternary_quantize(
#     x: torch.Tensor,
# ):
#     min_max_value = x.abs().mean().clamp(min=1e-5)
#     quantized = clamp_pt.apply(noisy_round_pt.apply(x / min_max_value))
#     return min_max_value * quantized


def ternary_quantize_mean_special(
    x: torch.Tensor,
):
    min_max_value = x.abs().mean(dim=-1, keepdim=True).mean(dim=-2, keepdim=True).clamp(min=1e-5)
    quantized = clamp_pt.apply(round_grad_pt.apply(x / min_max_value))
    return min_max_value * quantized


def ternary_quantize(
    x: torch.Tensor,
):
    min_max_value = x.abs().mean().clamp(min=1e-5)
    quantized = clamp_pt.apply(round_grad_pt.apply(x / min_max_value))
    return min_max_value * quantized


# def quantized_sigmoid(x):
#     x = torch.nn.functional.sigmoid(x)
#     return noisy_round_pt.apply(x)


def quantized_sigmoid(x):
    x = torch.nn.functional.sigmoid(x)
    return round_grad_pt.apply(x)


def quantized_tanh(x):
    x = torch.nn.functional.tanh(x)
    return round_grad_pt.apply(x)


def quantized_relu2(
    x: torch.Tensor,
    num_bits=8,
    eps=1e-5,
):
    # 1. Apply the ReLU-squared activation.
    # The result is guaranteed to be non-negative (>= 0).
    x = torch.nn.functional.relu(x)
    x = x.square()

    # 2. Find the maximum value along the inner dimension (last dim).
    # We clamp at eps to prevent division by zero if a row is all zeros.
    xmax = x.max(-1, keepdim=True).values.clamp(min=eps)

    # 3. Determine the number of quantization steps.
    # As requested: "num steps should be 2**num_bits - 1"
    num_steps = (2**num_bits - 1)

    # 4. Quantize the values.
    # The range is [0, xmax].
    # We scale x to [0, num_steps], round it, then scale it back.
    
    # Scale to [0, num_steps]
    scaled_x = x / xmax * num_steps
    
    # Round to the nearest integer step (using your custom grad function)
    rounded_x = round_grad_pt.apply(scaled_x)
    
    # Scale back to the original magnitude range [0, xmax]
    quantized_x = rounded_x / num_steps * xmax

    return quantized_x


def quantized_linear(
    x: torch.Tensor,
    num_bits=8,
    eps=1e-5,
):
    """
    Applies symmetric linear quantization to the input tensor x.
    Quantizes positive and negative values separately based on the
    per-channel (last dim) min/max.
    """
    
    # Determine the number of quantization steps for each side (pos/neg)
    # (2**num_bits - 1) total steps. We split them around zero.
    num_steps = (2**num_bits - 1) // 2

    # Create a mask for positive (>= 0) values
    pos = x >= 0

    # Find the per-channel min/max values
    # Clamp at eps to prevent division by zero
    xnmin = x.min(-1, keepdim=True).values.clamp(max=-eps)
    xpmax = x.max(-1, keepdim=True).values.clamp(min=eps)

    # Quantize the positive part:
    # Scale to [0, num_steps], round, and scale back to [0, xpmax]
    xp = round_grad_pt.apply(x / xpmax * num_steps) / num_steps * xpmax

    # Quantize the negative part:
    # Scale to [0, num_steps] (since x/xnmin is positive), round, 
    # and scale back to [xnmin, 0]
    xn = round_grad_pt.apply(x / xnmin * num_steps) / num_steps * xnmin

    # Combine the quantized positive and negative parts
    x = xp * pos + xn * (~pos)

    return x


def quantized_gelu(
    x: torch.Tensor,
    num_bits=8,
    eps=1e-5,
):
    x = torch.nn.functional.gelu(x)

    num_steps = (2**num_bits - 1) // 2

    pos = x >= 0
    xnmin = x.min(-1, keepdim=True).values.clamp(max=-eps)

    xpmax = x.max(-1, keepdim=True).values.clamp(min=eps)

    xp = round_pt.apply(x / xpmax * num_steps) / num_steps * xpmax
    xn = round_pt.apply(x / xnmin * num_steps) / num_steps * xnmin
    x = xp * pos + xn * (~pos)

    return x


class Quantize(nn.Module):
    def __init__(
        self,
        quantize_func,
        out_scalars=False,
        shape=None,
        device=None,
        dtype=None,
        *args,
        **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)
        self.quantize_func = quantize_func

        self._post_quantized_mode = False
        self._max_value = 1.0 #nn.Parameter(torch.tensor(1.0))

        self.out_scalars = None
        if out_scalars:
            self.out_scalars = nn.Parameter(
                torch.sign(
                    torch.randn((shape[0], 1, *shape[2:]), device=device, dtype=dtype)
                )
            )

    def forward(self, x):
        if self.quantize_func is None:
            return x
        if self._post_quantized_mode:
            return x * self._max_value
        if self.out_scalars is not None:
            return self.out_scalars * self.quantize_func(x)
        return self.quantize_func(x)

    def quantize(self, x, round=False, to_scalar=True, dtype=None):
        self._post_quantized_mode = True
        if self.quantize_func is None:
            return

        qx = self.quantize_func(x.data)
        self._max_value.data = torch.max(torch.abs(qx))
        qx = qx / self._max_value.data
        qx = qx.round() if round else qx
        qx = qx.to(dtype) if dtype is not None else qx
        if to_scalar:
            nu = torch.unique(qx.flatten())
            if len(nu) == 1:
                qx = nu[:1]
        x.data = qx


class QuantizedLinear(nn.Linear):
    def __init__(
        self, *args, weight_quantize_func=None, bias_quantize_func=None, **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)
        self.quantize_weight = Quantize(weight_quantize_func)
        self.quantize_bias = Quantize(bias_quantize_func)

    def forward(self, input):
        qw = self.quantize_weight(self.weight)
        qb = self.quantize_bias(self.bias)
        return F.linear(input, qw, qb)

    def quantize(self, dtype=None):
        self.quantize_weight.quantize(self.weight, dtype=dtype)
        self.quantize_bias.quantize(self.bias, dtype=dtype)


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # quantization
        self.quantize_qkvo_w = Quantize(ternary_quantize_mean_special) if ENABLE_ATTN_QUANTIZATION else lambda x: x
        self.quantize_qkvo_w2 = Quantize(ternary_quantize) if ENABLE_ATTN_QUANTIZATION else lambda x: x
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        if ENABLE_ATTN_PRE_ACT_QUANTIZATION:
            if ENABLE_EXTREME_PRE_ATTN_ACT_QUANTIZATION:
                x = quantized_tanh(x)
            else:
                # TODO: use linear not relu2?
                x = quantized_linear(x, num_bits=8)

        q, k, v = F.linear(x, self.quantize_qkvo_w(self.qkvo_w.view(4, self.hdim, self.dim)[:3]).flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        if ENABLE_ATTN_ACT_QUANTIZATION:
            if ENABLE_EXTREME_ATTN_ACT_QUANTIZATION:
                q = quantized_tanh(q)
                k = quantized_tanh(k)
                #v = quantized_tanh(v, num_bits=8)  # can skip
            else:
                # TODO: use linear not relu2?
                q = quantized_linear(q, num_bits=8)
                k = quantized_linear(k, num_bits=8)
                #v = quantized_linear(v, num_bits=8)  # can skip

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, (self.qkvo_w.view(4, self.hdim, self.dim)[3]).type_as(y)) # self.quantize_qkvo_w2
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # quantization
        self.quantize_c_fc = Quantize(ternary_quantize) if ENABLE_MLP_QUANTIZATION else lambda x: x
        self.quantize_c_proj = Quantize(ternary_quantize) if ENABLE_MLP_QUANTIZATION else lambda x: x
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        if ENABLE_MLP_PRE_ACT_QUANTIZATION:
            if ENABLE_EXTREME_PRE_MLP_ACT_QUANTIZATION:
                x = quantized_tanh(x)
            else:
                x = quantized_linear(x, num_bits=8)
        x = F.linear(x, self.quantize_c_fc(self.c_fc).T.type_as(x))
        if ENABLE_MLP_ACT_QUANTIZATION:
            if ENABLE_EXTREME_MLP_ACT_QUANTIZATION:
                x = quantized_sigmoid(x)
            else:
                x = quantized_relu2(x, num_bits=8)
        else:
            x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.quantize_c_fc(self.c_proj).type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 100 # 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = True
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
Running PyTorch 2.9.0+cu128 compiled for CUDA 12.8
Running Triton version 3.5.0
Sat Nov  8 17:37:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:06:00.0 Off |                    0 |
| N/A   50C    P0             81W /  350W |    1103MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           71588      C   /usr/bin/python3                       1094MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/2285 train_time:711ms step_avg:710.66ms
step:2/2285 train_time:1293ms step_avg:646.69ms
step:3/2285 train_time:1974ms step_avg:657.92ms
step:4/2285 train_time:2704ms step_avg:676.02ms
step:5/2285 train_time:3441ms step_avg:688.26ms
step:6/2285 train_time:4184ms step_avg:697.28ms
step:7/2285 train_time:4917ms step_avg:702.38ms
step:8/2285 train_time:5656ms step_avg:707.02ms
step:9/2285 train_time:6396ms step_avg:710.65ms
step:10/2285 train_time:7143ms step_avg:714.34ms
step:11/2285 train_time:7886ms step_avg:716.94ms
step:12/2285 train_time:8635ms step_avg:719.56ms
step:13/2285 train_time:9378ms step_avg:721.36ms
step:14/2285 train_time:10126ms step_avg:723.27ms
step:15/2285 train_time:10867ms step_avg:724.45ms
step:16/2285 train_time:11615ms step_avg:725.94ms
step:17/2285 train_time:12359ms step_avg:727.01ms
step:18/2285 train_time:13106ms step_avg:728.09ms
step:19/2285 train_time:13848ms step_avg:728.83ms
step:20/2285 train_time:14589ms step_avg:729.45ms
step:21/2285 train_time:15327ms step_avg:729.88ms
step:22/2285 train_time:16069ms step_avg:730.41ms
step:23/2285 train_time:16810ms step_avg:730.89ms
step:24/2285 train_time:17556ms step_avg:731.50ms
step:25/2285 train_time:18293ms step_avg:731.73ms
step:26/2285 train_time:19036ms step_avg:732.17ms
step:27/2285 train_time:19774ms step_avg:732.37ms
step:28/2285 train_time:20516ms step_avg:732.72ms
step:29/2285 train_time:21257ms step_avg:732.99ms
step:30/2285 train_time:22001ms step_avg:733.35ms
step:31/2285 train_time:22740ms step_avg:733.55ms
step:32/2285 train_time:23483ms step_avg:733.84ms
step:33/2285 train_time:24221ms step_avg:733.98ms
step:34/2285 train_time:24965ms step_avg:734.26ms
step:35/2285 train_time:25704ms step_avg:734.41ms
step:36/2285 train_time:26448ms step_avg:734.68ms
step:37/2285 train_time:27189ms step_avg:734.84ms
step:38/2285 train_time:27934ms step_avg:735.11ms
step:39/2285 train_time:28673ms step_avg:735.21ms
step:40/2285 train_time:29418ms step_avg:735.45ms
step:41/2285 train_time:30158ms step_avg:735.57ms
step:42/2285 train_time:30903ms step_avg:735.79ms
step:43/2285 train_time:31643ms step_avg:735.88ms
step:44/2285 train_time:32388ms step_avg:736.08ms
step:45/2285 train_time:33130ms step_avg:736.22ms
step:46/2285 train_time:33873ms step_avg:736.38ms
step:47/2285 train_time:34614ms step_avg:736.47ms
step:48/2285 train_time:35359ms step_avg:736.64ms
step:49/2285 train_time:36099ms step_avg:736.71ms
step:50/2285 train_time:36844ms step_avg:736.88ms
step:51/2285 train_time:37584ms step_avg:736.94ms
step:52/2285 train_time:38330ms step_avg:737.11ms
step:53/2285 train_time:39072ms step_avg:737.21ms
step:54/2285 train_time:39818ms step_avg:737.36ms
step:55/2285 train_time:40559ms step_avg:737.43ms
step:56/2285 train_time:41306ms step_avg:737.60ms
step:57/2285 train_time:42048ms step_avg:737.69ms
step:58/2285 train_time:42793ms step_avg:737.82ms
step:59/2285 train_time:43535ms step_avg:737.88ms
step:60/2285 train_time:44282ms step_avg:738.03ms
step:61/2285 train_time:45024ms step_avg:738.10ms
step:62/2285 train_time:45771ms step_avg:738.25ms
step:63/2285 train_time:46516ms step_avg:738.35ms
step:64/2285 train_time:47261ms step_avg:738.45ms
step:65/2285 train_time:48003ms step_avg:738.51ms
step:66/2285 train_time:48751ms step_avg:738.65ms
step:67/2285 train_time:49494ms step_avg:738.71ms
step:68/2285 train_time:50243ms step_avg:738.87ms
step:69/2285 train_time:50986ms step_avg:738.92ms
step:70/2285 train_time:51733ms step_avg:739.04ms
step:71/2285 train_time:52476ms step_avg:739.11ms
step:72/2285 train_time:53223ms step_avg:739.21ms
step:73/2285 train_time:53965ms step_avg:739.25ms
step:74/2285 train_time:54713ms step_avg:739.37ms
step:75/2285 train_time:55457ms step_avg:739.43ms
step:76/2285 train_time:56204ms step_avg:739.53ms
step:77/2285 train_time:56947ms step_avg:739.57ms
step:78/2285 train_time:57694ms step_avg:739.67ms
step:79/2285 train_time:58438ms step_avg:739.72ms
step:80/2285 train_time:59185ms step_avg:739.81ms
step:81/2285 train_time:59928ms step_avg:739.86ms
step:82/2285 train_time:60676ms step_avg:739.95ms
step:83/2285 train_time:61419ms step_avg:739.99ms
step:84/2285 train_time:62167ms step_avg:740.08ms
step:85/2285 train_time:62911ms step_avg:740.13ms
step:86/2285 train_time:63659ms step_avg:740.22ms
step:87/2285 train_time:64401ms step_avg:740.24ms
step:88/2285 train_time:65150ms step_avg:740.34ms
step:89/2285 train_time:65894ms step_avg:740.38ms
step:90/2285 train_time:66648ms step_avg:740.53ms
step:91/2285 train_time:67386ms step_avg:740.50ms
step:92/2285 train_time:68136ms step_avg:740.61ms
step:93/2285 train_time:68878ms step_avg:740.63ms
step:94/2285 train_time:69625ms step_avg:740.70ms
step:95/2285 train_time:70370ms step_avg:740.74ms
step:96/2285 train_time:71119ms step_avg:740.82ms
step:97/2285 train_time:71862ms step_avg:740.84ms
step:98/2285 train_time:72614ms step_avg:740.96ms
step:99/2285 train_time:73358ms step_avg:740.99ms
step:100/2285 train_time:74107ms step_avg:741.07ms
step:100/2285 val_loss:5.0776 train_time:74204ms step_avg:742.04ms
step:101/2285 train_time:74853ms step_avg:741.12ms
step:102/2285 train_time:75603ms step_avg:741.20ms
step:103/2285 train_time:76347ms step_avg:741.23ms
step:104/2285 train_time:77095ms step_avg:741.30ms
step:105/2285 train_time:77839ms step_avg:741.32ms
step:106/2285 train_time:78587ms step_avg:741.39ms
step:107/2285 train_time:79331ms step_avg:741.42ms
step:108/2285 train_time:80080ms step_avg:741.48ms
step:109/2285 train_time:80825ms step_avg:741.51ms
step:110/2285 train_time:81575ms step_avg:741.59ms
step:111/2285 train_time:82319ms step_avg:741.61ms
step:112/2285 train_time:83068ms step_avg:741.68ms
step:113/2285 train_time:83813ms step_avg:741.71ms
step:114/2285 train_time:84560ms step_avg:741.76ms
step:115/2285 train_time:85304ms step_avg:741.78ms
step:116/2285 train_time:86053ms step_avg:741.84ms
step:117/2285 train_time:86797ms step_avg:741.86ms
step:118/2285 train_time:87547ms step_avg:741.93ms
step:119/2285 train_time:88292ms step_avg:741.95ms
step:120/2285 train_time:89041ms step_avg:742.01ms
step:121/2285 train_time:89784ms step_avg:742.02ms
step:122/2285 train_time:90536ms step_avg:742.10ms
step:123/2285 train_time:91279ms step_avg:742.10ms
step:124/2285 train_time:92028ms step_avg:742.16ms
step:125/2285 train_time:92771ms step_avg:742.17ms
step:126/2285 train_time:93520ms step_avg:742.22ms
step:127/2285 train_time:94264ms step_avg:742.24ms
step:128/2285 train_time:95015ms step_avg:742.30ms
step:129/2285 train_time:95757ms step_avg:742.30ms
step:130/2285 train_time:96507ms step_avg:742.36ms
step:131/2285 train_time:97252ms step_avg:742.38ms
step:132/2285 train_time:98000ms step_avg:742.42ms
step:133/2285 train_time:98743ms step_avg:742.43ms
step:134/2285 train_time:99490ms step_avg:742.46ms
step:135/2285 train_time:100234ms step_avg:742.47ms
step:136/2285 train_time:100983ms step_avg:742.52ms
step:137/2285 train_time:101728ms step_avg:742.54ms
step:138/2285 train_time:102477ms step_avg:742.59ms
step:139/2285 train_time:103219ms step_avg:742.58ms
step:140/2285 train_time:103968ms step_avg:742.63ms
step:141/2285 train_time:104711ms step_avg:742.63ms
step:142/2285 train_time:105460ms step_avg:742.67ms
step:143/2285 train_time:106205ms step_avg:742.70ms
step:144/2285 train_time:106955ms step_avg:742.74ms
step:145/2285 train_time:107697ms step_avg:742.74ms
step:146/2285 train_time:108447ms step_avg:742.79ms
step:147/2285 train_time:109189ms step_avg:742.78ms
step:148/2285 train_time:109937ms step_avg:742.82ms
step:149/2285 train_time:110679ms step_avg:742.81ms
step:150/2285 train_time:111427ms step_avg:742.85ms
step:151/2285 train_time:112170ms step_avg:742.85ms
step:152/2285 train_time:112919ms step_avg:742.89ms
step:153/2285 train_time:113663ms step_avg:742.89ms
step:154/2285 train_time:114410ms step_avg:742.92ms
step:155/2285 train_time:115153ms step_avg:742.92ms
step:156/2285 train_time:115903ms step_avg:742.97ms
step:157/2285 train_time:116647ms step_avg:742.97ms
step:158/2285 train_time:117396ms step_avg:743.01ms
step:159/2285 train_time:118139ms step_avg:743.01ms
step:160/2285 train_time:118884ms step_avg:743.02ms
step:161/2285 train_time:119630ms step_avg:743.04ms
step:162/2285 train_time:120378ms step_avg:743.08ms
step:163/2285 train_time:121119ms step_avg:743.06ms
step:164/2285 train_time:121866ms step_avg:743.09ms
step:165/2285 train_time:122611ms step_avg:743.10ms
step:166/2285 train_time:123359ms step_avg:743.12ms
step:167/2285 train_time:124102ms step_avg:743.13ms
step:168/2285 train_time:124849ms step_avg:743.15ms
step:169/2285 train_time:125590ms step_avg:743.14ms
step:170/2285 train_time:126337ms step_avg:743.16ms
step:171/2285 train_time:127079ms step_avg:743.15ms
step:172/2285 train_time:127828ms step_avg:743.18ms
step:173/2285 train_time:128570ms step_avg:743.18ms
step:174/2285 train_time:129316ms step_avg:743.20ms
step:175/2285 train_time:130058ms step_avg:743.19ms
step:176/2285 train_time:130806ms step_avg:743.21ms
step:177/2285 train_time:131548ms step_avg:743.21ms
step:178/2285 train_time:132295ms step_avg:743.23ms
step:179/2285 train_time:133037ms step_avg:743.22ms
step:180/2285 train_time:133785ms step_avg:743.25ms
step:181/2285 train_time:134528ms step_avg:743.25ms
step:182/2285 train_time:135276ms step_avg:743.27ms
step:183/2285 train_time:136018ms step_avg:743.27ms
step:184/2285 train_time:136763ms step_avg:743.28ms
step:185/2285 train_time:137506ms step_avg:743.28ms
step:186/2285 train_time:138254ms step_avg:743.30ms
step:187/2285 train_time:138995ms step_avg:743.29ms
step:188/2285 train_time:139744ms step_avg:743.32ms
step:189/2285 train_time:140485ms step_avg:743.31ms
step:190/2285 train_time:141234ms step_avg:743.33ms
step:191/2285 train_time:141974ms step_avg:743.32ms
step:192/2285 train_time:142724ms step_avg:743.36ms
step:193/2285 train_time:143466ms step_avg:743.35ms
step:194/2285 train_time:144214ms step_avg:743.37ms
step:195/2285 train_time:144955ms step_avg:743.36ms
step:196/2285 train_time:145702ms step_avg:743.38ms
step:197/2285 train_time:146445ms step_avg:743.37ms
step:198/2285 train_time:147190ms step_avg:743.38ms
step:199/2285 train_time:147931ms step_avg:743.37ms
step:200/2285 train_time:148678ms step_avg:743.39ms
step:200/2285 val_loss:4.5447 train_time:148775ms step_avg:743.87ms
step:201/2285 train_time:149418ms step_avg:743.37ms
step:202/2285 train_time:150166ms step_avg:743.40ms
step:203/2285 train_time:150906ms step_avg:743.38ms
step:204/2285 train_time:151653ms step_avg:743.40ms
step:205/2285 train_time:152394ms step_avg:743.38ms
step:206/2285 train_time:153143ms step_avg:743.41ms
step:207/2285 train_time:153883ms step_avg:743.40ms
step:208/2285 train_time:154631ms step_avg:743.42ms
step:209/2285 train_time:155375ms step_avg:743.42ms
step:210/2285 train_time:156120ms step_avg:743.43ms
step:211/2285 train_time:156864ms step_avg:743.43ms
step:212/2285 train_time:157610ms step_avg:743.44ms
step:213/2285 train_time:158351ms step_avg:743.43ms
step:214/2285 train_time:159100ms step_avg:743.46ms
step:215/2285 train_time:159843ms step_avg:743.46ms
step:216/2285 train_time:160590ms step_avg:743.47ms
step:217/2285 train_time:161330ms step_avg:743.46ms
step:218/2285 train_time:162078ms step_avg:743.48ms
step:219/2285 train_time:162820ms step_avg:743.47ms
step:220/2285 train_time:163567ms step_avg:743.49ms
step:221/2285 train_time:164309ms step_avg:743.48ms
step:222/2285 train_time:165057ms step_avg:743.50ms
step:223/2285 train_time:165799ms step_avg:743.49ms
step:224/2285 train_time:166547ms step_avg:743.51ms
step:225/2285 train_time:167287ms step_avg:743.50ms
step:226/2285 train_time:168034ms step_avg:743.52ms
step:227/2285 train_time:168776ms step_avg:743.51ms
step:228/2285 train_time:169522ms step_avg:743.52ms
step:229/2285 train_time:170263ms step_avg:743.51ms
step:230/2285 train_time:171009ms step_avg:743.52ms
step:231/2285 train_time:171750ms step_avg:743.51ms
step:232/2285 train_time:172496ms step_avg:743.52ms
step:233/2285 train_time:173239ms step_avg:743.51ms
step:234/2285 train_time:173987ms step_avg:743.53ms
step:235/2285 train_time:174728ms step_avg:743.53ms
step:236/2285 train_time:175475ms step_avg:743.54ms
step:237/2285 train_time:176218ms step_avg:743.54ms
step:238/2285 train_time:176965ms step_avg:743.55ms
step:239/2285 train_time:177706ms step_avg:743.54ms
step:240/2285 train_time:178453ms step_avg:743.55ms
step:241/2285 train_time:179194ms step_avg:743.54ms
step:242/2285 train_time:179939ms step_avg:743.55ms
step:243/2285 train_time:180682ms step_avg:743.55ms
step:244/2285 train_time:181429ms step_avg:743.56ms
step:245/2285 train_time:182170ms step_avg:743.55ms
step:246/2285 train_time:182918ms step_avg:743.57ms
step:247/2285 train_time:183660ms step_avg:743.56ms
step:248/2285 train_time:184407ms step_avg:743.58ms
step:249/2285 train_time:185149ms step_avg:743.57ms
step:250/2285 train_time:185895ms step_avg:743.58ms
step:251/2285 train_time:186639ms step_avg:743.58ms
step:252/2285 train_time:187384ms step_avg:743.59ms
step:253/2285 train_time:188127ms step_avg:743.58ms
step:254/2285 train_time:188873ms step_avg:743.60ms
step:255/2285 train_time:189615ms step_avg:743.59ms
step:256/2285 train_time:190363ms step_avg:743.60ms
step:257/2285 train_time:191104ms step_avg:743.60ms
step:258/2285 train_time:191851ms step_avg:743.61ms
step:259/2285 train_time:192593ms step_avg:743.60ms
step:260/2285 train_time:193343ms step_avg:743.63ms
step:261/2285 train_time:194083ms step_avg:743.61ms
step:262/2285 train_time:194830ms step_avg:743.63ms
step:263/2285 train_time:195573ms step_avg:743.62ms
step:264/2285 train_time:196322ms step_avg:743.65ms
step:265/2285 train_time:197064ms step_avg:743.64ms
step:266/2285 train_time:197810ms step_avg:743.65ms
step:267/2285 train_time:198552ms step_avg:743.64ms
step:268/2285 train_time:199300ms step_avg:743.66ms
step:269/2285 train_time:200042ms step_avg:743.65ms
step:270/2285 train_time:200789ms step_avg:743.66ms
step:271/2285 train_time:201531ms step_avg:743.66ms
step:272/2285 train_time:202279ms step_avg:743.67ms
step:273/2285 train_time:203024ms step_avg:743.68ms
step:274/2285 train_time:203772ms step_avg:743.69ms
step:275/2285 train_time:204512ms step_avg:743.68ms
step:276/2285 train_time:205260ms step_avg:743.69ms
step:277/2285 train_time:206003ms step_avg:743.69ms
step:278/2285 train_time:206749ms step_avg:743.70ms
step:279/2285 train_time:207490ms step_avg:743.69ms
step:280/2285 train_time:208239ms step_avg:743.71ms
step:281/2285 train_time:208983ms step_avg:743.71ms
step:282/2285 train_time:209731ms step_avg:743.73ms
step:283/2285 train_time:210474ms step_avg:743.72ms
step:284/2285 train_time:211221ms step_avg:743.73ms
step:285/2285 train_time:211964ms step_avg:743.73ms
step:286/2285 train_time:212711ms step_avg:743.75ms
step:287/2285 train_time:213454ms step_avg:743.74ms
step:288/2285 train_time:214199ms step_avg:743.75ms
step:289/2285 train_time:214942ms step_avg:743.74ms
step:290/2285 train_time:215688ms step_avg:743.75ms
step:291/2285 train_time:216430ms step_avg:743.75ms
step:292/2285 train_time:217176ms step_avg:743.75ms
step:293/2285 train_time:217919ms step_avg:743.75ms
step:294/2285 train_time:218665ms step_avg:743.76ms
step:295/2285 train_time:219405ms step_avg:743.75ms
step:296/2285 train_time:220152ms step_avg:743.76ms
step:297/2285 train_time:220894ms step_avg:743.75ms
step:298/2285 train_time:221640ms step_avg:743.76ms
step:299/2285 train_time:222382ms step_avg:743.75ms
step:300/2285 train_time:223128ms step_avg:743.76ms
step:300/2285 val_loss:4.3029 train_time:223225ms step_avg:744.08ms
step:301/2285 train_time:223868ms step_avg:743.75ms
step:302/2285 train_time:224618ms step_avg:743.77ms
step:303/2285 train_time:225361ms step_avg:743.77ms
step:304/2285 train_time:226106ms step_avg:743.77ms
step:305/2285 train_time:226848ms step_avg:743.76ms
step:306/2285 train_time:227594ms step_avg:743.77ms
step:307/2285 train_time:228338ms step_avg:743.77ms
step:308/2285 train_time:229083ms step_avg:743.78ms
step:309/2285 train_time:229826ms step_avg:743.77ms
step:310/2285 train_time:230570ms step_avg:743.78ms
step:311/2285 train_time:231313ms step_avg:743.77ms
step:312/2285 train_time:232061ms step_avg:743.79ms
step:313/2285 train_time:232802ms step_avg:743.78ms
step:314/2285 train_time:233550ms step_avg:743.79ms
step:315/2285 train_time:234293ms step_avg:743.79ms
step:316/2285 train_time:235039ms step_avg:743.79ms
step:317/2285 train_time:235780ms step_avg:743.78ms
step:318/2285 train_time:236526ms step_avg:743.79ms
step:319/2285 train_time:237268ms step_avg:743.79ms
step:320/2285 train_time:238014ms step_avg:743.79ms
step:321/2285 train_time:238755ms step_avg:743.79ms
step:322/2285 train_time:239502ms step_avg:743.80ms
step:323/2285 train_time:240242ms step_avg:743.78ms
step:324/2285 train_time:240988ms step_avg:743.79ms
step:325/2285 train_time:241731ms step_avg:743.79ms
step:326/2285 train_time:242479ms step_avg:743.80ms
step:327/2285 train_time:243220ms step_avg:743.79ms
step:328/2285 train_time:243968ms step_avg:743.81ms
step:329/2285 train_time:244710ms step_avg:743.80ms
step:330/2285 train_time:245456ms step_avg:743.81ms
step:331/2285 train_time:246197ms step_avg:743.80ms
step:332/2285 train_time:246944ms step_avg:743.81ms
step:333/2285 train_time:247686ms step_avg:743.80ms
step:334/2285 train_time:248432ms step_avg:743.81ms
step:335/2285 train_time:249174ms step_avg:743.80ms
step:336/2285 train_time:249922ms step_avg:743.82ms
step:337/2285 train_time:250664ms step_avg:743.81ms
step:338/2285 train_time:251411ms step_avg:743.82ms
step:339/2285 train_time:252154ms step_avg:743.82ms
step:340/2285 train_time:252901ms step_avg:743.83ms
step:341/2285 train_time:253642ms step_avg:743.82ms
step:342/2285 train_time:254389ms step_avg:743.83ms
step:343/2285 train_time:255130ms step_avg:743.82ms
step:344/2285 train_time:255876ms step_avg:743.82ms
step:345/2285 train_time:256618ms step_avg:743.82ms
step:346/2285 train_time:257363ms step_avg:743.82ms
step:347/2285 train_time:258104ms step_avg:743.82ms
step:348/2285 train_time:258851ms step_avg:743.82ms
step:349/2285 train_time:259593ms step_avg:743.82ms
step:350/2285 train_time:260340ms step_avg:743.83ms
step:351/2285 train_time:261082ms step_avg:743.82ms
step:352/2285 train_time:261829ms step_avg:743.83ms
step:353/2285 train_time:262571ms step_avg:743.83ms
step:354/2285 train_time:263318ms step_avg:743.84ms
step:355/2285 train_time:264061ms step_avg:743.83ms
step:356/2285 train_time:264807ms step_avg:743.84ms
step:357/2285 train_time:265550ms step_avg:743.84ms
step:358/2285 train_time:266297ms step_avg:743.85ms
step:359/2285 train_time:267039ms step_avg:743.84ms
step:360/2285 train_time:267785ms step_avg:743.85ms
step:361/2285 train_time:268526ms step_avg:743.84ms
step:362/2285 train_time:269273ms step_avg:743.85ms
step:363/2285 train_time:270014ms step_avg:743.84ms
step:364/2285 train_time:270761ms step_avg:743.85ms
step:365/2285 train_time:271502ms step_avg:743.84ms
step:366/2285 train_time:272250ms step_avg:743.85ms
step:367/2285 train_time:272993ms step_avg:743.85ms
step:368/2285 train_time:273740ms step_avg:743.86ms
step:369/2285 train_time:274483ms step_avg:743.86ms
step:370/2285 train_time:275230ms step_avg:743.86ms
step:371/2285 train_time:275972ms step_avg:743.86ms
step:372/2285 train_time:276718ms step_avg:743.86ms
step:373/2285 train_time:277460ms step_avg:743.86ms
step:374/2285 train_time:278206ms step_avg:743.87ms
step:375/2285 train_time:278949ms step_avg:743.86ms
step:376/2285 train_time:279696ms step_avg:743.87ms
step:377/2285 train_time:280440ms step_avg:743.87ms
step:378/2285 train_time:281186ms step_avg:743.88ms
step:379/2285 train_time:281930ms step_avg:743.88ms
step:380/2285 train_time:282677ms step_avg:743.89ms
step:381/2285 train_time:283418ms step_avg:743.88ms
step:382/2285 train_time:284165ms step_avg:743.89ms
step:383/2285 train_time:284911ms step_avg:743.89ms
step:384/2285 train_time:285658ms step_avg:743.90ms
step:385/2285 train_time:286396ms step_avg:743.89ms
step:386/2285 train_time:287143ms step_avg:743.89ms
step:387/2285 train_time:287885ms step_avg:743.89ms
step:388/2285 train_time:288632ms step_avg:743.90ms
step:389/2285 train_time:289374ms step_avg:743.89ms
step:390/2285 train_time:290121ms step_avg:743.90ms
step:391/2285 train_time:290863ms step_avg:743.89ms
step:392/2285 train_time:291610ms step_avg:743.90ms
step:393/2285 train_time:292352ms step_avg:743.90ms
step:394/2285 train_time:293099ms step_avg:743.91ms
step:395/2285 train_time:293839ms step_avg:743.90ms
step:396/2285 train_time:294587ms step_avg:743.91ms
step:397/2285 train_time:295329ms step_avg:743.90ms
step:398/2285 train_time:296075ms step_avg:743.91ms
step:399/2285 train_time:296815ms step_avg:743.90ms
step:400/2285 train_time:297560ms step_avg:743.90ms
step:400/2285 val_loss:4.1387 train_time:297657ms step_avg:744.14ms
step:401/2285 train_time:298301ms step_avg:743.89ms
step:402/2285 train_time:299050ms step_avg:743.91ms
step:403/2285 train_time:299789ms step_avg:743.89ms
step:404/2285 train_time:300534ms step_avg:743.90ms
step:405/2285 train_time:301278ms step_avg:743.90ms
step:406/2285 train_time:302025ms step_avg:743.90ms
step:407/2285 train_time:302766ms step_avg:743.90ms
step:408/2285 train_time:303513ms step_avg:743.90ms
step:409/2285 train_time:304254ms step_avg:743.90ms
step:410/2285 train_time:305000ms step_avg:743.90ms
step:411/2285 train_time:305740ms step_avg:743.89ms
step:412/2285 train_time:306484ms step_avg:743.89ms
step:413/2285 train_time:307225ms step_avg:743.89ms
step:414/2285 train_time:307973ms step_avg:743.89ms
step:415/2285 train_time:308713ms step_avg:743.89ms
step:416/2285 train_time:309460ms step_avg:743.89ms
step:417/2285 train_time:310199ms step_avg:743.88ms
step:418/2285 train_time:310945ms step_avg:743.89ms
step:419/2285 train_time:311685ms step_avg:743.88ms
step:420/2285 train_time:312431ms step_avg:743.88ms
step:421/2285 train_time:313172ms step_avg:743.88ms
step:422/2285 train_time:313918ms step_avg:743.88ms
step:423/2285 train_time:314659ms step_avg:743.87ms
step:424/2285 train_time:315404ms step_avg:743.88ms
step:425/2285 train_time:316145ms step_avg:743.87ms
step:426/2285 train_time:316891ms step_avg:743.88ms
step:427/2285 train_time:317632ms step_avg:743.87ms
step:428/2285 train_time:318378ms step_avg:743.87ms
step:429/2285 train_time:319119ms step_avg:743.87ms
step:430/2285 train_time:319864ms step_avg:743.87ms
step:431/2285 train_time:320604ms step_avg:743.86ms
step:432/2285 train_time:321350ms step_avg:743.87ms
step:433/2285 train_time:322088ms step_avg:743.85ms
step:434/2285 train_time:322834ms step_avg:743.86ms
step:435/2285 train_time:323575ms step_avg:743.85ms
step:436/2285 train_time:324321ms step_avg:743.85ms
step:437/2285 train_time:325061ms step_avg:743.85ms
step:438/2285 train_time:325805ms step_avg:743.85ms
step:439/2285 train_time:326546ms step_avg:743.84ms
step:440/2285 train_time:327292ms step_avg:743.84ms
step:441/2285 train_time:328031ms step_avg:743.83ms
step:442/2285 train_time:328776ms step_avg:743.84ms
step:443/2285 train_time:329517ms step_avg:743.83ms
step:444/2285 train_time:330264ms step_avg:743.84ms
step:445/2285 train_time:331003ms step_avg:743.83ms
step:446/2285 train_time:331749ms step_avg:743.83ms
step:447/2285 train_time:332489ms step_avg:743.82ms
step:448/2285 train_time:333235ms step_avg:743.83ms
step:449/2285 train_time:333977ms step_avg:743.82ms
step:450/2285 train_time:334723ms step_avg:743.83ms
step:451/2285 train_time:335461ms step_avg:743.82ms
step:452/2285 train_time:336206ms step_avg:743.82ms
step:453/2285 train_time:336947ms step_avg:743.81ms
step:454/2285 train_time:337693ms step_avg:743.82ms
step:455/2285 train_time:338432ms step_avg:743.81ms
step:456/2285 train_time:339180ms step_avg:743.81ms
step:457/2285 train_time:339920ms step_avg:743.81ms
step:458/2285 train_time:340665ms step_avg:743.81ms
step:459/2285 train_time:341404ms step_avg:743.80ms
step:460/2285 train_time:342150ms step_avg:743.80ms
step:461/2285 train_time:342890ms step_avg:743.80ms
step:462/2285 train_time:343635ms step_avg:743.80ms
step:463/2285 train_time:344377ms step_avg:743.79ms
step:464/2285 train_time:345122ms step_avg:743.80ms
step:465/2285 train_time:345861ms step_avg:743.79ms
step:466/2285 train_time:346606ms step_avg:743.79ms
step:467/2285 train_time:347347ms step_avg:743.78ms
step:468/2285 train_time:348093ms step_avg:743.79ms
step:469/2285 train_time:348833ms step_avg:743.78ms
step:470/2285 train_time:349579ms step_avg:743.79ms
step:471/2285 train_time:350318ms step_avg:743.77ms
step:472/2285 train_time:351063ms step_avg:743.78ms
step:473/2285 train_time:351803ms step_avg:743.77ms
step:474/2285 train_time:352550ms step_avg:743.78ms
step:475/2285 train_time:353289ms step_avg:743.77ms
step:476/2285 train_time:354035ms step_avg:743.77ms
step:477/2285 train_time:354776ms step_avg:743.76ms
step:478/2285 train_time:355520ms step_avg:743.77ms
step:479/2285 train_time:356261ms step_avg:743.76ms
step:480/2285 train_time:357008ms step_avg:743.77ms
step:481/2285 train_time:357747ms step_avg:743.76ms
step:482/2285 train_time:358494ms step_avg:743.76ms
step:483/2285 train_time:359235ms step_avg:743.76ms
step:484/2285 train_time:359980ms step_avg:743.76ms
step:485/2285 train_time:360719ms step_avg:743.75ms
step:486/2285 train_time:361465ms step_avg:743.76ms
step:487/2285 train_time:362206ms step_avg:743.75ms
step:488/2285 train_time:362951ms step_avg:743.75ms
step:489/2285 train_time:363693ms step_avg:743.75ms
step:490/2285 train_time:364438ms step_avg:743.75ms
step:491/2285 train_time:365178ms step_avg:743.74ms
step:492/2285 train_time:365925ms step_avg:743.75ms
step:493/2285 train_time:366666ms step_avg:743.75ms
step:494/2285 train_time:367413ms step_avg:743.75ms
step:495/2285 train_time:368153ms step_avg:743.74ms
step:496/2285 train_time:368897ms step_avg:743.74ms
step:497/2285 train_time:369638ms step_avg:743.74ms
step:498/2285 train_time:370384ms step_avg:743.74ms
step:499/2285 train_time:371125ms step_avg:743.74ms
step:500/2285 train_time:371870ms step_avg:743.74ms
step:500/2285 val_loss:4.0452 train_time:371968ms step_avg:743.94ms
step:501/2285 train_time:372609ms step_avg:743.73ms
step:502/2285 train_time:373354ms step_avg:743.73ms
step:503/2285 train_time:374097ms step_avg:743.73ms
step:504/2285 train_time:374842ms step_avg:743.73ms
step:505/2285 train_time:375581ms step_avg:743.72ms
step:506/2285 train_time:376327ms step_avg:743.73ms
step:507/2285 train_time:377068ms step_avg:743.72ms
step:508/2285 train_time:377816ms step_avg:743.73ms
step:509/2285 train_time:378557ms step_avg:743.73ms
step:510/2285 train_time:379302ms step_avg:743.73ms
step:511/2285 train_time:380042ms step_avg:743.72ms
step:512/2285 train_time:380786ms step_avg:743.72ms
step:513/2285 train_time:381527ms step_avg:743.72ms
step:514/2285 train_time:382274ms step_avg:743.72ms
step:515/2285 train_time:383016ms step_avg:743.72ms
step:516/2285 train_time:383761ms step_avg:743.72ms
step:517/2285 train_time:384501ms step_avg:743.72ms
step:518/2285 train_time:385248ms step_avg:743.72ms
step:519/2285 train_time:385989ms step_avg:743.72ms
step:520/2285 train_time:386735ms step_avg:743.72ms
step:521/2285 train_time:387476ms step_avg:743.72ms
step:522/2285 train_time:388222ms step_avg:743.72ms
step:523/2285 train_time:388963ms step_avg:743.71ms
step:524/2285 train_time:389708ms step_avg:743.72ms
step:525/2285 train_time:390447ms step_avg:743.71ms
step:526/2285 train_time:391195ms step_avg:743.72ms
step:527/2285 train_time:391937ms step_avg:743.71ms
step:528/2285 train_time:392683ms step_avg:743.72ms
step:529/2285 train_time:393423ms step_avg:743.71ms
step:530/2285 train_time:394168ms step_avg:743.71ms
step:531/2285 train_time:394910ms step_avg:743.71ms
step:532/2285 train_time:395657ms step_avg:743.72ms
step:533/2285 train_time:396398ms step_avg:743.71ms
step:534/2285 train_time:397144ms step_avg:743.72ms
step:535/2285 train_time:397885ms step_avg:743.71ms
step:536/2285 train_time:398631ms step_avg:743.71ms
step:537/2285 train_time:399371ms step_avg:743.71ms
step:538/2285 train_time:400116ms step_avg:743.71ms
step:539/2285 train_time:400857ms step_avg:743.71ms
step:540/2285 train_time:401602ms step_avg:743.71ms
step:541/2285 train_time:402343ms step_avg:743.70ms
step:542/2285 train_time:403088ms step_avg:743.70ms
step:543/2285 train_time:403829ms step_avg:743.70ms
step:544/2285 train_time:404573ms step_avg:743.70ms
step:545/2285 train_time:405315ms step_avg:743.70ms
step:546/2285 train_time:406059ms step_avg:743.70ms
step:547/2285 train_time:406800ms step_avg:743.69ms
step:548/2285 train_time:407545ms step_avg:743.70ms
step:549/2285 train_time:408286ms step_avg:743.69ms
step:550/2285 train_time:409032ms step_avg:743.69ms
step:551/2285 train_time:409772ms step_avg:743.69ms
step:552/2285 train_time:410518ms step_avg:743.69ms
step:553/2285 train_time:411259ms step_avg:743.69ms
step:554/2285 train_time:412004ms step_avg:743.69ms
step:555/2285 train_time:412746ms step_avg:743.69ms
step:556/2285 train_time:413491ms step_avg:743.69ms
step:557/2285 train_time:414232ms step_avg:743.68ms
step:558/2285 train_time:414978ms step_avg:743.69ms
step:559/2285 train_time:415721ms step_avg:743.69ms
step:560/2285 train_time:416466ms step_avg:743.69ms
step:561/2285 train_time:417206ms step_avg:743.68ms
step:562/2285 train_time:417951ms step_avg:743.69ms
step:563/2285 train_time:418691ms step_avg:743.68ms
step:564/2285 train_time:419438ms step_avg:743.68ms
step:565/2285 train_time:420179ms step_avg:743.68ms
step:566/2285 train_time:420923ms step_avg:743.68ms
step:567/2285 train_time:421663ms step_avg:743.67ms
step:568/2285 train_time:422409ms step_avg:743.68ms
step:569/2285 train_time:423148ms step_avg:743.67ms
step:570/2285 train_time:423894ms step_avg:743.67ms
step:571/2285 train_time:424634ms step_avg:743.67ms
step:572/2285 train_time:425379ms step_avg:743.67ms
step:573/2285 train_time:426120ms step_avg:743.66ms
step:574/2285 train_time:426865ms step_avg:743.67ms
step:575/2285 train_time:427606ms step_avg:743.66ms
step:576/2285 train_time:428352ms step_avg:743.67ms
step:577/2285 train_time:429095ms step_avg:743.66ms
step:578/2285 train_time:429841ms step_avg:743.67ms
step:579/2285 train_time:430579ms step_avg:743.66ms
step:580/2285 train_time:431325ms step_avg:743.66ms
step:581/2285 train_time:432065ms step_avg:743.66ms
step:582/2285 train_time:432810ms step_avg:743.66ms
step:583/2285 train_time:433551ms step_avg:743.66ms
step:584/2285 train_time:434297ms step_avg:743.66ms
step:585/2285 train_time:435040ms step_avg:743.66ms
step:586/2285 train_time:435785ms step_avg:743.66ms
step:587/2285 train_time:436525ms step_avg:743.65ms
step:588/2285 train_time:437270ms step_avg:743.66ms
step:589/2285 train_time:438010ms step_avg:743.65ms
step:590/2285 train_time:438755ms step_avg:743.65ms
step:591/2285 train_time:439498ms step_avg:743.65ms
step:592/2285 train_time:440243ms step_avg:743.65ms
step:593/2285 train_time:440983ms step_avg:743.65ms
step:594/2285 train_time:441728ms step_avg:743.65ms
step:595/2285 train_time:442468ms step_avg:743.64ms
step:596/2285 train_time:443215ms step_avg:743.65ms
step:597/2285 train_time:443955ms step_avg:743.64ms
step:598/2285 train_time:444701ms step_avg:743.65ms
step:599/2285 train_time:445445ms step_avg:743.65ms
step:600/2285 train_time:446190ms step_avg:743.65ms
step:600/2285 val_loss:3.9638 train_time:446287ms step_avg:743.81ms
step:601/2285 train_time:446928ms step_avg:743.64ms
step:602/2285 train_time:447675ms step_avg:743.65ms
step:603/2285 train_time:448416ms step_avg:743.64ms
step:604/2285 train_time:449163ms step_avg:743.65ms
step:605/2285 train_time:449904ms step_avg:743.64ms
step:606/2285 train_time:450649ms step_avg:743.65ms
step:607/2285 train_time:451393ms step_avg:743.65ms
step:608/2285 train_time:452137ms step_avg:743.65ms
step:609/2285 train_time:452877ms step_avg:743.64ms
step:610/2285 train_time:453622ms step_avg:743.64ms
step:611/2285 train_time:454363ms step_avg:743.64ms
step:612/2285 train_time:455109ms step_avg:743.64ms
step:613/2285 train_time:455850ms step_avg:743.64ms
step:614/2285 train_time:456597ms step_avg:743.64ms
step:615/2285 train_time:457338ms step_avg:743.64ms
step:616/2285 train_time:458085ms step_avg:743.64ms
step:617/2285 train_time:458823ms step_avg:743.64ms
step:618/2285 train_time:459573ms step_avg:743.65ms
step:619/2285 train_time:460315ms step_avg:743.64ms
step:620/2285 train_time:461060ms step_avg:743.65ms
step:621/2285 train_time:461799ms step_avg:743.64ms
step:622/2285 train_time:462545ms step_avg:743.64ms
step:623/2285 train_time:463286ms step_avg:743.64ms
step:624/2285 train_time:464032ms step_avg:743.64ms
step:625/2285 train_time:464774ms step_avg:743.64ms
step:626/2285 train_time:465519ms step_avg:743.64ms
step:627/2285 train_time:466259ms step_avg:743.64ms
step:628/2285 train_time:467004ms step_avg:743.64ms
step:629/2285 train_time:467745ms step_avg:743.63ms
step:630/2285 train_time:468493ms step_avg:743.64ms
step:631/2285 train_time:469237ms step_avg:743.64ms
step:632/2285 train_time:469981ms step_avg:743.64ms
step:633/2285 train_time:470720ms step_avg:743.63ms
step:634/2285 train_time:471465ms step_avg:743.64ms
step:635/2285 train_time:472205ms step_avg:743.63ms
step:636/2285 train_time:472951ms step_avg:743.63ms
step:637/2285 train_time:473692ms step_avg:743.63ms
step:638/2285 train_time:474438ms step_avg:743.63ms
step:639/2285 train_time:475737ms step_avg:744.50ms
step:640/2285 train_time:476472ms step_avg:744.49ms
step:641/2285 train_time:477220ms step_avg:744.49ms
step:642/2285 train_time:477970ms step_avg:744.50ms
step:643/2285 train_time:478709ms step_avg:744.49ms
step:644/2285 train_time:479455ms step_avg:744.49ms
step:645/2285 train_time:480199ms step_avg:744.49ms
step:646/2285 train_time:480946ms step_avg:744.50ms
step:647/2285 train_time:481687ms step_avg:744.49ms
step:648/2285 train_time:482434ms step_avg:744.50ms
step:649/2285 train_time:483176ms step_avg:744.49ms
step:650/2285 train_time:483922ms step_avg:744.50ms
step:651/2285 train_time:484662ms step_avg:744.49ms
step:652/2285 train_time:485409ms step_avg:744.49ms
step:653/2285 train_time:486149ms step_avg:744.48ms
step:654/2285 train_time:486895ms step_avg:744.49ms
step:655/2285 train_time:487637ms step_avg:744.48ms
step:656/2285 train_time:488384ms step_avg:744.49ms
step:657/2285 train_time:489124ms step_avg:744.48ms
step:658/2285 train_time:489870ms step_avg:744.48ms
step:659/2285 train_time:490613ms step_avg:744.48ms
step:660/2285 train_time:491359ms step_avg:744.48ms
step:661/2285 train_time:492101ms step_avg:744.48ms
step:662/2285 train_time:492847ms step_avg:744.48ms
step:663/2285 train_time:493591ms step_avg:744.48ms
step:664/2285 train_time:494338ms step_avg:744.48ms
step:665/2285 train_time:495078ms step_avg:744.48ms
step:666/2285 train_time:495824ms step_avg:744.48ms
step:667/2285 train_time:496566ms step_avg:744.48ms
step:668/2285 train_time:497313ms step_avg:744.48ms
step:669/2285 train_time:498056ms step_avg:744.48ms
step:670/2285 train_time:498802ms step_avg:744.48ms
step:671/2285 train_time:499544ms step_avg:744.48ms
step:672/2285 train_time:500290ms step_avg:744.48ms
step:673/2285 train_time:501033ms step_avg:744.48ms
step:674/2285 train_time:501778ms step_avg:744.48ms
step:675/2285 train_time:502519ms step_avg:744.47ms
step:676/2285 train_time:503264ms step_avg:744.47ms
step:677/2285 train_time:504005ms step_avg:744.47ms
step:678/2285 train_time:504751ms step_avg:744.47ms
step:679/2285 train_time:505492ms step_avg:744.47ms
step:680/2285 train_time:506237ms step_avg:744.47ms
step:681/2285 train_time:506978ms step_avg:744.46ms
step:682/2285 train_time:507724ms step_avg:744.46ms
step:683/2285 train_time:508465ms step_avg:744.46ms
step:684/2285 train_time:509212ms step_avg:744.46ms
step:685/2285 train_time:509954ms step_avg:744.46ms
step:686/2285 train_time:510701ms step_avg:744.46ms
step:687/2285 train_time:511439ms step_avg:744.45ms
step:688/2285 train_time:512186ms step_avg:744.46ms
step:689/2285 train_time:512926ms step_avg:744.45ms
step:690/2285 train_time:513674ms step_avg:744.46ms
step:691/2285 train_time:514416ms step_avg:744.45ms
step:692/2285 train_time:515162ms step_avg:744.45ms
step:693/2285 train_time:515902ms step_avg:744.45ms
step:694/2285 train_time:516650ms step_avg:744.45ms
step:695/2285 train_time:517390ms step_avg:744.45ms
step:696/2285 train_time:518139ms step_avg:744.45ms
step:697/2285 train_time:518882ms step_avg:744.45ms
step:698/2285 train_time:519627ms step_avg:744.45ms
step:699/2285 train_time:520368ms step_avg:744.45ms
step:700/2285 train_time:521114ms step_avg:744.45ms
step:700/2285 val_loss:3.9105 train_time:521212ms step_avg:744.59ms
step:701/2285 train_time:521855ms step_avg:744.44ms
step:702/2285 train_time:522603ms step_avg:744.45ms
step:703/2285 train_time:523345ms step_avg:744.45ms
step:704/2285 train_time:524091ms step_avg:744.45ms
step:705/2285 train_time:524832ms step_avg:744.44ms
step:706/2285 train_time:525580ms step_avg:744.45ms
step:707/2285 train_time:526321ms step_avg:744.44ms
step:708/2285 train_time:527067ms step_avg:744.44ms
step:709/2285 train_time:527807ms step_avg:744.44ms
step:710/2285 train_time:528553ms step_avg:744.44ms
step:711/2285 train_time:529293ms step_avg:744.43ms
step:712/2285 train_time:530039ms step_avg:744.44ms
step:713/2285 train_time:530781ms step_avg:744.43ms
step:714/2285 train_time:531526ms step_avg:744.43ms
step:715/2285 train_time:532264ms step_avg:744.43ms
step:716/2285 train_time:533012ms step_avg:744.43ms
step:717/2285 train_time:533752ms step_avg:744.42ms
step:718/2285 train_time:534500ms step_avg:744.43ms
step:719/2285 train_time:535242ms step_avg:744.43ms
step:720/2285 train_time:535987ms step_avg:744.43ms
step:721/2285 train_time:536728ms step_avg:744.42ms
step:722/2285 train_time:537474ms step_avg:744.42ms
step:723/2285 train_time:538214ms step_avg:744.42ms
step:724/2285 train_time:538959ms step_avg:744.42ms
step:725/2285 train_time:539702ms step_avg:744.42ms
step:726/2285 train_time:540447ms step_avg:744.42ms
step:727/2285 train_time:541187ms step_avg:744.41ms
step:728/2285 train_time:541933ms step_avg:744.41ms
step:729/2285 train_time:542674ms step_avg:744.41ms
step:730/2285 train_time:543420ms step_avg:744.41ms
step:731/2285 train_time:544160ms step_avg:744.41ms
step:732/2285 train_time:544906ms step_avg:744.41ms
step:733/2285 train_time:545648ms step_avg:744.40ms
step:734/2285 train_time:546392ms step_avg:744.40ms
step:735/2285 train_time:547134ms step_avg:744.40ms
step:736/2285 train_time:547880ms step_avg:744.40ms
step:737/2285 train_time:548621ms step_avg:744.40ms
step:738/2285 train_time:549367ms step_avg:744.40ms
step:739/2285 train_time:550108ms step_avg:744.39ms
step:740/2285 train_time:550854ms step_avg:744.40ms
step:741/2285 train_time:551595ms step_avg:744.39ms
step:742/2285 train_time:552341ms step_avg:744.40ms
step:743/2285 train_time:553081ms step_avg:744.39ms
step:744/2285 train_time:553829ms step_avg:744.39ms
step:745/2285 train_time:554569ms step_avg:744.39ms
step:746/2285 train_time:555314ms step_avg:744.39ms
step:747/2285 train_time:556055ms step_avg:744.38ms
step:748/2285 train_time:556801ms step_avg:744.39ms
step:749/2285 train_time:557541ms step_avg:744.38ms
step:750/2285 train_time:558295ms step_avg:744.39ms
step:751/2285 train_time:559044ms step_avg:744.40ms
step:752/2285 train_time:559797ms step_avg:744.41ms
step:753/2285 train_time:560548ms step_avg:744.42ms
step:754/2285 train_time:561302ms step_avg:744.43ms
step:755/2285 train_time:562051ms step_avg:744.44ms
step:756/2285 train_time:562806ms step_avg:744.45ms
step:757/2285 train_time:563555ms step_avg:744.46ms
step:758/2285 train_time:564313ms step_avg:744.48ms
step:759/2285 train_time:565062ms step_avg:744.48ms
step:760/2285 train_time:565816ms step_avg:744.50ms
step:761/2285 train_time:566565ms step_avg:744.50ms
step:762/2285 train_time:567318ms step_avg:744.51ms
step:763/2285 train_time:568067ms step_avg:744.52ms
step:764/2285 train_time:568821ms step_avg:744.53ms
step:765/2285 train_time:569570ms step_avg:744.54ms
step:766/2285 train_time:570327ms step_avg:744.55ms
step:767/2285 train_time:571074ms step_avg:744.56ms
step:768/2285 train_time:571830ms step_avg:744.57ms
step:769/2285 train_time:572578ms step_avg:744.57ms
step:770/2285 train_time:573331ms step_avg:744.59ms
step:771/2285 train_time:574081ms step_avg:744.59ms
step:772/2285 train_time:574836ms step_avg:744.61ms
step:773/2285 train_time:575583ms step_avg:744.61ms
step:774/2285 train_time:576337ms step_avg:744.62ms
step:775/2285 train_time:577086ms step_avg:744.63ms
step:776/2285 train_time:577841ms step_avg:744.64ms
step:777/2285 train_time:578591ms step_avg:744.65ms
step:778/2285 train_time:579347ms step_avg:744.66ms
step:779/2285 train_time:580096ms step_avg:744.67ms
step:780/2285 train_time:580851ms step_avg:744.68ms
step:781/2285 train_time:581600ms step_avg:744.69ms
step:782/2285 train_time:582355ms step_avg:744.70ms
step:783/2285 train_time:583102ms step_avg:744.70ms
step:784/2285 train_time:583857ms step_avg:744.72ms
step:785/2285 train_time:584610ms step_avg:744.73ms
step:786/2285 train_time:585362ms step_avg:744.74ms
step:787/2285 train_time:586112ms step_avg:744.74ms
step:788/2285 train_time:586866ms step_avg:744.75ms
step:789/2285 train_time:587615ms step_avg:744.76ms
step:790/2285 train_time:588370ms step_avg:744.77ms
step:791/2285 train_time:589119ms step_avg:744.78ms
step:792/2285 train_time:589873ms step_avg:744.79ms
step:793/2285 train_time:590624ms step_avg:744.80ms
step:794/2285 train_time:591376ms step_avg:744.81ms
step:795/2285 train_time:592125ms step_avg:744.81ms
step:796/2285 train_time:592881ms step_avg:744.83ms
step:797/2285 train_time:593630ms step_avg:744.83ms
step:798/2285 train_time:594386ms step_avg:744.84ms
step:799/2285 train_time:595136ms step_avg:744.85ms
step:800/2285 train_time:595891ms step_avg:744.86ms
step:800/2285 val_loss:3.8460 train_time:595989ms step_avg:744.99ms
step:801/2285 train_time:596636ms step_avg:744.86ms
step:802/2285 train_time:597389ms step_avg:744.87ms
step:803/2285 train_time:598133ms step_avg:744.87ms
step:804/2285 train_time:598887ms step_avg:744.88ms
step:805/2285 train_time:599632ms step_avg:744.88ms
step:806/2285 train_time:600385ms step_avg:744.90ms
step:807/2285 train_time:601130ms step_avg:744.89ms
step:808/2285 train_time:601883ms step_avg:744.91ms
step:809/2285 train_time:602630ms step_avg:744.91ms
step:810/2285 train_time:603383ms step_avg:744.92ms
step:811/2285 train_time:604130ms step_avg:744.92ms
step:812/2285 train_time:604882ms step_avg:744.93ms
step:813/2285 train_time:605630ms step_avg:744.93ms
step:814/2285 train_time:606384ms step_avg:744.94ms
step:815/2285 train_time:607130ms step_avg:744.95ms
step:816/2285 train_time:607883ms step_avg:744.96ms
step:817/2285 train_time:608629ms step_avg:744.96ms
step:818/2285 train_time:609383ms step_avg:744.97ms
step:819/2285 train_time:610130ms step_avg:744.97ms
step:820/2285 train_time:610883ms step_avg:744.98ms
step:821/2285 train_time:611632ms step_avg:744.98ms
step:822/2285 train_time:612387ms step_avg:745.00ms
step:823/2285 train_time:613133ms step_avg:745.00ms
step:824/2285 train_time:613888ms step_avg:745.01ms
step:825/2285 train_time:614635ms step_avg:745.01ms
step:826/2285 train_time:615391ms step_avg:745.02ms
step:827/2285 train_time:616140ms step_avg:745.03ms
step:828/2285 train_time:616895ms step_avg:745.04ms
step:829/2285 train_time:617643ms step_avg:745.05ms
step:830/2285 train_time:618396ms step_avg:745.06ms
step:831/2285 train_time:619146ms step_avg:745.06ms
step:832/2285 train_time:619898ms step_avg:745.07ms
step:833/2285 train_time:620647ms step_avg:745.07ms
step:834/2285 train_time:621400ms step_avg:745.08ms
step:835/2285 train_time:622151ms step_avg:745.09ms
step:836/2285 train_time:622907ms step_avg:745.10ms
step:837/2285 train_time:623654ms step_avg:745.11ms
step:838/2285 train_time:624408ms step_avg:745.12ms
step:839/2285 train_time:625156ms step_avg:745.12ms
step:840/2285 train_time:625911ms step_avg:745.13ms
step:841/2285 train_time:626660ms step_avg:745.14ms
step:842/2285 train_time:627414ms step_avg:745.15ms
step:843/2285 train_time:628164ms step_avg:745.15ms
step:844/2285 train_time:628918ms step_avg:745.16ms
step:845/2285 train_time:629670ms step_avg:745.17ms
step:846/2285 train_time:630423ms step_avg:745.18ms
step:847/2285 train_time:631172ms step_avg:745.19ms
step:848/2285 train_time:631928ms step_avg:745.20ms
step:849/2285 train_time:632675ms step_avg:745.20ms
step:850/2285 train_time:633430ms step_avg:745.21ms
step:851/2285 train_time:634182ms step_avg:745.22ms
step:852/2285 train_time:634936ms step_avg:745.23ms
step:853/2285 train_time:635685ms step_avg:745.23ms
step:854/2285 train_time:636439ms step_avg:745.24ms
step:855/2285 train_time:637189ms step_avg:745.25ms
step:856/2285 train_time:637944ms step_avg:745.26ms
step:857/2285 train_time:638691ms step_avg:745.26ms
step:858/2285 train_time:639448ms step_avg:745.28ms
step:859/2285 train_time:640196ms step_avg:745.28ms
step:860/2285 train_time:640953ms step_avg:745.29ms
step:861/2285 train_time:641703ms step_avg:745.30ms
step:862/2285 train_time:642458ms step_avg:745.31ms
step:863/2285 train_time:643210ms step_avg:745.32ms
step:864/2285 train_time:643964ms step_avg:745.33ms
step:865/2285 train_time:644712ms step_avg:745.33ms
step:866/2285 train_time:645469ms step_avg:745.35ms
step:867/2285 train_time:646220ms step_avg:745.35ms
step:868/2285 train_time:646976ms step_avg:745.36ms
step:869/2285 train_time:647724ms step_avg:745.37ms
step:870/2285 train_time:648481ms step_avg:745.38ms
step:871/2285 train_time:649231ms step_avg:745.39ms
step:872/2285 train_time:649988ms step_avg:745.40ms
step:873/2285 train_time:650737ms step_avg:745.40ms
step:874/2285 train_time:651492ms step_avg:745.41ms
step:875/2285 train_time:652242ms step_avg:745.42ms
step:876/2285 train_time:652996ms step_avg:745.43ms
step:877/2285 train_time:653747ms step_avg:745.44ms
step:878/2285 train_time:654501ms step_avg:745.45ms
step:879/2285 train_time:655253ms step_avg:745.45ms
step:880/2285 train_time:656008ms step_avg:745.46ms
step:881/2285 train_time:656757ms step_avg:745.47ms
step:882/2285 train_time:657511ms step_avg:745.48ms
step:883/2285 train_time:658262ms step_avg:745.48ms
step:884/2285 train_time:659017ms step_avg:745.49ms
step:885/2285 train_time:659767ms step_avg:745.50ms
step:886/2285 train_time:660523ms step_avg:745.51ms
step:887/2285 train_time:661274ms step_avg:745.52ms
step:888/2285 train_time:662029ms step_avg:745.53ms
step:889/2285 train_time:662778ms step_avg:745.53ms
step:890/2285 train_time:663534ms step_avg:745.54ms
step:891/2285 train_time:664285ms step_avg:745.55ms
step:892/2285 train_time:665041ms step_avg:745.56ms
step:893/2285 train_time:665792ms step_avg:745.57ms
step:894/2285 train_time:666549ms step_avg:745.58ms
step:895/2285 train_time:667299ms step_avg:745.59ms
step:896/2285 train_time:668056ms step_avg:745.60ms
step:897/2285 train_time:668805ms step_avg:745.60ms
step:898/2285 train_time:669560ms step_avg:745.61ms
step:899/2285 train_time:670311ms step_avg:745.62ms
step:900/2285 train_time:671068ms step_avg:745.63ms
step:900/2285 val_loss:3.8085 train_time:671165ms step_avg:745.74ms
step:901/2285 train_time:671816ms step_avg:745.63ms
step:902/2285 train_time:672572ms step_avg:745.64ms
step:903/2285 train_time:673323ms step_avg:745.65ms
step:904/2285 train_time:674077ms step_avg:745.66ms
step:905/2285 train_time:674826ms step_avg:745.66ms
step:906/2285 train_time:675582ms step_avg:745.68ms
step:907/2285 train_time:676330ms step_avg:745.68ms
step:908/2285 train_time:677088ms step_avg:745.69ms
step:909/2285 train_time:677836ms step_avg:745.69ms
step:910/2285 train_time:678590ms step_avg:745.70ms
step:911/2285 train_time:679341ms step_avg:745.71ms
step:912/2285 train_time:680094ms step_avg:745.72ms
step:913/2285 train_time:680846ms step_avg:745.72ms
step:914/2285 train_time:681601ms step_avg:745.73ms
step:915/2285 train_time:682351ms step_avg:745.74ms
step:916/2285 train_time:683106ms step_avg:745.75ms
step:917/2285 train_time:683854ms step_avg:745.75ms
step:918/2285 train_time:684609ms step_avg:745.76ms
step:919/2285 train_time:685358ms step_avg:745.76ms
step:920/2285 train_time:686112ms step_avg:745.77ms
step:921/2285 train_time:686861ms step_avg:745.78ms
step:922/2285 train_time:687615ms step_avg:745.79ms
step:923/2285 train_time:688365ms step_avg:745.79ms
step:924/2285 train_time:689120ms step_avg:745.80ms
step:925/2285 train_time:689870ms step_avg:745.81ms
step:926/2285 train_time:690625ms step_avg:745.82ms
step:927/2285 train_time:691374ms step_avg:745.82ms
step:928/2285 train_time:692129ms step_avg:745.83ms
step:929/2285 train_time:692880ms step_avg:745.83ms
step:930/2285 train_time:693635ms step_avg:745.84ms
step:931/2285 train_time:694385ms step_avg:745.85ms
step:932/2285 train_time:695137ms step_avg:745.86ms
step:933/2285 train_time:695889ms step_avg:745.86ms
step:934/2285 train_time:696646ms step_avg:745.87ms
step:935/2285 train_time:697393ms step_avg:745.87ms
step:936/2285 train_time:698149ms step_avg:745.89ms
step:937/2285 train_time:698899ms step_avg:745.89ms
step:938/2285 train_time:699654ms step_avg:745.90ms
step:939/2285 train_time:700402ms step_avg:745.90ms
step:940/2285 train_time:701156ms step_avg:745.91ms
step:941/2285 train_time:701908ms step_avg:745.92ms
step:942/2285 train_time:702663ms step_avg:745.93ms
step:943/2285 train_time:703413ms step_avg:745.93ms
step:944/2285 train_time:704167ms step_avg:745.94ms
step:945/2285 train_time:704917ms step_avg:745.94ms
step:946/2285 train_time:705673ms step_avg:745.95ms
step:947/2285 train_time:706424ms step_avg:745.96ms
step:948/2285 train_time:707178ms step_avg:745.97ms
step:949/2285 train_time:707930ms step_avg:745.97ms
step:950/2285 train_time:708686ms step_avg:745.99ms
step:951/2285 train_time:709434ms step_avg:745.99ms
step:952/2285 train_time:710190ms step_avg:746.00ms
step:953/2285 train_time:710942ms step_avg:746.00ms
step:954/2285 train_time:711696ms step_avg:746.01ms
step:955/2285 train_time:712447ms step_avg:746.02ms
step:956/2285 train_time:713203ms step_avg:746.03ms
step:957/2285 train_time:713953ms step_avg:746.03ms
step:958/2285 train_time:714708ms step_avg:746.04ms
step:959/2285 train_time:715459ms step_avg:746.05ms
step:960/2285 train_time:716214ms step_avg:746.06ms
step:961/2285 train_time:716965ms step_avg:746.06ms
step:962/2285 train_time:717720ms step_avg:746.07ms
step:963/2285 train_time:718470ms step_avg:746.07ms
step:964/2285 train_time:719226ms step_avg:746.08ms
step:965/2285 train_time:719976ms step_avg:746.09ms
step:966/2285 train_time:720730ms step_avg:746.10ms
step:967/2285 train_time:721481ms step_avg:746.10ms
step:968/2285 train_time:722237ms step_avg:746.11ms
step:969/2285 train_time:722989ms step_avg:746.12ms
step:970/2285 train_time:723744ms step_avg:746.13ms
step:971/2285 train_time:724494ms step_avg:746.13ms
step:972/2285 train_time:725250ms step_avg:746.14ms
step:973/2285 train_time:725999ms step_avg:746.15ms
step:974/2285 train_time:726753ms step_avg:746.15ms
step:975/2285 train_time:727502ms step_avg:746.16ms
step:976/2285 train_time:728257ms step_avg:746.17ms
step:977/2285 train_time:729009ms step_avg:746.17ms
step:978/2285 train_time:729765ms step_avg:746.18ms
step:979/2285 train_time:730513ms step_avg:746.18ms
step:980/2285 train_time:731268ms step_avg:746.19ms
step:981/2285 train_time:732018ms step_avg:746.20ms
step:982/2285 train_time:732775ms step_avg:746.21ms
step:983/2285 train_time:733526ms step_avg:746.21ms
step:984/2285 train_time:734282ms step_avg:746.22ms
step:985/2285 train_time:735032ms step_avg:746.23ms
step:986/2285 train_time:735788ms step_avg:746.24ms
step:987/2285 train_time:736538ms step_avg:746.24ms
step:988/2285 train_time:737294ms step_avg:746.25ms
step:989/2285 train_time:738043ms step_avg:746.25ms
step:990/2285 train_time:738796ms step_avg:746.26ms
step:991/2285 train_time:739546ms step_avg:746.26ms
step:992/2285 train_time:740300ms step_avg:746.27ms
step:993/2285 train_time:741050ms step_avg:746.27ms
step:994/2285 train_time:741806ms step_avg:746.28ms
step:995/2285 train_time:742555ms step_avg:746.29ms
step:996/2285 train_time:743311ms step_avg:746.30ms
step:997/2285 train_time:744061ms step_avg:746.30ms
step:998/2285 train_time:744816ms step_avg:746.31ms
step:999/2285 train_time:745566ms step_avg:746.31ms
step:1000/2285 train_time:746319ms step_avg:746.32ms
step:1000/2285 val_loss:3.7755 train_time:746417ms step_avg:746.42ms
step:1001/2285 train_time:747069ms step_avg:746.32ms
step:1002/2285 train_time:747826ms step_avg:746.33ms
step:1003/2285 train_time:748578ms step_avg:746.34ms
step:1004/2285 train_time:749331ms step_avg:746.35ms
step:1005/2285 train_time:750080ms step_avg:746.35ms
step:1006/2285 train_time:750837ms step_avg:746.36ms
step:1007/2285 train_time:751587ms step_avg:746.36ms
step:1008/2285 train_time:752344ms step_avg:746.37ms
step:1009/2285 train_time:753094ms step_avg:746.38ms
step:1010/2285 train_time:753850ms step_avg:746.39ms
step:1011/2285 train_time:754601ms step_avg:746.39ms
step:1012/2285 train_time:755355ms step_avg:746.40ms
step:1013/2285 train_time:756107ms step_avg:746.40ms
step:1014/2285 train_time:756863ms step_avg:746.41ms
step:1015/2285 train_time:757611ms step_avg:746.42ms
step:1016/2285 train_time:758366ms step_avg:746.42ms
step:1017/2285 train_time:759116ms step_avg:746.43ms
step:1018/2285 train_time:759871ms step_avg:746.44ms
step:1019/2285 train_time:760620ms step_avg:746.44ms
step:1020/2285 train_time:761375ms step_avg:746.45ms
step:1021/2285 train_time:762126ms step_avg:746.45ms
step:1022/2285 train_time:762883ms step_avg:746.46ms
step:1023/2285 train_time:763630ms step_avg:746.46ms
step:1024/2285 train_time:764386ms step_avg:746.47ms
step:1025/2285 train_time:765137ms step_avg:746.48ms
step:1026/2285 train_time:765891ms step_avg:746.48ms
step:1027/2285 train_time:766643ms step_avg:746.49ms
step:1028/2285 train_time:767397ms step_avg:746.50ms
step:1029/2285 train_time:768147ms step_avg:746.50ms
step:1030/2285 train_time:768905ms step_avg:746.51ms
step:1031/2285 train_time:769653ms step_avg:746.51ms
step:1032/2285 train_time:770409ms step_avg:746.52ms
step:1033/2285 train_time:771160ms step_avg:746.52ms
step:1034/2285 train_time:771914ms step_avg:746.53ms
step:1035/2285 train_time:772666ms step_avg:746.54ms
step:1036/2285 train_time:773422ms step_avg:746.55ms
step:1037/2285 train_time:774170ms step_avg:746.55ms
step:1038/2285 train_time:774926ms step_avg:746.56ms
step:1039/2285 train_time:775677ms step_avg:746.56ms
step:1040/2285 train_time:776433ms step_avg:746.57ms
step:1041/2285 train_time:777185ms step_avg:746.58ms
step:1042/2285 train_time:777941ms step_avg:746.58ms
step:1043/2285 train_time:778691ms step_avg:746.59ms
step:1044/2285 train_time:779447ms step_avg:746.60ms
step:1045/2285 train_time:780197ms step_avg:746.60ms
step:1046/2285 train_time:780952ms step_avg:746.61ms
step:1047/2285 train_time:781705ms step_avg:746.61ms
step:1048/2285 train_time:782458ms step_avg:746.62ms
step:1049/2285 train_time:783207ms step_avg:746.62ms
step:1050/2285 train_time:783965ms step_avg:746.63ms
step:1051/2285 train_time:784712ms step_avg:746.63ms
step:1052/2285 train_time:785469ms step_avg:746.64ms
step:1053/2285 train_time:786216ms step_avg:746.64ms
step:1054/2285 train_time:786972ms step_avg:746.65ms
step:1055/2285 train_time:787722ms step_avg:746.66ms
step:1056/2285 train_time:788476ms step_avg:746.66ms
step:1057/2285 train_time:789227ms step_avg:746.67ms
step:1058/2285 train_time:789983ms step_avg:746.68ms
step:1059/2285 train_time:790731ms step_avg:746.68ms
step:1060/2285 train_time:791487ms step_avg:746.69ms
step:1061/2285 train_time:792238ms step_avg:746.69ms
step:1062/2285 train_time:792994ms step_avg:746.70ms
step:1063/2285 train_time:793743ms step_avg:746.70ms
step:1064/2285 train_time:794497ms step_avg:746.71ms
step:1065/2285 train_time:795248ms step_avg:746.71ms
step:1066/2285 train_time:796005ms step_avg:746.72ms
step:1067/2285 train_time:796755ms step_avg:746.72ms
step:1068/2285 train_time:797510ms step_avg:746.73ms
step:1069/2285 train_time:798260ms step_avg:746.74ms
step:1070/2285 train_time:799016ms step_avg:746.74ms
step:1071/2285 train_time:799767ms step_avg:746.75ms
step:1072/2285 train_time:800524ms step_avg:746.76ms
step:1073/2285 train_time:801272ms step_avg:746.76ms
step:1074/2285 train_time:802026ms step_avg:746.77ms
step:1075/2285 train_time:802776ms step_avg:746.77ms
step:1076/2285 train_time:803532ms step_avg:746.78ms
step:1077/2285 train_time:804281ms step_avg:746.78ms
step:1078/2285 train_time:805036ms step_avg:746.79ms
step:1079/2285 train_time:805785ms step_avg:746.79ms
step:1080/2285 train_time:806539ms step_avg:746.80ms
step:1081/2285 train_time:807290ms step_avg:746.80ms
step:1082/2285 train_time:808046ms step_avg:746.81ms
step:1083/2285 train_time:808794ms step_avg:746.81ms
step:1084/2285 train_time:809550ms step_avg:746.82ms
step:1085/2285 train_time:810300ms step_avg:746.82ms
step:1086/2285 train_time:811054ms step_avg:746.83ms
step:1087/2285 train_time:811804ms step_avg:746.83ms
step:1088/2285 train_time:812559ms step_avg:746.84ms
step:1089/2285 train_time:813310ms step_avg:746.84ms
step:1090/2285 train_time:814066ms step_avg:746.85ms
step:1091/2285 train_time:814818ms step_avg:746.85ms
step:1092/2285 train_time:815573ms step_avg:746.86ms
step:1093/2285 train_time:816323ms step_avg:746.86ms
step:1094/2285 train_time:817080ms step_avg:746.87ms
step:1095/2285 train_time:817830ms step_avg:746.88ms
step:1096/2285 train_time:818586ms step_avg:746.89ms
step:1097/2285 train_time:819338ms step_avg:746.89ms
step:1098/2285 train_time:820092ms step_avg:746.90ms
step:1099/2285 train_time:820843ms step_avg:746.90ms
step:1100/2285 train_time:821597ms step_avg:746.91ms
step:1100/2285 val_loss:3.7526 train_time:821695ms step_avg:747.00ms
step:1101/2285 train_time:822339ms step_avg:746.90ms
step:1102/2285 train_time:823090ms step_avg:746.91ms
step:1103/2285 train_time:823834ms step_avg:746.90ms
step:1104/2285 train_time:824583ms step_avg:746.90ms
step:1105/2285 train_time:825325ms step_avg:746.90ms
step:1106/2285 train_time:826072ms step_avg:746.90ms
step:1107/2285 train_time:826817ms step_avg:746.90ms
step:1108/2285 train_time:827565ms step_avg:746.90ms
step:1109/2285 train_time:828310ms step_avg:746.90ms
step:1110/2285 train_time:829062ms step_avg:746.90ms
step:1111/2285 train_time:829809ms step_avg:746.90ms
step:1112/2285 train_time:830565ms step_avg:746.91ms
step:1113/2285 train_time:831316ms step_avg:746.92ms
step:1114/2285 train_time:832071ms step_avg:746.92ms
step:1115/2285 train_time:832819ms step_avg:746.92ms
step:1116/2285 train_time:833573ms step_avg:746.93ms
step:1117/2285 train_time:834324ms step_avg:746.93ms
step:1118/2285 train_time:835081ms step_avg:746.94ms
step:1119/2285 train_time:835828ms step_avg:746.94ms
step:1120/2285 train_time:836584ms step_avg:746.95ms
step:1121/2285 train_time:837336ms step_avg:746.95ms
step:1122/2285 train_time:838090ms step_avg:746.96ms
step:1123/2285 train_time:838840ms step_avg:746.96ms
step:1124/2285 train_time:839593ms step_avg:746.97ms
step:1125/2285 train_time:840342ms step_avg:746.97ms
step:1126/2285 train_time:841101ms step_avg:746.98ms
step:1127/2285 train_time:841851ms step_avg:746.98ms
step:1128/2285 train_time:842605ms step_avg:746.99ms
step:1129/2285 train_time:843356ms step_avg:746.99ms
step:1130/2285 train_time:844111ms step_avg:747.00ms
step:1131/2285 train_time:844861ms step_avg:747.00ms
step:1132/2285 train_time:845614ms step_avg:747.01ms
step:1133/2285 train_time:846365ms step_avg:747.01ms
step:1134/2285 train_time:847121ms step_avg:747.02ms
step:1135/2285 train_time:847869ms step_avg:747.02ms
step:1136/2285 train_time:848625ms step_avg:747.03ms
step:1137/2285 train_time:849374ms step_avg:747.03ms
step:1138/2285 train_time:850126ms step_avg:747.03ms
step:1139/2285 train_time:850876ms step_avg:747.04ms
step:1140/2285 train_time:851630ms step_avg:747.04ms
step:1141/2285 train_time:852378ms step_avg:747.04ms
step:1142/2285 train_time:853132ms step_avg:747.05ms
step:1143/2285 train_time:853884ms step_avg:747.05ms
step:1144/2285 train_time:854638ms step_avg:747.06ms
step:1145/2285 train_time:855386ms step_avg:747.06ms
step:1146/2285 train_time:856142ms step_avg:747.07ms
step:1147/2285 train_time:856890ms step_avg:747.07ms
step:1148/2285 train_time:857645ms step_avg:747.08ms
step:1149/2285 train_time:858392ms step_avg:747.08ms
step:1150/2285 train_time:859148ms step_avg:747.08ms
step:1151/2285 train_time:859898ms step_avg:747.09ms
step:1152/2285 train_time:860651ms step_avg:747.09ms
step:1153/2285 train_time:861400ms step_avg:747.09ms
step:1154/2285 train_time:862155ms step_avg:747.10ms
step:1155/2285 train_time:862905ms step_avg:747.10ms
step:1156/2285 train_time:863663ms step_avg:747.11ms
step:1157/2285 train_time:864412ms step_avg:747.12ms
step:1158/2285 train_time:865167ms step_avg:747.12ms
step:1159/2285 train_time:865917ms step_avg:747.12ms
step:1160/2285 train_time:866670ms step_avg:747.13ms
step:1161/2285 train_time:867419ms step_avg:747.13ms
step:1162/2285 train_time:868173ms step_avg:747.14ms
step:1163/2285 train_time:868922ms step_avg:747.14ms
step:1164/2285 train_time:869678ms step_avg:747.15ms
step:1165/2285 train_time:870428ms step_avg:747.15ms
step:1166/2285 train_time:871182ms step_avg:747.15ms
step:1167/2285 train_time:871929ms step_avg:747.15ms
step:1168/2285 train_time:872684ms step_avg:747.16ms
step:1169/2285 train_time:873433ms step_avg:747.16ms
step:1170/2285 train_time:874187ms step_avg:747.17ms
step:1171/2285 train_time:874935ms step_avg:747.17ms
step:1172/2285 train_time:875690ms step_avg:747.18ms
step:1173/2285 train_time:876437ms step_avg:747.18ms
step:1174/2285 train_time:877191ms step_avg:747.18ms
step:1175/2285 train_time:877941ms step_avg:747.18ms
step:1176/2285 train_time:878694ms step_avg:747.19ms
step:1177/2285 train_time:879442ms step_avg:747.19ms
step:1178/2285 train_time:880193ms step_avg:747.19ms
step:1179/2285 train_time:880943ms step_avg:747.20ms
step:1180/2285 train_time:881696ms step_avg:747.20ms
step:1181/2285 train_time:882446ms step_avg:747.20ms
step:1182/2285 train_time:883200ms step_avg:747.21ms
step:1183/2285 train_time:883950ms step_avg:747.21ms
step:1184/2285 train_time:884704ms step_avg:747.22ms
step:1185/2285 train_time:885453ms step_avg:747.22ms
step:1186/2285 train_time:886207ms step_avg:747.22ms
step:1187/2285 train_time:886958ms step_avg:747.23ms
step:1188/2285 train_time:887712ms step_avg:747.23ms
step:1189/2285 train_time:888460ms step_avg:747.23ms
step:1190/2285 train_time:889215ms step_avg:747.24ms
step:1191/2285 train_time:889965ms step_avg:747.24ms
step:1192/2285 train_time:890721ms step_avg:747.25ms
step:1193/2285 train_time:891468ms step_avg:747.25ms
step:1194/2285 train_time:892224ms step_avg:747.26ms
step:1195/2285 train_time:892971ms step_avg:747.26ms
step:1196/2285 train_time:893725ms step_avg:747.26ms
step:1197/2285 train_time:894473ms step_avg:747.26ms
step:1198/2285 train_time:895230ms step_avg:747.27ms
step:1199/2285 train_time:895978ms step_avg:747.27ms
step:1200/2285 train_time:896733ms step_avg:747.28ms
step:1200/2285 val_loss:3.7217 train_time:896831ms step_avg:747.36ms
step:1201/2285 train_time:897482ms step_avg:747.28ms
step:1202/2285 train_time:898235ms step_avg:747.28ms
step:1203/2285 train_time:898985ms step_avg:747.29ms
step:1204/2285 train_time:899737ms step_avg:747.29ms
step:1205/2285 train_time:900487ms step_avg:747.29ms
step:1206/2285 train_time:901242ms step_avg:747.30ms
step:1207/2285 train_time:901993ms step_avg:747.30ms
step:1208/2285 train_time:902748ms step_avg:747.31ms
step:1209/2285 train_time:903497ms step_avg:747.31ms
step:1210/2285 train_time:904251ms step_avg:747.31ms
step:1211/2285 train_time:905002ms step_avg:747.32ms
step:1212/2285 train_time:905754ms step_avg:747.32ms
step:1213/2285 train_time:906501ms step_avg:747.32ms
step:1214/2285 train_time:907255ms step_avg:747.33ms
step:1215/2285 train_time:908003ms step_avg:747.33ms
step:1216/2285 train_time:908757ms step_avg:747.33ms
step:1217/2285 train_time:909506ms step_avg:747.33ms
step:1218/2285 train_time:910259ms step_avg:747.34ms
step:1219/2285 train_time:911010ms step_avg:747.34ms
step:1220/2285 train_time:911763ms step_avg:747.35ms
step:1221/2285 train_time:912510ms step_avg:747.35ms
step:1222/2285 train_time:913266ms step_avg:747.35ms
step:1223/2285 train_time:914013ms step_avg:747.35ms
step:1224/2285 train_time:914767ms step_avg:747.36ms
step:1225/2285 train_time:915514ms step_avg:747.36ms
step:1226/2285 train_time:916270ms step_avg:747.37ms
step:1227/2285 train_time:917019ms step_avg:747.37ms
step:1228/2285 train_time:917773ms step_avg:747.37ms
step:1229/2285 train_time:918524ms step_avg:747.37ms
step:1230/2285 train_time:919277ms step_avg:747.38ms
step:1231/2285 train_time:920026ms step_avg:747.38ms
step:1232/2285 train_time:920781ms step_avg:747.39ms
step:1233/2285 train_time:921530ms step_avg:747.39ms
step:1234/2285 train_time:922284ms step_avg:747.39ms
step:1235/2285 train_time:923032ms step_avg:747.39ms
step:1236/2285 train_time:923788ms step_avg:747.40ms
step:1237/2285 train_time:924537ms step_avg:747.40ms
step:1238/2285 train_time:925291ms step_avg:747.41ms
step:1239/2285 train_time:926040ms step_avg:747.41ms
step:1240/2285 train_time:926794ms step_avg:747.41ms
step:1241/2285 train_time:927542ms step_avg:747.41ms
step:1242/2285 train_time:928297ms step_avg:747.42ms
step:1243/2285 train_time:929045ms step_avg:747.42ms
step:1244/2285 train_time:929799ms step_avg:747.43ms
step:1245/2285 train_time:930549ms step_avg:747.43ms
step:1246/2285 train_time:931303ms step_avg:747.43ms
step:1247/2285 train_time:932050ms step_avg:747.43ms
step:1248/2285 train_time:932805ms step_avg:747.44ms
step:1249/2285 train_time:933554ms step_avg:747.44ms
step:1250/2285 train_time:934308ms step_avg:747.45ms
step:1251/2285 train_time:935055ms step_avg:747.45ms
step:1252/2285 train_time:935810ms step_avg:747.45ms
step:1253/2285 train_time:936559ms step_avg:747.45ms
step:1254/2285 train_time:937313ms step_avg:747.46ms
step:1255/2285 train_time:938061ms step_avg:747.46ms
step:1256/2285 train_time:938814ms step_avg:747.46ms
step:1257/2285 train_time:939563ms step_avg:747.46ms
step:1258/2285 train_time:940315ms step_avg:747.47ms
step:1259/2285 train_time:941062ms step_avg:747.47ms
step:1260/2285 train_time:941816ms step_avg:747.47ms
step:1261/2285 train_time:942565ms step_avg:747.47ms
step:1262/2285 train_time:943317ms step_avg:747.48ms
step:1263/2285 train_time:944065ms step_avg:747.48ms
step:1264/2285 train_time:944818ms step_avg:747.48ms
step:1265/2285 train_time:945565ms step_avg:747.48ms
step:1266/2285 train_time:946318ms step_avg:747.49ms
step:1267/2285 train_time:947065ms step_avg:747.49ms
step:1268/2285 train_time:947818ms step_avg:747.49ms
step:1269/2285 train_time:948566ms step_avg:747.49ms
step:1270/2285 train_time:949320ms step_avg:747.50ms
step:1271/2285 train_time:950069ms step_avg:747.50ms
step:1272/2285 train_time:950822ms step_avg:747.50ms
step:1273/2285 train_time:951570ms step_avg:747.50ms
step:1274/2285 train_time:952324ms step_avg:747.51ms
step:1275/2285 train_time:953072ms step_avg:747.51ms
step:1276/2285 train_time:953827ms step_avg:747.51ms
step:1277/2285 train_time:954573ms step_avg:747.51ms
step:1278/2285 train_time:955328ms step_avg:747.52ms
step:1279/2285 train_time:956075ms step_avg:747.52ms
step:1280/2285 train_time:956831ms step_avg:747.52ms
step:1281/2285 train_time:957579ms step_avg:747.52ms
step:1282/2285 train_time:958334ms step_avg:747.53ms
step:1283/2285 train_time:959082ms step_avg:747.53ms
step:1284/2285 train_time:959836ms step_avg:747.54ms
step:1285/2285 train_time:960585ms step_avg:747.54ms
step:1286/2285 train_time:961336ms step_avg:747.54ms
step:1287/2285 train_time:962085ms step_avg:747.54ms
step:1288/2285 train_time:962838ms step_avg:747.55ms
step:1289/2285 train_time:963587ms step_avg:747.55ms
step:1290/2285 train_time:964338ms step_avg:747.55ms
step:1291/2285 train_time:965089ms step_avg:747.55ms
step:1292/2285 train_time:965842ms step_avg:747.56ms
step:1293/2285 train_time:966591ms step_avg:747.56ms
step:1294/2285 train_time:967343ms step_avg:747.56ms
step:1295/2285 train_time:968091ms step_avg:747.56ms
step:1296/2285 train_time:968845ms step_avg:747.57ms
step:1297/2285 train_time:969592ms step_avg:747.57ms
step:1298/2285 train_time:970345ms step_avg:747.57ms
step:1299/2285 train_time:971092ms step_avg:747.57ms
step:1300/2285 train_time:971846ms step_avg:747.57ms
step:1300/2285 val_loss:3.6874 train_time:971943ms step_avg:747.65ms
step:1301/2285 train_time:972592ms step_avg:747.57ms
step:1302/2285 train_time:973348ms step_avg:747.58ms
step:1303/2285 train_time:974097ms step_avg:747.58ms
step:1304/2285 train_time:974850ms step_avg:747.58ms
step:1305/2285 train_time:975596ms step_avg:747.58ms
step:1306/2285 train_time:976351ms step_avg:747.59ms
step:1307/2285 train_time:977097ms step_avg:747.59ms
step:1308/2285 train_time:977851ms step_avg:747.59ms
step:1309/2285 train_time:978601ms step_avg:747.59ms
step:1310/2285 train_time:979354ms step_avg:747.60ms
step:1311/2285 train_time:980101ms step_avg:747.60ms
step:1312/2285 train_time:980854ms step_avg:747.60ms
step:1313/2285 train_time:981602ms step_avg:747.60ms
step:1314/2285 train_time:982354ms step_avg:747.61ms
step:1315/2285 train_time:983101ms step_avg:747.61ms
step:1316/2285 train_time:983854ms step_avg:747.61ms
step:1317/2285 train_time:984603ms step_avg:747.61ms
step:1318/2285 train_time:985356ms step_avg:747.61ms
step:1319/2285 train_time:986104ms step_avg:747.61ms
step:1320/2285 train_time:986858ms step_avg:747.62ms
step:1321/2285 train_time:987606ms step_avg:747.62ms
step:1322/2285 train_time:988361ms step_avg:747.63ms
step:1323/2285 train_time:989107ms step_avg:747.62ms
step:1324/2285 train_time:989863ms step_avg:747.63ms
step:1325/2285 train_time:990609ms step_avg:747.63ms
step:1326/2285 train_time:991365ms step_avg:747.64ms
step:1327/2285 train_time:992112ms step_avg:747.64ms
step:1328/2285 train_time:992866ms step_avg:747.64ms
step:1329/2285 train_time:993614ms step_avg:747.64ms
step:1330/2285 train_time:994368ms step_avg:747.64ms
step:1331/2285 train_time:995116ms step_avg:747.65ms
step:1332/2285 train_time:995871ms step_avg:747.65ms
step:1333/2285 train_time:996618ms step_avg:747.65ms
step:1334/2285 train_time:997373ms step_avg:747.66ms
step:1335/2285 train_time:998121ms step_avg:747.66ms
step:1336/2285 train_time:998875ms step_avg:747.66ms
step:1337/2285 train_time:999624ms step_avg:747.66ms
step:1338/2285 train_time:1000379ms step_avg:747.67ms
step:1339/2285 train_time:1001127ms step_avg:747.67ms
step:1340/2285 train_time:1001881ms step_avg:747.67ms
step:1341/2285 train_time:1002627ms step_avg:747.67ms
step:1342/2285 train_time:1003382ms step_avg:747.68ms
step:1343/2285 train_time:1004128ms step_avg:747.68ms
step:1344/2285 train_time:1004882ms step_avg:747.68ms
step:1345/2285 train_time:1005627ms step_avg:747.68ms
step:1346/2285 train_time:1006382ms step_avg:747.68ms
step:1347/2285 train_time:1007128ms step_avg:747.68ms
step:1348/2285 train_time:1007882ms step_avg:747.69ms
step:1349/2285 train_time:1008629ms step_avg:747.69ms
step:1350/2285 train_time:1009384ms step_avg:747.69ms
step:1351/2285 train_time:1010128ms step_avg:747.69ms
step:1352/2285 train_time:1010884ms step_avg:747.70ms
step:1353/2285 train_time:1011630ms step_avg:747.69ms
step:1354/2285 train_time:1012385ms step_avg:747.70ms
step:1355/2285 train_time:1013133ms step_avg:747.70ms
step:1356/2285 train_time:1013886ms step_avg:747.70ms
step:1357/2285 train_time:1014634ms step_avg:747.70ms
step:1358/2285 train_time:1015387ms step_avg:747.71ms
step:1359/2285 train_time:1016135ms step_avg:747.71ms
step:1360/2285 train_time:1016890ms step_avg:747.71ms
step:1361/2285 train_time:1017638ms step_avg:747.71ms
step:1362/2285 train_time:1018391ms step_avg:747.72ms
step:1363/2285 train_time:1019139ms step_avg:747.72ms
step:1364/2285 train_time:1019893ms step_avg:747.72ms
step:1365/2285 train_time:1020641ms step_avg:747.72ms
step:1366/2285 train_time:1021393ms step_avg:747.73ms
step:1367/2285 train_time:1022142ms step_avg:747.73ms
step:1368/2285 train_time:1022893ms step_avg:747.73ms
step:1369/2285 train_time:1023643ms step_avg:747.73ms
step:1370/2285 train_time:1024396ms step_avg:747.73ms
step:1371/2285 train_time:1025146ms step_avg:747.74ms
step:1372/2285 train_time:1025898ms step_avg:747.74ms
step:1373/2285 train_time:1026648ms step_avg:747.74ms
step:1374/2285 train_time:1027402ms step_avg:747.75ms
step:1375/2285 train_time:1028148ms step_avg:747.74ms
step:1376/2285 train_time:1028903ms step_avg:747.75ms
step:1377/2285 train_time:1029651ms step_avg:747.75ms
step:1378/2285 train_time:1030404ms step_avg:747.75ms
step:1379/2285 train_time:1031150ms step_avg:747.75ms
step:1380/2285 train_time:1031904ms step_avg:747.76ms
step:1381/2285 train_time:1032652ms step_avg:747.76ms
step:1382/2285 train_time:1033405ms step_avg:747.76ms
step:1383/2285 train_time:1034152ms step_avg:747.76ms
step:1384/2285 train_time:1034906ms step_avg:747.76ms
step:1385/2285 train_time:1035655ms step_avg:747.77ms
step:1386/2285 train_time:1036407ms step_avg:747.77ms
step:1387/2285 train_time:1037158ms step_avg:747.77ms
step:1388/2285 train_time:1037911ms step_avg:747.77ms
step:1389/2285 train_time:1038659ms step_avg:747.77ms
step:1390/2285 train_time:1039413ms step_avg:747.78ms
step:1391/2285 train_time:1040162ms step_avg:747.78ms
step:1392/2285 train_time:1040914ms step_avg:747.78ms
step:1393/2285 train_time:1041664ms step_avg:747.78ms
step:1394/2285 train_time:1042417ms step_avg:747.79ms
step:1395/2285 train_time:1043166ms step_avg:747.79ms
step:1396/2285 train_time:1043919ms step_avg:747.79ms
step:1397/2285 train_time:1044667ms step_avg:747.79ms
step:1398/2285 train_time:1045420ms step_avg:747.80ms
step:1399/2285 train_time:1046167ms step_avg:747.80ms
step:1400/2285 train_time:1046922ms step_avg:747.80ms
step:1400/2285 val_loss:3.6616 train_time:1047019ms step_avg:747.87ms
step:1401/2285 train_time:1047671ms step_avg:747.80ms
step:1402/2285 train_time:1048425ms step_avg:747.81ms
step:1403/2285 train_time:1049171ms step_avg:747.81ms
step:1404/2285 train_time:1049926ms step_avg:747.81ms
step:1405/2285 train_time:1050675ms step_avg:747.81ms
step:1406/2285 train_time:1051429ms step_avg:747.82ms
step:1407/2285 train_time:1052176ms step_avg:747.82ms
step:1408/2285 train_time:1052930ms step_avg:747.82ms
step:1409/2285 train_time:1053679ms step_avg:747.82ms
step:1410/2285 train_time:1054433ms step_avg:747.83ms
step:1411/2285 train_time:1055182ms step_avg:747.83ms
step:1412/2285 train_time:1055935ms step_avg:747.83ms
step:1413/2285 train_time:1056683ms step_avg:747.83ms
step:1414/2285 train_time:1057436ms step_avg:747.83ms
step:1415/2285 train_time:1058184ms step_avg:747.83ms
step:1416/2285 train_time:1058936ms step_avg:747.84ms
step:1417/2285 train_time:1059684ms step_avg:747.84ms
step:1418/2285 train_time:1060438ms step_avg:747.84ms
step:1419/2285 train_time:1061186ms step_avg:747.84ms
step:1420/2285 train_time:1061940ms step_avg:747.85ms
step:1421/2285 train_time:1062689ms step_avg:747.85ms
step:1422/2285 train_time:1063446ms step_avg:747.85ms
step:1423/2285 train_time:1064193ms step_avg:747.85ms
step:1424/2285 train_time:1064946ms step_avg:747.86ms
step:1425/2285 train_time:1065694ms step_avg:747.86ms
step:1426/2285 train_time:1066448ms step_avg:747.86ms
step:1427/2285 train_time:1067196ms step_avg:747.86ms
step:1428/2285 train_time:1067949ms step_avg:747.86ms
step:1429/2285 train_time:1068696ms step_avg:747.86ms
step:1430/2285 train_time:1069450ms step_avg:747.87ms
step:1431/2285 train_time:1070200ms step_avg:747.87ms
step:1432/2285 train_time:1070953ms step_avg:747.87ms
step:1433/2285 train_time:1071702ms step_avg:747.87ms
step:1434/2285 train_time:1072455ms step_avg:747.88ms
step:1435/2285 train_time:1073206ms step_avg:747.88ms
step:1436/2285 train_time:1073960ms step_avg:747.88ms
step:1437/2285 train_time:1074709ms step_avg:747.88ms
step:1438/2285 train_time:1075465ms step_avg:747.89ms
step:1439/2285 train_time:1076211ms step_avg:747.89ms
step:1440/2285 train_time:1076965ms step_avg:747.89ms
step:1441/2285 train_time:1077711ms step_avg:747.89ms
step:1442/2285 train_time:1078465ms step_avg:747.90ms
step:1443/2285 train_time:1079212ms step_avg:747.89ms
step:1444/2285 train_time:1079966ms step_avg:747.90ms
step:1445/2285 train_time:1080712ms step_avg:747.90ms
step:1446/2285 train_time:1081466ms step_avg:747.90ms
step:1447/2285 train_time:1082212ms step_avg:747.90ms
step:1448/2285 train_time:1082968ms step_avg:747.91ms
step:1449/2285 train_time:1083714ms step_avg:747.90ms
step:1450/2285 train_time:1084468ms step_avg:747.91ms
step:1451/2285 train_time:1085216ms step_avg:747.91ms
step:1452/2285 train_time:1085969ms step_avg:747.91ms
step:1453/2285 train_time:1086717ms step_avg:747.91ms
step:1454/2285 train_time:1087470ms step_avg:747.92ms
step:1455/2285 train_time:1088217ms step_avg:747.92ms
step:1456/2285 train_time:1088970ms step_avg:747.92ms
step:1457/2285 train_time:1089720ms step_avg:747.92ms
step:1458/2285 train_time:1090472ms step_avg:747.92ms
step:1459/2285 train_time:1091220ms step_avg:747.92ms
step:1460/2285 train_time:1091974ms step_avg:747.93ms
step:1461/2285 train_time:1092723ms step_avg:747.93ms
step:1462/2285 train_time:1093476ms step_avg:747.93ms
step:1463/2285 train_time:1094225ms step_avg:747.93ms
step:1464/2285 train_time:1094979ms step_avg:747.94ms
step:1465/2285 train_time:1095729ms step_avg:747.94ms
step:1466/2285 train_time:1096484ms step_avg:747.94ms
step:1467/2285 train_time:1097232ms step_avg:747.94ms
step:1468/2285 train_time:1097985ms step_avg:747.95ms
step:1469/2285 train_time:1098732ms step_avg:747.95ms
step:1470/2285 train_time:1099485ms step_avg:747.95ms
step:1471/2285 train_time:1100234ms step_avg:747.95ms
step:1472/2285 train_time:1100987ms step_avg:747.95ms
step:1473/2285 train_time:1101733ms step_avg:747.95ms
step:1474/2285 train_time:1102488ms step_avg:747.96ms
step:1475/2285 train_time:1103236ms step_avg:747.96ms
step:1476/2285 train_time:1103988ms step_avg:747.96ms
step:1477/2285 train_time:1104737ms step_avg:747.96ms
step:1478/2285 train_time:1105489ms step_avg:747.96ms
step:1479/2285 train_time:1106241ms step_avg:747.97ms
step:1480/2285 train_time:1106993ms step_avg:747.97ms
step:1481/2285 train_time:1107742ms step_avg:747.97ms
step:1482/2285 train_time:1108495ms step_avg:747.97ms
step:1483/2285 train_time:1109243ms step_avg:747.97ms
step:1484/2285 train_time:1109995ms step_avg:747.98ms
step:1485/2285 train_time:1110743ms step_avg:747.97ms
step:1486/2285 train_time:1111496ms step_avg:747.98ms
step:1487/2285 train_time:1112245ms step_avg:747.98ms
step:1488/2285 train_time:1112997ms step_avg:747.98ms
step:1489/2285 train_time:1113745ms step_avg:747.98ms
step:1490/2285 train_time:1114498ms step_avg:747.99ms
step:1491/2285 train_time:1115248ms step_avg:747.99ms
step:1492/2285 train_time:1116003ms step_avg:747.99ms
step:1493/2285 train_time:1116750ms step_avg:747.99ms
step:1494/2285 train_time:1117505ms step_avg:748.00ms
step:1495/2285 train_time:1118252ms step_avg:747.99ms
step:1496/2285 train_time:1119007ms step_avg:748.00ms
step:1497/2285 train_time:1119753ms step_avg:748.00ms
step:1498/2285 train_time:1120514ms step_avg:748.01ms
step:1499/2285 train_time:1121268ms step_avg:748.01ms
step:1500/2285 train_time:1122027ms step_avg:748.02ms
step:1500/2285 val_loss:3.6332 train_time:1122125ms step_avg:748.08ms
step:1501/2285 train_time:1122764ms step_avg:748.01ms
step:1502/2285 train_time:1123521ms step_avg:748.02ms
step:1503/2285 train_time:1124271ms step_avg:748.02ms
step:1504/2285 train_time:1125024ms step_avg:748.02ms
step:1505/2285 train_time:1125774ms step_avg:748.02ms
step:1506/2285 train_time:1126529ms step_avg:748.03ms
step:1507/2285 train_time:1127280ms step_avg:748.03ms
step:1508/2285 train_time:1128037ms step_avg:748.03ms
step:1509/2285 train_time:1128784ms step_avg:748.03ms
step:1510/2285 train_time:1129540ms step_avg:748.04ms
step:1511/2285 train_time:1130290ms step_avg:748.04ms
step:1512/2285 train_time:1131045ms step_avg:748.05ms
step:1513/2285 train_time:1131796ms step_avg:748.05ms
step:1514/2285 train_time:1132550ms step_avg:748.05ms
step:1515/2285 train_time:1133303ms step_avg:748.05ms
step:1516/2285 train_time:1134061ms step_avg:748.06ms
step:1517/2285 train_time:1134812ms step_avg:748.06ms
step:1518/2285 train_time:1135568ms step_avg:748.07ms
step:1519/2285 train_time:1136320ms step_avg:748.07ms
step:1520/2285 train_time:1137079ms step_avg:748.08ms
step:1521/2285 train_time:1137830ms step_avg:748.08ms
step:1522/2285 train_time:1138585ms step_avg:748.08ms
step:1523/2285 train_time:1139336ms step_avg:748.09ms
step:1524/2285 train_time:1140091ms step_avg:748.09ms
step:1525/2285 train_time:1140843ms step_avg:748.09ms
step:1526/2285 train_time:1141601ms step_avg:748.10ms
step:1527/2285 train_time:1142354ms step_avg:748.10ms
step:1528/2285 train_time:1143110ms step_avg:748.11ms
step:1529/2285 train_time:1143862ms step_avg:748.11ms
step:1530/2285 train_time:1144618ms step_avg:748.12ms
step:1531/2285 train_time:1145370ms step_avg:748.12ms
step:1532/2285 train_time:1146127ms step_avg:748.12ms
step:1533/2285 train_time:1146880ms step_avg:748.13ms
step:1534/2285 train_time:1147639ms step_avg:748.13ms
step:1535/2285 train_time:1148387ms step_avg:748.13ms
step:1536/2285 train_time:1149147ms step_avg:748.14ms
step:1537/2285 train_time:1149898ms step_avg:748.14ms
step:1538/2285 train_time:1150656ms step_avg:748.15ms
step:1539/2285 train_time:1151408ms step_avg:748.15ms
step:1540/2285 train_time:1152166ms step_avg:748.16ms
step:1541/2285 train_time:1152920ms step_avg:748.16ms
step:1542/2285 train_time:1153680ms step_avg:748.17ms
step:1543/2285 train_time:1154436ms step_avg:748.18ms
step:1544/2285 train_time:1155190ms step_avg:748.18ms
step:1545/2285 train_time:1155943ms step_avg:748.18ms
step:1546/2285 train_time:1156702ms step_avg:748.19ms
step:1547/2285 train_time:1157456ms step_avg:748.19ms
step:1548/2285 train_time:1158213ms step_avg:748.20ms
step:1549/2285 train_time:1158967ms step_avg:748.20ms
step:1550/2285 train_time:1159727ms step_avg:748.21ms
step:1551/2285 train_time:1160481ms step_avg:748.22ms
step:1552/2285 train_time:1161238ms step_avg:748.22ms
step:1553/2285 train_time:1161991ms step_avg:748.22ms
step:1554/2285 train_time:1162748ms step_avg:748.23ms
step:1555/2285 train_time:1163502ms step_avg:748.23ms
step:1556/2285 train_time:1164263ms step_avg:748.24ms
step:1557/2285 train_time:1165018ms step_avg:748.25ms
step:1558/2285 train_time:1165777ms step_avg:748.25ms
step:1559/2285 train_time:1166528ms step_avg:748.25ms
step:1560/2285 train_time:1167288ms step_avg:748.26ms
step:1561/2285 train_time:1168042ms step_avg:748.27ms
step:1562/2285 train_time:1168801ms step_avg:748.27ms
step:1563/2285 train_time:1169556ms step_avg:748.28ms
step:1564/2285 train_time:1170313ms step_avg:748.28ms
step:1565/2285 train_time:1171069ms step_avg:748.29ms
step:1566/2285 train_time:1171828ms step_avg:748.29ms
step:1567/2285 train_time:1172583ms step_avg:748.30ms
step:1568/2285 train_time:1173344ms step_avg:748.31ms
step:1569/2285 train_time:1174096ms step_avg:748.31ms
step:1570/2285 train_time:1174853ms step_avg:748.31ms
step:1571/2285 train_time:1175608ms step_avg:748.32ms
step:1572/2285 train_time:1176368ms step_avg:748.33ms
step:1573/2285 train_time:1177123ms step_avg:748.33ms
step:1574/2285 train_time:1177884ms step_avg:748.34ms
step:1575/2285 train_time:1178638ms step_avg:748.34ms
step:1576/2285 train_time:1179397ms step_avg:748.35ms
step:1577/2285 train_time:1180150ms step_avg:748.35ms
step:1578/2285 train_time:1180912ms step_avg:748.36ms
step:1579/2285 train_time:1181666ms step_avg:748.36ms
step:1580/2285 train_time:1182427ms step_avg:748.37ms
step:1581/2285 train_time:1183183ms step_avg:748.38ms
step:1582/2285 train_time:1183943ms step_avg:748.38ms
step:1583/2285 train_time:1184697ms step_avg:748.39ms
step:1584/2285 train_time:1185456ms step_avg:748.39ms
step:1585/2285 train_time:1186209ms step_avg:748.40ms
step:1586/2285 train_time:1186969ms step_avg:748.40ms
step:1587/2285 train_time:1187723ms step_avg:748.41ms
step:1588/2285 train_time:1188484ms step_avg:748.42ms
step:1589/2285 train_time:1189238ms step_avg:748.42ms
step:1590/2285 train_time:1189997ms step_avg:748.43ms
step:1591/2285 train_time:1190750ms step_avg:748.43ms
step:1592/2285 train_time:1191508ms step_avg:748.43ms
step:1593/2285 train_time:1192264ms step_avg:748.44ms
step:1594/2285 train_time:1193023ms step_avg:748.45ms
step:1595/2285 train_time:1193778ms step_avg:748.45ms
step:1596/2285 train_time:1194538ms step_avg:748.46ms
step:1597/2285 train_time:1195291ms step_avg:748.46ms
step:1598/2285 train_time:1196049ms step_avg:748.47ms
step:1599/2285 train_time:1196802ms step_avg:748.47ms
step:1600/2285 train_time:1197562ms step_avg:748.48ms
step:1600/2285 val_loss:3.6049 train_time:1197660ms step_avg:748.54ms
step:1601/2285 train_time:1198317ms step_avg:748.48ms
step:1602/2285 train_time:1199077ms step_avg:748.49ms
step:1603/2285 train_time:1199830ms step_avg:748.49ms
step:1604/2285 train_time:1200586ms step_avg:748.49ms
step:1605/2285 train_time:1201340ms step_avg:748.50ms
step:1606/2285 train_time:1202100ms step_avg:748.51ms
step:1607/2285 train_time:1202853ms step_avg:748.51ms
step:1608/2285 train_time:1203614ms step_avg:748.52ms
step:1609/2285 train_time:1204365ms step_avg:748.52ms
step:1610/2285 train_time:1205122ms step_avg:748.52ms
step:1611/2285 train_time:1205878ms step_avg:748.53ms
step:1612/2285 train_time:1206637ms step_avg:748.53ms
step:1613/2285 train_time:1207390ms step_avg:748.54ms
step:1614/2285 train_time:1208148ms step_avg:748.54ms
step:1615/2285 train_time:1208902ms step_avg:748.55ms
step:1616/2285 train_time:1209663ms step_avg:748.55ms
step:1617/2285 train_time:1210415ms step_avg:748.56ms
step:1618/2285 train_time:1211175ms step_avg:748.56ms
step:1619/2285 train_time:1211928ms step_avg:748.57ms
step:1620/2285 train_time:1212688ms step_avg:748.57ms
step:1621/2285 train_time:1213442ms step_avg:748.58ms
step:1622/2285 train_time:1214200ms step_avg:748.58ms
step:1623/2285 train_time:1214955ms step_avg:748.59ms
step:1624/2285 train_time:1215714ms step_avg:748.59ms
step:1625/2285 train_time:1216470ms step_avg:748.60ms
step:1626/2285 train_time:1217229ms step_avg:748.60ms
step:1627/2285 train_time:1217984ms step_avg:748.61ms
step:1628/2285 train_time:1218744ms step_avg:748.61ms
step:1629/2285 train_time:1219498ms step_avg:748.62ms
step:1630/2285 train_time:1220258ms step_avg:748.62ms
step:1631/2285 train_time:1221011ms step_avg:748.63ms
step:1632/2285 train_time:1221768ms step_avg:748.63ms
step:1633/2285 train_time:1222523ms step_avg:748.64ms
step:1634/2285 train_time:1223282ms step_avg:748.64ms
step:1635/2285 train_time:1224036ms step_avg:748.65ms
step:1636/2285 train_time:1224794ms step_avg:748.65ms
step:1637/2285 train_time:1225548ms step_avg:748.65ms
step:1638/2285 train_time:1226307ms step_avg:748.66ms
step:1639/2285 train_time:1227061ms step_avg:748.66ms
step:1640/2285 train_time:1227822ms step_avg:748.67ms
step:1641/2285 train_time:1228576ms step_avg:748.68ms
step:1642/2285 train_time:1229334ms step_avg:748.68ms
step:1643/2285 train_time:1230091ms step_avg:748.69ms
step:1644/2285 train_time:1230848ms step_avg:748.69ms
step:1645/2285 train_time:1231603ms step_avg:748.70ms
step:1646/2285 train_time:1232362ms step_avg:748.70ms
step:1647/2285 train_time:1233117ms step_avg:748.71ms
step:1648/2285 train_time:1233877ms step_avg:748.71ms
step:1649/2285 train_time:1234630ms step_avg:748.71ms
step:1650/2285 train_time:1235387ms step_avg:748.72ms
step:1651/2285 train_time:1236143ms step_avg:748.72ms
step:1652/2285 train_time:1236902ms step_avg:748.73ms
step:1653/2285 train_time:1237657ms step_avg:748.73ms
step:1654/2285 train_time:1238417ms step_avg:748.74ms
step:1655/2285 train_time:1239171ms step_avg:748.74ms
step:1656/2285 train_time:1239928ms step_avg:748.75ms
step:1657/2285 train_time:1240681ms step_avg:748.75ms
step:1658/2285 train_time:1241442ms step_avg:748.76ms
step:1659/2285 train_time:1242197ms step_avg:748.76ms
step:1660/2285 train_time:1242956ms step_avg:748.77ms
step:1661/2285 train_time:1243710ms step_avg:748.77ms
step:1662/2285 train_time:1244469ms step_avg:748.78ms
step:1663/2285 train_time:1245220ms step_avg:748.78ms
step:1664/2285 train_time:1245979ms step_avg:748.79ms
step:1665/2285 train_time:1246732ms step_avg:748.79ms
step:1666/2285 train_time:1247491ms step_avg:748.79ms
step:1667/2285 train_time:1248242ms step_avg:748.80ms
step:1668/2285 train_time:1249002ms step_avg:748.80ms
step:1669/2285 train_time:1249758ms step_avg:748.81ms
step:1670/2285 train_time:1250517ms step_avg:748.81ms
step:1671/2285 train_time:1251271ms step_avg:748.82ms
step:1672/2285 train_time:1252028ms step_avg:748.82ms
step:1673/2285 train_time:1252783ms step_avg:748.82ms
step:1674/2285 train_time:1253543ms step_avg:748.83ms
step:1675/2285 train_time:1254297ms step_avg:748.83ms
step:1676/2285 train_time:1255057ms step_avg:748.84ms
step:1677/2285 train_time:1255812ms step_avg:748.84ms
step:1678/2285 train_time:1256570ms step_avg:748.85ms
step:1679/2285 train_time:1257324ms step_avg:748.85ms
step:1680/2285 train_time:1258082ms step_avg:748.86ms
step:1681/2285 train_time:1258838ms step_avg:748.86ms
step:1682/2285 train_time:1259598ms step_avg:748.87ms
step:1683/2285 train_time:1260352ms step_avg:748.87ms
step:1684/2285 train_time:1261110ms step_avg:748.88ms
step:1685/2285 train_time:1261865ms step_avg:748.88ms
step:1686/2285 train_time:1262624ms step_avg:748.89ms
step:1687/2285 train_time:1263379ms step_avg:748.89ms
step:1688/2285 train_time:1264138ms step_avg:748.90ms
step:1689/2285 train_time:1264893ms step_avg:748.90ms
step:1690/2285 train_time:1265653ms step_avg:748.91ms
step:1691/2285 train_time:1266405ms step_avg:748.91ms
step:1692/2285 train_time:1267164ms step_avg:748.91ms
step:1693/2285 train_time:1267919ms step_avg:748.92ms
step:1694/2285 train_time:1268679ms step_avg:748.92ms
step:1695/2285 train_time:1269433ms step_avg:748.93ms
step:1696/2285 train_time:1270195ms step_avg:748.94ms
step:1697/2285 train_time:1270947ms step_avg:748.94ms
step:1698/2285 train_time:1271706ms step_avg:748.94ms
step:1699/2285 train_time:1272461ms step_avg:748.95ms
step:1700/2285 train_time:1273222ms step_avg:748.95ms
step:1700/2285 val_loss:3.5855 train_time:1273321ms step_avg:749.01ms
step:1701/2285 train_time:1273976ms step_avg:748.96ms
step:1702/2285 train_time:1274738ms step_avg:748.96ms
step:1703/2285 train_time:1275493ms step_avg:748.97ms
step:1704/2285 train_time:1276253ms step_avg:748.97ms
step:1705/2285 train_time:1277006ms step_avg:748.98ms
step:1706/2285 train_time:1277765ms step_avg:748.98ms
step:1707/2285 train_time:1278518ms step_avg:748.99ms
step:1708/2285 train_time:1279279ms step_avg:748.99ms
step:1709/2285 train_time:1280034ms step_avg:749.00ms
step:1710/2285 train_time:1280792ms step_avg:749.00ms
step:1711/2285 train_time:1281547ms step_avg:749.00ms
step:1712/2285 train_time:1282306ms step_avg:749.01ms
step:1713/2285 train_time:1283059ms step_avg:749.01ms
step:1714/2285 train_time:1283819ms step_avg:749.02ms
step:1715/2285 train_time:1284575ms step_avg:749.02ms
step:1716/2285 train_time:1285336ms step_avg:749.03ms
step:1717/2285 train_time:1286089ms step_avg:749.03ms
step:1718/2285 train_time:1286850ms step_avg:749.04ms
step:1719/2285 train_time:1287602ms step_avg:749.04ms
step:1720/2285 train_time:1288361ms step_avg:749.05ms
step:1721/2285 train_time:1289115ms step_avg:749.05ms
step:1722/2285 train_time:1289877ms step_avg:749.06ms
step:1723/2285 train_time:1290631ms step_avg:749.06ms
step:1724/2285 train_time:1291391ms step_avg:749.07ms
step:1725/2285 train_time:1292147ms step_avg:749.07ms
step:1726/2285 train_time:1292905ms step_avg:749.08ms
step:1727/2285 train_time:1293659ms step_avg:749.08ms
step:1728/2285 train_time:1294419ms step_avg:749.09ms
step:1729/2285 train_time:1295173ms step_avg:749.09ms
step:1730/2285 train_time:1295937ms step_avg:749.10ms
step:1731/2285 train_time:1296691ms step_avg:749.10ms
step:1732/2285 train_time:1297451ms step_avg:749.11ms
step:1733/2285 train_time:1298206ms step_avg:749.11ms
step:1734/2285 train_time:1298965ms step_avg:749.11ms
step:1735/2285 train_time:1299719ms step_avg:749.12ms
step:1736/2285 train_time:1300478ms step_avg:749.12ms
step:1737/2285 train_time:1301234ms step_avg:749.13ms
step:1738/2285 train_time:1301995ms step_avg:749.13ms
step:1739/2285 train_time:1302748ms step_avg:749.14ms
step:1740/2285 train_time:1303507ms step_avg:749.14ms
step:1741/2285 train_time:1304260ms step_avg:749.14ms
step:1742/2285 train_time:1305019ms step_avg:749.15ms
step:1743/2285 train_time:1305773ms step_avg:749.15ms
step:1744/2285 train_time:1306534ms step_avg:749.16ms
step:1745/2285 train_time:1307289ms step_avg:749.16ms
step:1746/2285 train_time:1308047ms step_avg:749.17ms
step:1747/2285 train_time:1308801ms step_avg:749.17ms
step:1748/2285 train_time:1309560ms step_avg:749.18ms
step:1749/2285 train_time:1310314ms step_avg:749.18ms
step:1750/2285 train_time:1311075ms step_avg:749.19ms
step:1751/2285 train_time:1311830ms step_avg:749.19ms
step:1752/2285 train_time:1312590ms step_avg:749.20ms
step:1753/2285 train_time:1313344ms step_avg:749.20ms
step:1754/2285 train_time:1314102ms step_avg:749.20ms
step:1755/2285 train_time:1314858ms step_avg:749.21ms
step:1756/2285 train_time:1315619ms step_avg:749.21ms
step:1757/2285 train_time:1316372ms step_avg:749.22ms
step:1758/2285 train_time:1317131ms step_avg:749.22ms
step:1759/2285 train_time:1317887ms step_avg:749.22ms
step:1760/2285 train_time:1318645ms step_avg:749.23ms
step:1761/2285 train_time:1319399ms step_avg:749.23ms
step:1762/2285 train_time:1320158ms step_avg:749.24ms
step:1763/2285 train_time:1320914ms step_avg:749.24ms
step:1764/2285 train_time:1321674ms step_avg:749.25ms
step:1765/2285 train_time:1322428ms step_avg:749.25ms
step:1766/2285 train_time:1323188ms step_avg:749.26ms
step:1767/2285 train_time:1323940ms step_avg:749.26ms
step:1768/2285 train_time:1324699ms step_avg:749.26ms
step:1769/2285 train_time:1325453ms step_avg:749.27ms
step:1770/2285 train_time:1326213ms step_avg:749.27ms
step:1771/2285 train_time:1326967ms step_avg:749.28ms
step:1772/2285 train_time:1327725ms step_avg:749.28ms
step:1773/2285 train_time:1328478ms step_avg:749.28ms
step:1774/2285 train_time:1329239ms step_avg:749.29ms
step:1775/2285 train_time:1329993ms step_avg:749.29ms
step:1776/2285 train_time:1330754ms step_avg:749.30ms
step:1777/2285 train_time:1331509ms step_avg:749.30ms
step:1778/2285 train_time:1332267ms step_avg:749.31ms
step:1779/2285 train_time:1333022ms step_avg:749.31ms
step:1780/2285 train_time:1333782ms step_avg:749.32ms
step:1781/2285 train_time:1334537ms step_avg:749.32ms
step:1782/2285 train_time:1335298ms step_avg:749.33ms
step:1783/2285 train_time:1336053ms step_avg:749.33ms
step:1784/2285 train_time:1336813ms step_avg:749.33ms
step:1785/2285 train_time:1337569ms step_avg:749.34ms
step:1786/2285 train_time:1338328ms step_avg:749.34ms
step:1787/2285 train_time:1339081ms step_avg:749.35ms
step:1788/2285 train_time:1339840ms step_avg:749.35ms
step:1789/2285 train_time:1340594ms step_avg:749.35ms
step:1790/2285 train_time:1341354ms step_avg:749.36ms
step:1791/2285 train_time:1342108ms step_avg:749.36ms
step:1792/2285 train_time:1342865ms step_avg:749.37ms
step:1793/2285 train_time:1343620ms step_avg:749.37ms
step:1794/2285 train_time:1344381ms step_avg:749.38ms
step:1795/2285 train_time:1345135ms step_avg:749.38ms
step:1796/2285 train_time:1345893ms step_avg:749.38ms
step:1797/2285 train_time:1346647ms step_avg:749.39ms
step:1798/2285 train_time:1347405ms step_avg:749.39ms
step:1799/2285 train_time:1348158ms step_avg:749.39ms
step:1800/2285 train_time:1348919ms step_avg:749.40ms
step:1800/2285 val_loss:3.5633 train_time:1349018ms step_avg:749.45ms
step:1801/2285 train_time:1349671ms step_avg:749.40ms
step:1802/2285 train_time:1350432ms step_avg:749.41ms
step:1803/2285 train_time:1351187ms step_avg:749.41ms
step:1804/2285 train_time:1351947ms step_avg:749.42ms
step:1805/2285 train_time:1352703ms step_avg:749.42ms
step:1806/2285 train_time:1353461ms step_avg:749.42ms
step:1807/2285 train_time:1354213ms step_avg:749.43ms
step:1808/2285 train_time:1354974ms step_avg:749.43ms
step:1809/2285 train_time:1355729ms step_avg:749.44ms
step:1810/2285 train_time:1356490ms step_avg:749.44ms
step:1811/2285 train_time:1357243ms step_avg:749.44ms
step:1812/2285 train_time:1358004ms step_avg:749.45ms
step:1813/2285 train_time:1358757ms step_avg:749.45ms
step:1814/2285 train_time:1359516ms step_avg:749.46ms
step:1815/2285 train_time:1360272ms step_avg:749.46ms
step:1816/2285 train_time:1361031ms step_avg:749.47ms
step:1817/2285 train_time:1361787ms step_avg:749.47ms
step:1818/2285 train_time:1362546ms step_avg:749.48ms
step:1819/2285 train_time:1363298ms step_avg:749.48ms
step:1820/2285 train_time:1364058ms step_avg:749.48ms
step:1821/2285 train_time:1364809ms step_avg:749.48ms
step:1822/2285 train_time:1365572ms step_avg:749.49ms
step:1823/2285 train_time:1366324ms step_avg:749.49ms
step:1824/2285 train_time:1367084ms step_avg:749.50ms
step:1825/2285 train_time:1367836ms step_avg:749.50ms
step:1826/2285 train_time:1368592ms step_avg:749.50ms
step:1827/2285 train_time:1369347ms step_avg:749.51ms
step:1828/2285 train_time:1370108ms step_avg:749.51ms
step:1829/2285 train_time:1370863ms step_avg:749.51ms
step:1830/2285 train_time:1371620ms step_avg:749.52ms
step:1831/2285 train_time:1372373ms step_avg:749.52ms
step:1832/2285 train_time:1373132ms step_avg:749.53ms
step:1833/2285 train_time:1373886ms step_avg:749.53ms
step:1834/2285 train_time:1374647ms step_avg:749.53ms
step:1835/2285 train_time:1375397ms step_avg:749.54ms
step:1836/2285 train_time:1376156ms step_avg:749.54ms
step:1837/2285 train_time:1376909ms step_avg:749.54ms
step:1838/2285 train_time:1377673ms step_avg:749.55ms
step:1839/2285 train_time:1378426ms step_avg:749.55ms
step:1840/2285 train_time:1379184ms step_avg:749.56ms
step:1841/2285 train_time:1379939ms step_avg:749.56ms
step:1842/2285 train_time:1380696ms step_avg:749.56ms
step:1843/2285 train_time:1381450ms step_avg:749.57ms
step:1844/2285 train_time:1382208ms step_avg:749.57ms
step:1845/2285 train_time:1382962ms step_avg:749.57ms
step:1846/2285 train_time:1383721ms step_avg:749.58ms
step:1847/2285 train_time:1384476ms step_avg:749.58ms
step:1848/2285 train_time:1385234ms step_avg:749.59ms
step:1849/2285 train_time:1385987ms step_avg:749.59ms
step:1850/2285 train_time:1386748ms step_avg:749.59ms
step:1851/2285 train_time:1387502ms step_avg:749.60ms
step:1852/2285 train_time:1388261ms step_avg:749.60ms
step:1853/2285 train_time:1389013ms step_avg:749.60ms
step:1854/2285 train_time:1389773ms step_avg:749.61ms
step:1855/2285 train_time:1390527ms step_avg:749.61ms
step:1856/2285 train_time:1391287ms step_avg:749.62ms
step:1857/2285 train_time:1392041ms step_avg:749.62ms
step:1858/2285 train_time:1392800ms step_avg:749.62ms
step:1859/2285 train_time:1393552ms step_avg:749.62ms
step:1860/2285 train_time:1394313ms step_avg:749.63ms
step:1861/2285 train_time:1395066ms step_avg:749.63ms
step:1862/2285 train_time:1395826ms step_avg:749.64ms
step:1863/2285 train_time:1396579ms step_avg:749.64ms
step:1864/2285 train_time:1397336ms step_avg:749.64ms
step:1865/2285 train_time:1398089ms step_avg:749.65ms
step:1866/2285 train_time:1398848ms step_avg:749.65ms
step:1867/2285 train_time:1399602ms step_avg:749.65ms
step:1868/2285 train_time:1400361ms step_avg:749.66ms
step:1869/2285 train_time:1401114ms step_avg:749.66ms
step:1870/2285 train_time:1401874ms step_avg:749.66ms
step:1871/2285 train_time:1402627ms step_avg:749.67ms
step:1872/2285 train_time:1403386ms step_avg:749.67ms
step:1873/2285 train_time:1404140ms step_avg:749.67ms
step:1874/2285 train_time:1404901ms step_avg:749.68ms
step:1875/2285 train_time:1405652ms step_avg:749.68ms
step:1876/2285 train_time:1406412ms step_avg:749.69ms
step:1877/2285 train_time:1407167ms step_avg:749.69ms
step:1878/2285 train_time:1407925ms step_avg:749.69ms
step:1879/2285 train_time:1408680ms step_avg:749.70ms
step:1880/2285 train_time:1409438ms step_avg:749.70ms
step:1881/2285 train_time:1410192ms step_avg:749.70ms
step:1882/2285 train_time:1410952ms step_avg:749.71ms
step:1883/2285 train_time:1411707ms step_avg:749.71ms
step:1884/2285 train_time:1412464ms step_avg:749.72ms
step:1885/2285 train_time:1413214ms step_avg:749.72ms
step:1886/2285 train_time:1413973ms step_avg:749.72ms
step:1887/2285 train_time:1414729ms step_avg:749.72ms
step:1888/2285 train_time:1415487ms step_avg:749.73ms
step:1889/2285 train_time:1416239ms step_avg:749.73ms
step:1890/2285 train_time:1416998ms step_avg:749.73ms
step:1891/2285 train_time:1417753ms step_avg:749.74ms
step:1892/2285 train_time:1418513ms step_avg:749.74ms
step:1893/2285 train_time:1419266ms step_avg:749.74ms
step:1894/2285 train_time:1420025ms step_avg:749.75ms
step:1895/2285 train_time:1420780ms step_avg:749.75ms
step:1896/2285 train_time:1421536ms step_avg:749.76ms
step:1897/2285 train_time:1422289ms step_avg:749.76ms
step:1898/2285 train_time:1423050ms step_avg:749.76ms
step:1899/2285 train_time:1423803ms step_avg:749.76ms
step:1900/2285 train_time:1424563ms step_avg:749.77ms
step:1900/2285 val_loss:3.5451 train_time:1424660ms step_avg:749.82ms
step:1901/2285 train_time:1425315ms step_avg:749.77ms
step:1902/2285 train_time:1426075ms step_avg:749.78ms
step:1903/2285 train_time:1426830ms step_avg:749.78ms
step:1904/2285 train_time:1427588ms step_avg:749.78ms
step:1905/2285 train_time:1428340ms step_avg:749.78ms
step:1906/2285 train_time:1429099ms step_avg:749.79ms
step:1907/2285 train_time:1429852ms step_avg:749.79ms
step:1908/2285 train_time:1430612ms step_avg:749.80ms
step:1909/2285 train_time:1431363ms step_avg:749.80ms
step:1910/2285 train_time:1432121ms step_avg:749.80ms
step:1911/2285 train_time:1432875ms step_avg:749.80ms
step:1912/2285 train_time:1433637ms step_avg:749.81ms
step:1913/2285 train_time:1434389ms step_avg:749.81ms
step:1914/2285 train_time:1435149ms step_avg:749.82ms
step:1915/2285 train_time:1435901ms step_avg:749.82ms
step:1916/2285 train_time:1436661ms step_avg:749.82ms
step:1917/2285 train_time:1437416ms step_avg:749.83ms
step:1918/2285 train_time:1438174ms step_avg:749.83ms
step:1919/2285 train_time:1438928ms step_avg:749.83ms
step:1920/2285 train_time:1439685ms step_avg:749.84ms
step:1921/2285 train_time:1440439ms step_avg:749.84ms
step:1922/2285 train_time:1441200ms step_avg:749.84ms
step:1923/2285 train_time:1441955ms step_avg:749.85ms
step:1924/2285 train_time:1442714ms step_avg:749.85ms
step:1925/2285 train_time:1443469ms step_avg:749.85ms
step:1926/2285 train_time:1444227ms step_avg:749.86ms
step:1927/2285 train_time:1444982ms step_avg:749.86ms
step:1928/2285 train_time:1445739ms step_avg:749.86ms
step:1929/2285 train_time:1446495ms step_avg:749.87ms
step:1930/2285 train_time:1447256ms step_avg:749.87ms
step:1931/2285 train_time:1448010ms step_avg:749.88ms
step:1932/2285 train_time:1448767ms step_avg:749.88ms
step:1933/2285 train_time:1449522ms step_avg:749.88ms
step:1934/2285 train_time:1450281ms step_avg:749.89ms
step:1935/2285 train_time:1451037ms step_avg:749.89ms
step:1936/2285 train_time:1451797ms step_avg:749.90ms
step:1937/2285 train_time:1452553ms step_avg:749.90ms
step:1938/2285 train_time:1453312ms step_avg:749.90ms
step:1939/2285 train_time:1454064ms step_avg:749.90ms
step:1940/2285 train_time:1454822ms step_avg:749.91ms
step:1941/2285 train_time:1455577ms step_avg:749.91ms
step:1942/2285 train_time:1456336ms step_avg:749.92ms
step:1943/2285 train_time:1457090ms step_avg:749.92ms
step:1944/2285 train_time:1457848ms step_avg:749.92ms
step:1945/2285 train_time:1458603ms step_avg:749.92ms
step:1946/2285 train_time:1459362ms step_avg:749.93ms
step:1947/2285 train_time:1460117ms step_avg:749.93ms
step:1948/2285 train_time:1460876ms step_avg:749.94ms
step:1949/2285 train_time:1461630ms step_avg:749.94ms
step:1950/2285 train_time:1462389ms step_avg:749.94ms
step:1951/2285 train_time:1463142ms step_avg:749.94ms
step:1952/2285 train_time:1463901ms step_avg:749.95ms
step:1953/2285 train_time:1464655ms step_avg:749.95ms
step:1954/2285 train_time:1465416ms step_avg:749.96ms
step:1955/2285 train_time:1466169ms step_avg:749.96ms
step:1956/2285 train_time:1466929ms step_avg:749.96ms
step:1957/2285 train_time:1467682ms step_avg:749.97ms
step:1958/2285 train_time:1468443ms step_avg:749.97ms
step:1959/2285 train_time:1469199ms step_avg:749.97ms
step:1960/2285 train_time:1469958ms step_avg:749.98ms
step:1961/2285 train_time:1470715ms step_avg:749.98ms
step:1962/2285 train_time:1471475ms step_avg:749.99ms
step:1963/2285 train_time:1472227ms step_avg:749.99ms
step:1964/2285 train_time:1472987ms step_avg:749.99ms
step:1965/2285 train_time:1473739ms step_avg:749.99ms
step:1966/2285 train_time:1474502ms step_avg:750.00ms
step:1967/2285 train_time:1475257ms step_avg:750.00ms
step:1968/2285 train_time:1476016ms step_avg:750.01ms
step:1969/2285 train_time:1476771ms step_avg:750.01ms
step:1970/2285 train_time:1477531ms step_avg:750.02ms
step:1971/2285 train_time:1478282ms step_avg:750.02ms
step:1972/2285 train_time:1479041ms step_avg:750.02ms
step:1973/2285 train_time:1479796ms step_avg:750.02ms
step:1974/2285 train_time:1480557ms step_avg:750.03ms
step:1975/2285 train_time:1481311ms step_avg:750.03ms
step:1976/2285 train_time:1482070ms step_avg:750.04ms
step:1977/2285 train_time:1482822ms step_avg:750.04ms
step:1978/2285 train_time:1483582ms step_avg:750.04ms
step:1979/2285 train_time:1484336ms step_avg:750.04ms
step:1980/2285 train_time:1485096ms step_avg:750.05ms
step:1981/2285 train_time:1485851ms step_avg:750.05ms
step:1982/2285 train_time:1486608ms step_avg:750.05ms
step:1983/2285 train_time:1487363ms step_avg:750.06ms
step:1984/2285 train_time:1488122ms step_avg:750.06ms
step:1985/2285 train_time:1488877ms step_avg:750.06ms
step:1986/2285 train_time:1489636ms step_avg:750.07ms
step:1987/2285 train_time:1490391ms step_avg:750.07ms
step:1988/2285 train_time:1491147ms step_avg:750.07ms
step:1989/2285 train_time:1491900ms step_avg:750.08ms
step:1990/2285 train_time:1492661ms step_avg:750.08ms
step:1991/2285 train_time:1493411ms step_avg:750.08ms
step:1992/2285 train_time:1494169ms step_avg:750.08ms
step:1993/2285 train_time:1494924ms step_avg:750.09ms
step:1994/2285 train_time:1495682ms step_avg:750.09ms
step:1995/2285 train_time:1496437ms step_avg:750.09ms
step:1996/2285 train_time:1497197ms step_avg:750.10ms
step:1997/2285 train_time:1497953ms step_avg:750.10ms
step:1998/2285 train_time:1498712ms step_avg:750.11ms
step:1999/2285 train_time:1499465ms step_avg:750.11ms
step:2000/2285 train_time:1500223ms step_avg:750.11ms
step:2000/2285 val_loss:3.5267 train_time:1500321ms step_avg:750.16ms
step:2001/2285 train_time:1500976ms step_avg:750.11ms
step:2002/2285 train_time:1501735ms step_avg:750.12ms
step:2003/2285 train_time:1502487ms step_avg:750.12ms
step:2004/2285 train_time:1503246ms step_avg:750.12ms
step:2005/2285 train_time:1503999ms step_avg:750.12ms
step:2006/2285 train_time:1504759ms step_avg:750.13ms
step:2007/2285 train_time:1505513ms step_avg:750.13ms
step:2008/2285 train_time:1506270ms step_avg:750.13ms
step:2009/2285 train_time:1507025ms step_avg:750.14ms
step:2010/2285 train_time:1507783ms step_avg:750.14ms
step:2011/2285 train_time:1508538ms step_avg:750.14ms
step:2012/2285 train_time:1509298ms step_avg:750.15ms
step:2013/2285 train_time:1510052ms step_avg:750.15ms
step:2014/2285 train_time:1510807ms step_avg:750.15ms
step:2015/2285 train_time:1511561ms step_avg:750.15ms
step:2016/2285 train_time:1512322ms step_avg:750.16ms
step:2017/2285 train_time:1513076ms step_avg:750.16ms
step:2018/2285 train_time:1513834ms step_avg:750.17ms
step:2019/2285 train_time:1514586ms step_avg:750.17ms
step:2020/2285 train_time:1515345ms step_avg:750.17ms
step:2021/2285 train_time:1516099ms step_avg:750.17ms
step:2022/2285 train_time:1516859ms step_avg:750.18ms
step:2023/2285 train_time:1517612ms step_avg:750.18ms
step:2024/2285 train_time:1518370ms step_avg:750.18ms
step:2025/2285 train_time:1519125ms step_avg:750.19ms
step:2026/2285 train_time:1519884ms step_avg:750.19ms
step:2027/2285 train_time:1520640ms step_avg:750.19ms
step:2028/2285 train_time:1521397ms step_avg:750.20ms
step:2029/2285 train_time:1522151ms step_avg:750.20ms
step:2030/2285 train_time:1522909ms step_avg:750.20ms
step:2031/2285 train_time:1523661ms step_avg:750.20ms
step:2032/2285 train_time:1524420ms step_avg:750.21ms
step:2033/2285 train_time:1525174ms step_avg:750.21ms
step:2034/2285 train_time:1525933ms step_avg:750.21ms
step:2035/2285 train_time:1526685ms step_avg:750.21ms
step:2036/2285 train_time:1527445ms step_avg:750.22ms
step:2037/2285 train_time:1528199ms step_avg:750.22ms
step:2038/2285 train_time:1528959ms step_avg:750.23ms
step:2039/2285 train_time:1529713ms step_avg:750.23ms
step:2040/2285 train_time:1530468ms step_avg:750.23ms
step:2041/2285 train_time:1531225ms step_avg:750.23ms
step:2042/2285 train_time:1531983ms step_avg:750.24ms
step:2043/2285 train_time:1532737ms step_avg:750.24ms
step:2044/2285 train_time:1533498ms step_avg:750.24ms
step:2045/2285 train_time:1534251ms step_avg:750.24ms
step:2046/2285 train_time:1535011ms step_avg:750.25ms
step:2047/2285 train_time:1535764ms step_avg:750.25ms
step:2048/2285 train_time:1536524ms step_avg:750.26ms
step:2049/2285 train_time:1537278ms step_avg:750.26ms
step:2050/2285 train_time:1538037ms step_avg:750.26ms
step:2051/2285 train_time:1538791ms step_avg:750.26ms
step:2052/2285 train_time:1539551ms step_avg:750.27ms
step:2053/2285 train_time:1540303ms step_avg:750.27ms
step:2054/2285 train_time:1541064ms step_avg:750.27ms
step:2055/2285 train_time:1541816ms step_avg:750.28ms
step:2056/2285 train_time:1542578ms step_avg:750.28ms
step:2057/2285 train_time:1543330ms step_avg:750.28ms
step:2058/2285 train_time:1544089ms step_avg:750.29ms
step:2059/2285 train_time:1544840ms step_avg:750.29ms
step:2060/2285 train_time:1545600ms step_avg:750.29ms
step:2061/2285 train_time:1546353ms step_avg:750.29ms
step:2062/2285 train_time:1547109ms step_avg:750.30ms
step:2063/2285 train_time:1547863ms step_avg:750.30ms
step:2064/2285 train_time:1548621ms step_avg:750.30ms
step:2065/2285 train_time:1549375ms step_avg:750.30ms
step:2066/2285 train_time:1550133ms step_avg:750.31ms
step:2067/2285 train_time:1550887ms step_avg:750.31ms
step:2068/2285 train_time:1551646ms step_avg:750.31ms
step:2069/2285 train_time:1552399ms step_avg:750.31ms
step:2070/2285 train_time:1553160ms step_avg:750.32ms
step:2071/2285 train_time:1553914ms step_avg:750.32ms
step:2072/2285 train_time:1554671ms step_avg:750.32ms
step:2073/2285 train_time:1555424ms step_avg:750.33ms
step:2074/2285 train_time:1556184ms step_avg:750.33ms
step:2075/2285 train_time:1556939ms step_avg:750.33ms
step:2076/2285 train_time:1557698ms step_avg:750.34ms
step:2077/2285 train_time:1558452ms step_avg:750.34ms
step:2078/2285 train_time:1559210ms step_avg:750.34ms
step:2079/2285 train_time:1559962ms step_avg:750.34ms
step:2080/2285 train_time:1560723ms step_avg:750.35ms
step:2081/2285 train_time:1561475ms step_avg:750.35ms
step:2082/2285 train_time:1562233ms step_avg:750.35ms
step:2083/2285 train_time:1562986ms step_avg:750.35ms
step:2084/2285 train_time:1563745ms step_avg:750.36ms
step:2085/2285 train_time:1564499ms step_avg:750.36ms
step:2086/2285 train_time:1565259ms step_avg:750.36ms
step:2087/2285 train_time:1566012ms step_avg:750.37ms
step:2088/2285 train_time:1566767ms step_avg:750.37ms
step:2089/2285 train_time:1567521ms step_avg:750.37ms
step:2090/2285 train_time:1568279ms step_avg:750.37ms
step:2091/2285 train_time:1569033ms step_avg:750.37ms
step:2092/2285 train_time:1569789ms step_avg:750.38ms
step:2093/2285 train_time:1570544ms step_avg:750.38ms
step:2094/2285 train_time:1571302ms step_avg:750.38ms
step:2095/2285 train_time:1572056ms step_avg:750.38ms
step:2096/2285 train_time:1572815ms step_avg:750.39ms
step:2097/2285 train_time:1573566ms step_avg:750.39ms
step:2098/2285 train_time:1574326ms step_avg:750.39ms
step:2099/2285 train_time:1575078ms step_avg:750.39ms
step:2100/2285 train_time:1575838ms step_avg:750.40ms
step:2100/2285 val_loss:3.5117 train_time:1575936ms step_avg:750.45ms
step:2101/2285 train_time:1576590ms step_avg:750.40ms
step:2102/2285 train_time:1577348ms step_avg:750.40ms
step:2103/2285 train_time:1578104ms step_avg:750.41ms
step:2104/2285 train_time:1578864ms step_avg:750.41ms
step:2105/2285 train_time:1579618ms step_avg:750.41ms
step:2106/2285 train_time:1580377ms step_avg:750.42ms
step:2107/2285 train_time:1581129ms step_avg:750.42ms
step:2108/2285 train_time:1581888ms step_avg:750.42ms
step:2109/2285 train_time:1582643ms step_avg:750.42ms
step:2110/2285 train_time:1583404ms step_avg:750.43ms
step:2111/2285 train_time:1584158ms step_avg:750.43ms
step:2112/2285 train_time:1584918ms step_avg:750.43ms
step:2113/2285 train_time:1585668ms step_avg:750.43ms
step:2114/2285 train_time:1586427ms step_avg:750.44ms
step:2115/2285 train_time:1587181ms step_avg:750.44ms
step:2116/2285 train_time:1587941ms step_avg:750.44ms
step:2117/2285 train_time:1588694ms step_avg:750.45ms
step:2118/2285 train_time:1589451ms step_avg:750.45ms
step:2119/2285 train_time:1590205ms step_avg:750.45ms
step:2120/2285 train_time:1590963ms step_avg:750.45ms
step:2121/2285 train_time:1591716ms step_avg:750.46ms
step:2122/2285 train_time:1592473ms step_avg:750.46ms
step:2123/2285 train_time:1593230ms step_avg:750.46ms
step:2124/2285 train_time:1593987ms step_avg:750.46ms
step:2125/2285 train_time:1594743ms step_avg:750.47ms
step:2126/2285 train_time:1595504ms step_avg:750.47ms
step:2127/2285 train_time:1596257ms step_avg:750.47ms
step:2128/2285 train_time:1597014ms step_avg:750.48ms
step:2129/2285 train_time:1597769ms step_avg:750.48ms
step:2130/2285 train_time:1598529ms step_avg:750.48ms
step:2131/2285 train_time:1599283ms step_avg:750.48ms
step:2132/2285 train_time:1600044ms step_avg:750.49ms
step:2133/2285 train_time:1600800ms step_avg:750.49ms
step:2134/2285 train_time:1601559ms step_avg:750.50ms
step:2135/2285 train_time:1602314ms step_avg:750.50ms
step:2136/2285 train_time:1603073ms step_avg:750.50ms
step:2137/2285 train_time:1603828ms step_avg:750.50ms
step:2138/2285 train_time:1604586ms step_avg:750.51ms
step:2139/2285 train_time:1605339ms step_avg:750.51ms
step:2140/2285 train_time:1606100ms step_avg:750.51ms
step:2141/2285 train_time:1606852ms step_avg:750.51ms
step:2142/2285 train_time:1607609ms step_avg:750.52ms
step:2143/2285 train_time:1608363ms step_avg:750.52ms
step:2144/2285 train_time:1609121ms step_avg:750.52ms
step:2145/2285 train_time:1609876ms step_avg:750.53ms
step:2146/2285 train_time:1610634ms step_avg:750.53ms
step:2147/2285 train_time:1611388ms step_avg:750.53ms
step:2148/2285 train_time:1612147ms step_avg:750.53ms
step:2149/2285 train_time:1612900ms step_avg:750.54ms
step:2150/2285 train_time:1613660ms step_avg:750.54ms
step:2151/2285 train_time:1614416ms step_avg:750.54ms
step:2152/2285 train_time:1615173ms step_avg:750.55ms
step:2153/2285 train_time:1615928ms step_avg:750.55ms
step:2154/2285 train_time:1616688ms step_avg:750.55ms
step:2155/2285 train_time:1617442ms step_avg:750.55ms
step:2156/2285 train_time:1618202ms step_avg:750.56ms
step:2157/2285 train_time:1618955ms step_avg:750.56ms
step:2158/2285 train_time:1619714ms step_avg:750.56ms
step:2159/2285 train_time:1620466ms step_avg:750.56ms
step:2160/2285 train_time:1621228ms step_avg:750.57ms
step:2161/2285 train_time:1621982ms step_avg:750.57ms
step:2162/2285 train_time:1622742ms step_avg:750.57ms
step:2163/2285 train_time:1623496ms step_avg:750.58ms
step:2164/2285 train_time:1624255ms step_avg:750.58ms
step:2165/2285 train_time:1625011ms step_avg:750.58ms
step:2166/2285 train_time:1625768ms step_avg:750.59ms
step:2167/2285 train_time:1626522ms step_avg:750.59ms
step:2168/2285 train_time:1627284ms step_avg:750.59ms
step:2169/2285 train_time:1628037ms step_avg:750.59ms
step:2170/2285 train_time:1628795ms step_avg:750.60ms
step:2171/2285 train_time:1629549ms step_avg:750.60ms
step:2172/2285 train_time:1630308ms step_avg:750.60ms
step:2173/2285 train_time:1631064ms step_avg:750.60ms
step:2174/2285 train_time:1631823ms step_avg:750.61ms
step:2175/2285 train_time:1632578ms step_avg:750.61ms
step:2176/2285 train_time:1633335ms step_avg:750.61ms
step:2177/2285 train_time:1634089ms step_avg:750.62ms
step:2178/2285 train_time:1634848ms step_avg:750.62ms
step:2179/2285 train_time:1635599ms step_avg:750.62ms
step:2180/2285 train_time:1636361ms step_avg:750.62ms
step:2181/2285 train_time:1637113ms step_avg:750.63ms
step:2182/2285 train_time:1637873ms step_avg:750.63ms
step:2183/2285 train_time:1638628ms step_avg:750.63ms
step:2184/2285 train_time:1639389ms step_avg:750.64ms
step:2185/2285 train_time:1640143ms step_avg:750.64ms
step:2186/2285 train_time:1640902ms step_avg:750.64ms
step:2187/2285 train_time:1641657ms step_avg:750.64ms
step:2188/2285 train_time:1642414ms step_avg:750.65ms
step:2189/2285 train_time:1643169ms step_avg:750.65ms
step:2190/2285 train_time:1643928ms step_avg:750.65ms
step:2191/2285 train_time:1644684ms step_avg:750.65ms
step:2192/2285 train_time:1645444ms step_avg:750.66ms
step:2193/2285 train_time:1646199ms step_avg:750.66ms
step:2194/2285 train_time:1646958ms step_avg:750.66ms
step:2195/2285 train_time:1647710ms step_avg:750.66ms
step:2196/2285 train_time:1648470ms step_avg:750.67ms
step:2197/2285 train_time:1649222ms step_avg:750.67ms
step:2198/2285 train_time:1649982ms step_avg:750.67ms
step:2199/2285 train_time:1650736ms step_avg:750.68ms
step:2200/2285 train_time:1651495ms step_avg:750.68ms
step:2200/2285 val_loss:3.4981 train_time:1651594ms step_avg:750.72ms
step:2201/2285 train_time:1652249ms step_avg:750.68ms
step:2202/2285 train_time:1653009ms step_avg:750.69ms
step:2203/2285 train_time:1653763ms step_avg:750.69ms
step:2204/2285 train_time:1654518ms step_avg:750.69ms
step:2205/2285 train_time:1655273ms step_avg:750.69ms
step:2206/2285 train_time:1656034ms step_avg:750.70ms
step:2207/2285 train_time:1656788ms step_avg:750.70ms
step:2208/2285 train_time:1657545ms step_avg:750.70ms
step:2209/2285 train_time:1658299ms step_avg:750.70ms
step:2210/2285 train_time:1659058ms step_avg:750.71ms
step:2211/2285 train_time:1659813ms step_avg:750.71ms
step:2212/2285 train_time:1660572ms step_avg:750.71ms
step:2213/2285 train_time:1661327ms step_avg:750.71ms
step:2214/2285 train_time:1662085ms step_avg:750.72ms
step:2215/2285 train_time:1662838ms step_avg:750.72ms
step:2216/2285 train_time:1663596ms step_avg:750.72ms
step:2217/2285 train_time:1664352ms step_avg:750.72ms
step:2218/2285 train_time:1665112ms step_avg:750.73ms
step:2219/2285 train_time:1665864ms step_avg:750.73ms
step:2220/2285 train_time:1666622ms step_avg:750.73ms
step:2221/2285 train_time:1667376ms step_avg:750.73ms
step:2222/2285 train_time:1668136ms step_avg:750.74ms
step:2223/2285 train_time:1668888ms step_avg:750.74ms
step:2224/2285 train_time:1669647ms step_avg:750.74ms
step:2225/2285 train_time:1670397ms step_avg:750.74ms
step:2226/2285 train_time:1671159ms step_avg:750.75ms
step:2227/2285 train_time:1671911ms step_avg:750.75ms
step:2228/2285 train_time:1672671ms step_avg:750.75ms
step:2229/2285 train_time:1673425ms step_avg:750.75ms
step:2230/2285 train_time:1674184ms step_avg:750.76ms
step:2231/2285 train_time:1674937ms step_avg:750.76ms
step:2232/2285 train_time:1675696ms step_avg:750.76ms
step:2233/2285 train_time:1676449ms step_avg:750.76ms
step:2234/2285 train_time:1677210ms step_avg:750.77ms
step:2235/2285 train_time:1677963ms step_avg:750.77ms
step:2236/2285 train_time:1678723ms step_avg:750.77ms
step:2237/2285 train_time:1679475ms step_avg:750.77ms
step:2238/2285 train_time:1680235ms step_avg:750.78ms
step:2239/2285 train_time:1680987ms step_avg:750.78ms
step:2240/2285 train_time:1681745ms step_avg:750.78ms
step:2241/2285 train_time:1682497ms step_avg:750.78ms
step:2242/2285 train_time:1683256ms step_avg:750.78ms
step:2243/2285 train_time:1684011ms step_avg:750.79ms
step:2244/2285 train_time:1684771ms step_avg:750.79ms
step:2245/2285 train_time:1685525ms step_avg:750.79ms
step:2246/2285 train_time:1686284ms step_avg:750.79ms
step:2247/2285 train_time:1687040ms step_avg:750.80ms
step:2248/2285 train_time:1687801ms step_avg:750.80ms
step:2249/2285 train_time:1688557ms step_avg:750.80ms
step:2250/2285 train_time:1689317ms step_avg:750.81ms
step:2251/2285 train_time:1690072ms step_avg:750.81ms
step:2252/2285 train_time:1690833ms step_avg:750.81ms
step:2253/2285 train_time:1692194ms step_avg:751.08ms
step:2254/2285 train_time:1692961ms step_avg:751.09ms
step:2255/2285 train_time:1693715ms step_avg:751.09ms
step:2256/2285 train_time:1694476ms step_avg:751.10ms
step:2257/2285 train_time:1695233ms step_avg:751.10ms
step:2258/2285 train_time:1695996ms step_avg:751.11ms
step:2259/2285 train_time:1696749ms step_avg:751.11ms
step:2260/2285 train_time:1697507ms step_avg:751.11ms
step:2261/2285 train_time:1698259ms step_avg:751.11ms
step:2262/2285 train_time:1699020ms step_avg:751.11ms
step:2263/2285 train_time:1699776ms step_avg:751.12ms
step:2264/2285 train_time:1700538ms step_avg:751.12ms
step:2265/2285 train_time:1701294ms step_avg:751.12ms
step:2266/2285 train_time:1702055ms step_avg:751.13ms
step:2267/2285 train_time:1702812ms step_avg:751.13ms
step:2268/2285 train_time:1703570ms step_avg:751.13ms
step:2269/2285 train_time:1704326ms step_avg:751.14ms
step:2270/2285 train_time:1705088ms step_avg:751.14ms
step:2271/2285 train_time:1705840ms step_avg:751.14ms
step:2272/2285 train_time:1706601ms step_avg:751.14ms
step:2273/2285 train_time:1707357ms step_avg:751.15ms
step:2274/2285 train_time:1708119ms step_avg:751.15ms
step:2275/2285 train_time:1708872ms step_avg:751.15ms
step:2276/2285 train_time:1709633ms step_avg:751.16ms
step:2277/2285 train_time:1710388ms step_avg:751.16ms
step:2278/2285 train_time:1711145ms step_avg:751.16ms
step:2279/2285 train_time:1711901ms step_avg:751.16ms
step:2280/2285 train_time:1712660ms step_avg:751.17ms
step:2281/2285 train_time:1713418ms step_avg:751.17ms
step:2282/2285 train_time:1714179ms step_avg:751.17ms
step:2283/2285 train_time:1714935ms step_avg:751.18ms
step:2284/2285 train_time:1715695ms step_avg:751.18ms
step:2285/2285 train_time:1716451ms step_avg:751.18ms
step:2285/2285 val_loss:3.4894 train_time:1716549ms step_avg:751.22ms
peak memory allocated: 30846 MiB reserved: 52870 MiB
